[
    {
        "module_number": 1,
        "module_name": "Fundamentals of System Hardware Script",
        "file_name": "Module 1 Fundamentals of System Hardware Script.docx",
        "transcript": [
            {
                "title": "Introduction 1.2",
                "data": [
                    "In this first module we just want to give everybody a start off with a basis on what a computer is and how it works. In the future modules we're going to be doing a lot of programming. We're going to be doing a lot of operating systems concerns.",
                    "We're going to be doing a lot of things that you need to know about what goes on inside the computer and so in this module we just like to start you off with the very basis of what's inside the computer and how things work. We don't know what your background is we don't know where you come from or what your history is so if some of this is a little bit basic, then we apologize but for everybody we\u2019d like to start off on the same foot.",
                    ""
                ]
            },
            {
                "title": "Overview 1.3",
                "data": [
                    "So just an overview for what we're going to be covering in this module we\u2019ll cover the definition of a computer, types of computers, what's inside a computer (just an overview of what's in what the components are), what each component does, the commonalities between the components, how the components communicate with each other, how the CPU works, the idea of a memory hierarchy, hard disks, and some computer networking concerns.",
                    ""
                ]
            },
            {
                "title": "Definition 1.4",
                "data": [
                    "To give an easy definition of what a computer is a very basic definition the computer is an electromechanical device which takes input, does processing and produces output. And we know that what we define as a computer today is a heck of a lot more, but this is just a basic definition for what we think should be in a computer in the most basic form: taking input, do some processing, and producing output. It meets the very basis of the definition and it doesn't have a lot of superfluous items that we are not necessarily concerned with.",
                    ""
                ]
            },
            {
                "title": "Types of Computers 1.5",
                "data": [
                    "We have a lot of different types of computers in use in the environments today as well as and past history. So we have this concept of a mainframe which is a very, very large computer which used to act for lack of a better term, used to act as a central point for all the computing done on a campus. So for example, one university might have just one mainframe for each campus in the entire university and that's where all the processing was done. Imagine only having one computer for the entire university. It gets very very busy and the time on that computer is very much in demand. And then we move to a more common definition today which is that of a server, and a server would be something that probably exists in a data center or in a room dedicated with air conditioning and power controls. This would not be used by most people, so it would have a lot of capability a lot of RAM a lot of CPU (we will talk about these definitions later) but the server serves one purpose. And a desktop would be used by an end user at one physical location (obviously) because you don't want to carry a desktop around too much. A laptop we know we just carry it around and use it just as a regular computer. A tablet is pretty much the same as a regular laptop except without a keyboard, and that's basically what we've come to these days. And then we could also add that a portable phone is by definition a computer because it does take input, do some processing on that input (your voice), and then encoding your voice, and then it produces output in the form of a signal it gets transmitted to your cell provider.",
                    "So all of these are types of computers and we're going to explore the commonalities between these computers in just a minute.",
                    ""
                ]
            },
            {
                "title": "Inside a Computer 1.6",
                "data": [
                    "So in this image, we've got what looks like the inside of a desktop computer and you're seeing hopefully what you would expect to see when you open up a regular desktop computer. In the upper right there we've got the power supply, which has the big label that says Dell on it. And below that is what we call the main board or motherboard of the computer where we have most of the main components related to the computer. Down below it you'll see the CPU. The memory is off to the left. A little bit further than that you have the secondary storage device which in this case looks like a spinning hard drive. All the way up in the upper left we have the tertiary storage device which looks like a CD-ROM and then run about the middle we have the video controller card. So these are the major macro components of what's inside of a computer and each of them communicate with each other through the main board which has components known as the system bus, which we're going to talk about in a little bit.",
                    ""
                ]
            },
            {
                "title": "What\u2019s Common Between Them 1.7",
                "data": [
                    "So all computers every computer will have at least one central processing unit and the central processing unit is the brain of the computer. That's where all the real work is done. Main memory \u2013 main memory is where the code and the data are stored, but main memory has got to be temporary storage because once we turn the computer off all of that is lost. And then because we're going to need some sort of a permanent storage system, there's got to be a secondary storage system where the information is stored permanently. Of course all computers are going to have something that relates all those, so that they can communicate. Most computers are also going to have a video graphics controller where images can be rendered and we can put them on display on a screen.",
                    "They'll also probably have a network interface so that we can communicate on the Internet or in the local area network. And then of course they're probably going to have a peripheral interface which is a USB or a Thunderbolt, or a Firewire, or a SCSI. And all of these components meet the requirements so that we can bring data into the computer, we can process that data, and then we can produce output. And if you look back at each one of these individual items that I've mentioned you're going to see that they either do one thing or two things: input processing or output and that's the basic idea.",
                    "Most devices inside the computer are either IO devices or they're processing.",
                    "",
                    "Communications Between the Devices 1.8 ",
                    "So all the communications inside of the machine the communications between the individual components are done via a bus. And a bus is, actually the way that I usually think about it is a big yellow school bus moving from one component to the next and carrying, not children in this case, but information and the information can move either from one component to the other or vice versa.",
                    "So it's a bidirectional communications pathway in which we can send information and the bus is actually something that exists physically in the system. So if we put the system under a microscope, we could see the little copper lines that go on the mainboard or on the component to act as the physical bus. So this is something that we can see physically on the system if we probably put it under a microscope because it's rather small, but the system bus is the main pathway between the CPU and main memory.",
                    "Now IO devices often have access to the system bus as well for communications both to main memory and to the CPU but the system bus is primarily used priority for CPU and main memory communications back and forth, and that's what has to be done very very quickly inside the system.",
                    ""
                ]
            },
            {
                "title": "The CPU 1.9",
                "data": [
                    "So the brain of the computer is what's known as the central processing unit or the CPU and this is where all of the processing, all of the data processing, all the commands that we're going to run inside of a computer. This is where all that is done. It is a single piece of silicon in the form of a chip designed and manufactured by a company like Intel or AMD. or there's a few others that produce these chips, and this is the only location in the entire system where code can actually be run.",
                    "Now the code that's going to run here is not the code that we're going to write, at least not in the first parts of this course. The code that we're going to write in the first part of this course is what's known as a high level language and the CPU only runs machine language code. Machine language code is not easy to write and we will do something a little bit like it but it will then convert to machine language code. ",
                    "In short machine language codes for machines and human code is for\u2026 higher level code is for humans. The CPU operates on a fetch, decode, execute cycle which we'll talk about in just a little bit, but it runs this cycle very very quickly and when it's running this cycle each CPU has its own set of instructions which it understands.",
                    "So the CPU has a language for itself if you will. Each CPU really only has a small amount of memory, there's not a lot of memory inside the CPU and those are called registers. And the registers are where we store information temporarily, in order to do the immediate processing.",
                    "But we only have a few of these registers so we have to be\u2026 When we're writing machine language code, we have to be very careful to use them without wasting the memory. And the CPU may have a cache memory which can perform more quickly than main memory, but it's not really much of a concern for us because as programmers we don't have a lot of capability for working with cache memory, in terms of what's inside the CPU.",
                    "",
                    ""
                ]
            },
            {
                "title": "Machine Language 1.10",
                "data": [
                    "Computers only understand very basic commands and I've listed some of the commands here, so you can see them: move, add, subtract, multiply, compare, and jump. You're not seeing any of the higher level commands that you might see in a high level language. So there's really no function calls; there's not a lot of not a lot of commands that we would see except for very basic math operations. There are a lot of machine language instructions, but this set of machine language instructions is limited and we can't really add to it. The machine language instructions are designed physically into the chip.",
                    "So the chip is programmed by its manufacture and it's written in silicone in the chip. So we can\u2019t expand on that we have no capability to add instructions, but we have the capability to build higher level languages and make the higher level languages convert to the machine language instructions.",
                    ""
                ]
            },
            {
                "title": "Instruction Set 1.11",
                "data": [
                    "When the desires of the CPU create the instructions what they're doing is creating what's called a set of instructions or an instruction set and these are the set of instructions that the CPU can perform. And it's usually a very small set just about one hundred instructions or so each one represented by a numeric value and when the CPU receives a particular instruction it performs that task that's associated with the instruction.",
                    "Now one instruction might actually have multiple codes associated with it if there are different variables that are associated with that instruction but fundamentally we only have a very small set of instructions and they do limited operations. And we have to work with the higher level languages to bring the higher level languages code down into the machine language code so that it can actually run on the CPU.",
                    ""
                ]
            },
            {
                "title": "Fetch-Execute Cycle 1.12",
                "data": [
                    "Probably the most important consideration here is what the CPU does when it has to execute instructions. And it performs what\u2019s called the fetch-execute cycle, or the fetch-decode-execute cycle. What it does is move one instruction from main memory, which we haven\u2019t talked about but we will in just a minute, it moves it from main memory into a register in the CPU, which is known as the instruction register. And then it decodes the instruction and if necessary moves in any necessary parameters, so it copies those parameters in from main memory. And once everything is complied in the CPU, once everything is loaded in the CPU, then the CPU can actually execute that instruction and do whatever the task is that is asked. For example, if we had an add instruction which added two numbers together, the fetch would be for a particular memory location. And we would bring that memory location to the register, and we\u2019d add that to a value, maybe that\u2019s already in the CPU or maybe its in main memory. Once the operation is complete, then we repeat the process with the next instruction in the sequence. So this operation happens very very very quickly in the system and it has to happen literally millions of times per second in the CPU. Which means each operation could take as little as a bout 10 nanoseconds to perform this operation which is unbelievably small. Nano is one times ten to the negative nine, so we\u2019re talking about something happening incredibly quickly inside the system. So while it happens very frequently, literally millions of times a second, it happens very very quickly."
                ]
            },
            {
                "title": "Memory 1.13",
                "data": [
                    "The instructions and all the data has to come from somewhere. And in order for the code to be executed it has to be in a register built into the CPU, that one that we spoke about called the instruction register. So why don't we just load all of the code directly into the CPU and then run it directly from there.",
                    "The unfortunate fact is that the amount of memory inside of a CPU is really measured in bytes. It's a very small amount of memory. So it doesn't fit a lot of code in fact we really only want to store one instruction directly inside the CPU and the reason that is because each additional byte of memory costs a really significant amount of money to add to each CPU and the manufacturers of these CPU's like Intel and A.M.D. want to keep the costs down as much as possible so we can store everything in registers because it's just too expensive so what we do is we create a memory hierarchy whereby each layer of the hierarchy adds a little bit more space and costs a little bit less but in addition it runs a little bit slower. And we'll see that.",
                    ""
                ]
            },
            {
                "title": "The Hierarchy 1.14",
                "data": [
                    "So at the top of the memory hierarchy you can see that we have registers and registers a built regularly into the CPU; there's only a few of them, the size is measured in bytes. But the time to access each of those bytes is measured in nanoseconds and registers are the only place that instructions can be executed.",
                    "So we need to use the registers very sparingly. We need to be very cautious about how we use the registers, and in fact in the higher level languages we're not going to concern ourselves with how to use the registers. But when we get down to machine level instructions we have to be cautious about that. The next level of the hierarchy is cache, and in cache we can measure. There's actually two layers of cache but we can measure the total in megabytes. And each of them have nanosecond access times. Although it is slower than a register it can help speed up the processing and the CPU does this automatically in terms of things like pipelining and bringing in  instructions before they're really needed. The processor designers take care of the cache and it's not terribly useful for programmers there's not a lot that we can do inside the cache of a CPU. The next level is probably one of the most important ones that we have to take care of and that's RAM. RAM is measured, of course, in gigabytes, but it does have a lot slower access time; this is over ten nanoseconds access time. So it's a factor of ten slower than the registers.",
                    "But it's the only place where we can really store code and store our data. We do have to take into account, as I said before, that RAM is volatile memory. So once the computer is shut down all the contents of RAM are erased completely and that's automatic, it's just by design - the way the RAM is designed. But RAM is incredibly useful because that's where the data is going to come from that's where the code is going to come from to load the code into the registers one by one.",
                    "Of course if we have temporary storage, we're going to need some permanent storage because we don't lose everything every time the computers shut down. So the secondary storage devices which have sizes that may be measured in terabytes have much slower access times on the order of ten milliseconds, which if you remember we talked about RAM is ten nanoseconds that's one times ten to the negative nine, ten milliseconds is one times ten to the negative third, so secondary storage is about a million times slower than RAM but it's the only permanent storage device that we have inside the system.",
                    "So if we want to save something so that it's not erased when the power is lost on the computer, the secondary storage is where we have to store. Secondary storage is usually in the form of a hard drive or a more modern solid state disk drive, but we use it for the permanent storage of the system.",
                    "Thankfully this is where the operating system comes from this is where your code comes from when it starts off and then it's loaded into RAM, of course so that it can execute. But this is where we store just about everything on the system. Tertiary storage is a term that we use for offline storage and these are things like your USB, flash drives, a CD-ROM drives, if anybody still uses those, possibly even tape drives, if we're talking about a server. These have sizes that can be left to your imagination and we can talk about offline storage in the form of petabytes and possibly even exabytes. This is used for backup and it's used for information that doesn't need to be immediately accessible.",
                    "You can imagine that the time it takes to load a tape would certainly be significant if we were talking about it in comparison to in ten millisecond access time but that's the idea behind the memory hierarchy as we're working at the top things are faster but significantly smaller and much more costly as we're working at the bottom things are much much slower but more larger and much less cost much cheaper.",
                    "",
                    "Ram 1.15 ",
                    "RAM is another term that we use for main memory and that's because of it's the way it's constructed it's known as random access memory because any place in the memory can be accessed in the same amount of time, which in computer science is known as random time.",
                    "So we can access any byte of main memory at the same amount of time. The areas of main memory inside RAM are broken down into bytes and each byte is access independently, so we can access all of the bytes of main memory as individual bytes or we can group them together in terms of known as a word or D word or cue word double word or quad word.",
                    "Usually they're accessed in terms of the data types which we're accessing might have a particular size. So for example in C++ it's very common to have an integer which is a size of four bytes which is equivalent to a D word or double word. Unfortunately was RAM, as we said when the computers turned off everything in RAM is lost, all of it is completely erased and that's by virtue of its design.",
                    "So once the electrical power is shut down to RAM, then the RAM no longer contains any information. When running a program all the machine language instructions are brought into RAM and then there one by one pulled by the CPU during the fetch and execute cycle into that instruction register and then process. So RAM is an absolutely critical portion of the system and we have to know how it works in order to use.",
                    "",
                    "RAM 1.16 ",
                    "As you can see from this, this is what looks like a physical RAM stick or it's known as a dual inline memory module. What we have here is a printed circuit board or a PCB with a number of chips. This one is made by Samsung Corp. Each of the individual chips store a certain amount of RAM. Regardless you can't really peer into it and look at what's going on and as soon as you pull the power out everything's lost anyhow. So the basic idea here is that you have a physical device in which we can temporarily store some information.",
                    ""
                ]
            },
            {
                "title": "Secondary Storage 1.17",
                "data": [
                    "So the secondary storage can be broken down into two types, and this is kind of representing a change in the industry as we are today we're moving from hard disk drives to solid state disk drives for a lot of computers. Now hardest drives are not going away by any means. Hard disk drives are also known as spinning drives and the reason for that is that they contain magnetic material which is in the form of a disk and the disk rotates at a constant velocity, and it's measured in a number of revolutions per minute.",
                    "And that's published by the manufacturer usually is fifty four hundred RPM or seventy two hundred RPM or possibly even ten thousand RPM. The disk rotates and we have a head that moves in and out along the disk for different radii on the disk and this allows us to define, by polar coordinates, a particular radius with a particular rotation angle and a height (the head) to indicate a specific point on the disk where we want to either read or write. So we can access the entire magnetic material which is where the real storage happens. Unfortunately with this system if we move the head, which has some time that it takes to move, if we move the head to the innermost radius and then do a read operation and then try and move the head to the outermost radius, it really takes a significant amount of time in comparison to move in the head to an adjacent radius. So what we call the each radius is known as a track, and it looks like a track if you were running around in a in a gym. Each track can hold a certain amount of information and the tracks are all adjacent to one another but moving from one track to the next adjacent track takes a very insignificant amount of time, I shouldn\u2019t say insignificant, it still takes an amount of time, but moving from an inner most track to a very far distant track will take quite a significant amount of time. In addition to that we have this rotation that happens and on a drive with, for example a fifty four hundred RPM, we can recognize that it takes eleven milliseconds to do one full rotation of the disc. So if we move the head to the appropriate location and happen to notice that the sector that we want to read, the location where we wanted to read just went past, it's going to take eleven milliseconds for that to come around again. So that could take some significant time, also. The benefit to having a hard disk drive is that the size can be enormous, sizes of four terabytes six terabytes are not usual today. So these sizes are really significant amount of storage space.",
                    "The unfortunate downside is that accessing that storage does take a little bit of extra time with a solid state disk drive, which is where a lot of the industry is moving to for most end user machines. This solid state destroy don't have anything that moves, hence the term solid state. They contain a number of chips which look just like your USB flash drives, if you ever happen to accidentally crack one of those open. All the data is stored electrically in the chips and it looks very much like RAM chips, but the chips are in a very different design, the chips are designed very differently.",
                    "These are store data permanently, so that even if the power is removed from the chip the data is still stored which is a great thing because this is secondary storage and we don't want to lose the data if the power is removed. Thankfully with solid state disk drives, all the data can be accessed in random time and they can all be accessed in the same amount of time. So accessing the first byte on the drive and then accessing the last byte on the drive doesn't take any additional amount of time other than the first or the second bite.",
                    "Unfortunately due to the cost and design of these drives they're much smaller, so solid state drive are significantly smaller, they're a little bit more expensive but hard drives are larger, and they're cheaper but they perform slower. So again we have the memory hierarchy working here just inside of the term of secondary storage. So even inside secondary storage we still have this concept of a memory hierarchy working.",
                    ""
                ]
            },
            {
                "title": "HDD vs. SDD 1.18",
                "data": [
                    "As you can see from the images, what we have here on the top is a spinning hard drive and you can see that magnetic disk that I was talking about there. The head is the component that looks like a straight line and that one is in what's known as a parking location right now once the disk spins up.",
                    "It would come off that parking location and move to different radii along the disk and that this would just continue spinning along until the read operation was complete or until the drive was shut down completely. So one of the issues that come up with a hard disk drive is that it does waste a lot of energy to move that disk physically, so there is quite a bit of energy savings in in going with the solid state disk drive, which is along the bottom, and there's a performance benefit also for solid state disks. Unfortunately the downside is with a solid state disk you get significantly less storage than with a hard disk drive.",
                    ""
                ]
            },
            {
                "title": "Networking 1.19",
                "data": [
                    "Since we now live in a globally connected world. We now understand the data can come from a variety of sources literally around the world or even part of it, right?  The ISS has connections to the Internet, so we might be talking with somebody on the Internet and they might not even be on Earth. Networks are connected via the Internet and we can access data anywhere on our network, so now we can really access data from anywhere.",
                    "And it's important to understand how that data comes in and how that data moves around the Internet and around networks. So that's where we're going to talk about.",
                    ""
                ]
            },
            {
                "title": "Physical Connections 1.20",
                "data": [
                    "So there's a lot of different ways that computers can be connected physically. And we want to take a look at a few of those in the most popular ones that have come up with are copper, fiber, and wireless. The copper is your standard Ethernet cable or any sort of physical cable that involves a metallic connection, and that's usually called copper because it's usually made up of copper. Very commonly inside of a copper cable there might be eight wires, four pairs of cables and they're twisted together in a form of an unshielded twisted pair or you might have heard UTP cable so that's the idea behind a copper physical connection. Fiber is an Advent that's come out in the past thirty years and it's actually where we transmit information via light on a piece of glass.",
                    "That's a cable but it can transmit with a lot less attenuation, so we lose a lot less information in that data transfer via light and it can go a lot farther distances. So fiber cables go faster and they travel longer distances, unfortunately they cost a lot more too because it's glass. And then the latest one is wireless, we've had wireless only for about the past twenty years and it's certainly taken off.",
                    "We see wireless virtually everywhere usually in the form of a wifi connection which we'll talk about. But wifi is only one type of wireless and we have to understand that there's quite a few types of wireless including microwave and a lot of others. Protocols are in use for a lot of situations, for example we want to know when we need to start sending data, we want to know who it's coming from, we want to know who it's going to. In terms of physical connections, the idea of a protocol is very important because it's a language, it's a language that two computers are going to be talking and each of them have to understand the complete language. So we need to know, we need to have a very well defined protocol and thankfully we do. As far as physical connections are concerned we can talk about ethernet and ethernet physical connection would define both the protocol and the type of connection so we might have for example one hundred base T, which is a fast ether net connection better known as Fast ethernet, that's one hundred megabit connection of ethernet over copper and it defines how we would communicate; it defines when we start sending data, it defines when we stop sending data, and it defines the format for information that's being passed over that network. So we have a lot of these different protocols including wifi which you've heard of as the 802.11 group, the 802.11 group is defined by the IEEE and it has a lot of different sub working groups for example 802.11B was the original one that produced eleven megabit wifi and we've 802.11N, and AC, and there's a lot of different letters that go associated with that and then we have ATM which is asynchronous transfer mode which is used in a lot of backbone networks for transmission of high capacity links.",
                    "So all of these are our physical types of connections that we would have to physically connect to multiple computers to a network.",
                    ""
                ]
            },
            {
                "title": "Packets 1.21",
                "data": [
                    "When we used to make a call on the telephone, we'd have a physical connection. A physical connection between your telephone and the destination telephone and you can go back to the very, very old days, your parents and grandparents days, of making telephone calls. They used to call the local operator and ask for a connection to the national operator and the national operator would connect to another local operator another local operator would connect to the destination. Along this line they would create a physical channel so one wire that went from the source phone, your phone, to the destination phone.",
                    "That's absolutely not what happens at all today. Now we're sending packets. So today we send small amounts of information, very, very small amounts of information usually of around a thousand bytes or fifteen hundred bytes and the data gets sent from one program running on a machine to another program but unfortunately it can't be sent directly, it has to be sent via protocols which each one will add a small amount of information to this packet.",
                    "So the way that we send it is buying capsulated the packet and it's given to the lower level protocols, so we take the data and we give it to the next lower level, and then the next lower level gives it to another lower level, and the next lower level gives it's another lower level, so we have a higher key that's created whereby the data stands at the top. And then we have a small amount of information for application protocols added to the front of that data, and then a network, for example, layer will add a small amount of data to the front of that. Now the purpose of all of this is so that when it's received on the destination the destination can decide which for example application is supposed to receive this information so we might have for just to use a good example we might have two tabs of chrome open, or even we might have chrome and Internet Explorer open and if we have chrome an Internet Explorer open and both are browsing the exact same website at the exact same time when we receive data back, it's really important for the computer to be able to route appropriately the response from the website either to chrome or to Internet Explorer and we do that via quite a few protocols which will talk about in just a minute, but it's important to know that path back. When the packet moves to the logical protocol we add a header and so on and so forth until the data is actually sent on the physical network.",
                    ""
                ]
            },
            {
                "title": "Layers We Commonly Use 1.22",
                "data": [
                    "So we have a lot of layers that we often use, and I've chosen some ones that are probably familiar with here. Right now you're watching this video via a website and the way that you've done that is by connecting via they hypertext transport protocol or HTTP. And that's an application level protocol which is used to send data to and from websites.",
                    "We also have the Simple Mail Transport Protocol  (SMTP) which is used to pass mail between mail servers or the Internet Message Access Protocol, the Internet Mail Access Protocol (IMAP) which is used to obtain our mail when it finally is on our mail server we use IMAP to get that down into our machines.",
                    "So those are the application level protocols and then if we move down the scope we can look at logical level protocols which is actually more formally called network level protocols and that's usually broken down into two layers and it ties back to the idea that we have to simulate the concept of a connection like we used to have on the telephone networks.",
                    "So if we're talking about simulating a connection. There's going to be a little bit of overhead involved, so there's going to be some extra data and there's going to be some extra processing time involved in creating a connection, oriented connection if we don't need that, if we don't need to have guaranteed delivery of their packets or if we don't need care that they get out of order, for example, we can use what's called a connection less connection and the most common ones that are in use today are UDP and TCP. UDP being the connection less and TCP being the connection oriented.",
                    "So right now you're watching this video via HTTP over TCP.  We also need to know how to route the packets appropriately from a global perspective. So now each machine in the world is required to have what's known as an Internet Protocol address, an IP address, and that IP address has to be unique for every computer in the world. And I know I'm being overly simplistic, and those people that know will know that there's things called network address translation and we deal with this because of the exhaust of the IPV4 protocol addressing scheme, but let's be simple and say that every computer in the world has to have a globally unique IP address. What that means is that your IP address is unique to your computer and if I need to send information directly to your computer I can do so by sending it directly to your IP address. And IP addresses are grouped together so that they form networks and it creates a larger whole which is great, but the IP protocol will add an additional header so that the information can be routed to your machine and you know where it's coming from. And then we have, lastly, the physical layer which often adds both the header end of footer to the end which is mostly concerned with local addressing and how to deal with whatever eccentricities there are in the physical medium.",
                    "So we can talk about ethernet which is very common protocol for use on a physical layer it's in fact probably what you're using and then 802.11 has some physical concerns as well as we move do HTTP header we would add a TCP header we would add an IP header and then we would add some data link or physical header like ethernet So as you're watching this right now we've probably gone through a couple of million packets of information that have been sent via HTTP, TCP, IP, and ethernet altogether.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 2,
        "module_name": "Positional Number Systems",
        "file_name": "Module 2 Positional Number Systems.docx",
        "transcript": [
            {
                "title": "Conclusion 1.23",
                "data": [
                    "",
                    "Digital Data 1.2 ",
                    "Hi there, hoping you're having a great day. Today we're going to talk about position and number systems, and we're all used to working with the decimal number system. We're going to see some other number systems today and see how we can generalize the way we work with a decimal number system to work with some other kinds of number systems. But before we do that, let's talk, let's say a few words about a memory of a presentation. ",
                    "",
                    "So we know that our memory basically is made out of bits that are grouped into a bytes, each one of these bits physically is made with electricity flow or a magnetic field. But we like to think of it abstractly as zero and one because it only has two states. We would need to figure out how to represent a lot of data using only this kind of a primitive unit. So we can think that for example numbers can be represented using the binary number system, we'll talk about it in much more detail soon. But the idea is to represent not only numbers, but other kinds of data is as well. ",
                    "",
                    "For example text, if we write a document or an email. How is that represented using zeros and ones and not symbols as the alphabet symbols? Images; how do were present images with all the colors and details with the zeros and ones? How do we represent videos, or how do we represent audio? All of that should be represented using only zeros and ones. So the key idea here is that everything would be represented as a number, and in a few minutes we see how we represent numbers, but first let's see that all kinds of data can be transformed into numbers, for example text. We have a mapping for each letter, for each symbol in our alphabet into a number. So if we want to take the text, for example \u201cHello world!\u201d we can use this mapping. For example, the upper case \u201cH\u201d would be 72. So we start with 72 and then the lowercase, \u201ce\u201d is 101, so we have 101, and then lowercase \u201cl\u201d is 108. So we have two 108\u2019s and so on. Every character has its own mapped number, and we have a sequence of numbers to represent basically a text or a sentence. Not only letters, but also a symbol, such as space or a question mark. Every symbol has its own a number that its mapped to, so text also is mapped into numbers. ",
                    "",
                    "Images are a bit more tricky. Let's say we have a beautiful image that we want to represent with numbers. So you know that images is a very dense matrix of pixels: each small pixel has its own unique color and then a color can also be represented using numbers. To represent a color, a common way to do that is using an R.G.B., a representation basically the amount of red, green, and blue. Each one ranges 0 to 255. So for example, this purple here is 120 red, 100 green, and 200 blue. The Pink has other values of R G. and B. and so on. So basically, each pixel has three numbers that represent its color and then the entire picture would be a very, very long sequence of numbers that represent the level of each color. Videos are basically a sequence of images, so if we know how to represent images with numbers, obviously we can represent videos with numbers. Audio also can be represented with numbers. If we sample the audio wave at a very high rate of times, we can see the voltage level in each point of time that is also a number. ",
                    "",
                    "So basically, all our data is first transformed into numbers. Numbers can be represented with the zeros and ones, we see that in a few minutes. And therefore, all our data in our memory is then represented with the zeros and ones. Let's go ahead and see how we can represent numbers with zeros and ones.",
                    ""
                ]
            },
            {
                "title": "Counting in Base 10 2.1",
                "data": [
                    "In order to get used to working in other number systems, let's start with the most trivial thing we can do. Let\u2019s start counting, and in order to be able to count in different number systems, let's take a deeper look at the decimal number system in base ten. How we and what we're doing in that number system and then we'll try to generalize it for other number systems as well. ",
                    "",
                    "So let's start counting in the decimal number system in base ten kind of going back to second grade, but let's do that. So, zero after that would come one, after one would come two, three, four, five, six, seven, eight, nine. Then, when we get to the tenth object, instead of having a single symbol to represent the amount (the quantity of ten), we grouped ten ones into a single ten. We have this ten in the second position in our number and then we have zero additional ones. So one zero basically means one group of ten and zero additional ones. Then we have eleven, which is one group of ten and a single addition of one, twelve, thirteen, fourteen, up to nineteen. After that, when comes the next object having a group of ten and ten ones, these ten ones would be grouped into another group of ten, so together we would have two groups of ten and zero ones. And so it continues twenty one, twenty two, and so on, up to, for example ninety nine. And after ninety nine, the next object would have ten ones that would be grouped into another ten, therefore, would have ten tens and these ten tens would be grouped into a single one hundred that is represented in the next position. So we have one of one hundred group, zero of the tens groups, zero of the one. And so on, it continues. ",
                    "",
                    "So this is what we do when we are counting in base ten; how we look at the positions of the digits in a decimal representation. The digits we use in order to represent the numbers are zero to nine, to represent all the different values we can have in each position. After nine, we're just not using this position anymore we're grouping it and moving to the next position. ",
                    ""
                ]
            },
            {
                "title": "Counting in Base 5 2.2",
                "data": [
                    "Let's try to do the same. Now let's try counting in base five. ",
                    "",
                    "So it would start zero, one, two, three, four, and now in base five, we are grouping five objects together. In base ten, ten objects (ten ones) were grouped into a ten, ten tens were grouped into a hundred, and so on in base five were grouping more often. We're grouping five objects together so five ones would be grouped into a base five-ten, and five tens would be grouped to one hundred and so on. ",
                    "",
                    "So now that we are counting the fifth object, instead of having the single symbol a single digit to represent five objects, we are using one group of five objects and base five-ten and zero additional ones. After that, we have eleven, twelve, thirteen, and fourteen, and the next object would give us, again, five ones, that would group to another group of ten. So, we would have two groups of base five-ten and zero additional ones. And so on: twenty one, twenty two, up to one hundred. Can you think what comes before hundred? I hope you said forty four, because after forty four the next object would be five ones that would be grouped to another ten, and then we would have five tens that would be Group to one base five-hundred. No tens, no ones, and hundred and one, hundred two, and so on. In this case, the digits we needed to use are only zero to four, because we don't need any other symbols to represent numbers in this kind of representation. We are grouping the five objects together so we only need to represent quantities of zero, one, two, three, and four, as a single object. ",
                    "",
                    "Counting in Base 8 2.3 ",
                    "The same thing would work when we are counting in other number systems as well. I'll talk about some of the more important ones. One of them is the octal number system, short for base eight. The digits here would be zero to seven; we're going to group eight objects together, so we need symbols zero up to seven. It would start zero, one, two, three, four, five, six, seven. After this seven, we would have one group of octal ten and zero ones, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen. After seventeen, the next one would be grouped into another ten. So we would have two tens, zero ones and so on. Again, try to think what comes before a hundred. Hope you got it right, it's 77 right? After 77, the next object would be grouped into another ten, the next eight tens would be grouped into one single object of 100 or 100 things.",
                    ""
                ]
            },
            {
                "title": "Counting in Base 2 2.4",
                "data": [
                    "You can see that since our computer uses zeros and ones, different number systems use different symbols for the digits. If we want to use only zeros and ones base, two would do exactly that. For base two, the only digits we would need to use is two and one, because two objects are grouped together. Let\u2019s start counting in base two, in the binary base system, and so it's zero, one after that would come ten. We're grouping the two ones into a single binary ten and no additional ones, after that we would have eleven. What comes after eleven? 100 right, because we're grouping two ones into a ten then we would have two ten's that would be grouped into one hundred. After that we would have 101, 110, 111 and then 1,000. See how fast we got to 1,000. Actually it makes sense because we group much more often; every two elements are grouped together. Not like in the decimal number system that ten objects would be needed to grouped together to a ten and then only 10 tens would be grouped to a hundred in only 10 hundreds would be grouped to a thousand. Here, much more often we are groupings, our numbers grow along much faster. So, we can keep on counting in binary.",
                    ""
                ]
            },
            {
                "title": "Counting in Base 16 2.5",
                "data": [
                    "We\u2019ll talk about the binary number system in much more detail in a few minutes. But let's take a look at another very important number system: the hexadecimal number system. which is also called the base sixteen. Let's start counting in base sixteen this time. So zero, one, two, three, four, five, six, seven, eight, nine, after nine. So now we have a problem, because we are still not supposed to group the ones to a ten at this stage, right we are grouping only sixteen ones to hexadecimal ten but we don't have any more digits anymore symbols to use for a single digit number. So, when we worked with the number systems that are less than ten, I don't know, the octave, the binary, we just used a subset of the digits zero to nine. For the octal, we use zero to seven. For binary, we use zero to nine. But, now that we have a bigger number system, in this case base sixteen, we need additional symbols. ",
                    "",
                    "So, the common symbols for basics in common digits for basic things is zero to nine, which are the first ten digits we use anyway. And then we need five more, that would be the first five letters in the English alphabet A.B.C. D. E. and F.. So after nine, instead of grouping together and moving to the next position, we still have a single digit number represents the quantity of ten. That is A and then we have B. C. D. E. and F. which represents the amount of fifteen object, see still a single digit number, and then the sixteenth object would be grouped into a single ten and the next position , a decimal ten so we have one and zero additional ones. And then we have eleven, twelve, thirteen, nineteen, after nineteen comes. I hope you didn't say twenty, it's 1a right? Because after one hexadecimal ten and nine ones we have one hexadecimal tens and ten ones that are still not grouped, that is written 1a, after that we have 1b, 1c, up to 1f. After 1f, now comes the twenty, because we are grouping sixteen ones into a single hexadecimal ten. In this case, we would have two tens and no additional ones. Twenty one, twenty two, twenty three, up to 2f, then would come for thirty and so on. Can you think what comes before one hundred? Hope you said f.f. because it represents fifteen tens and fifteen ones. The next one would be grouped into another ten then we would have sixteen ten, grouped into one hundred and so on.",
                    ""
                ]
            },
            {
                "title": "Equivalent Representation 2.6",
                "data": [
                    "Okay, now that we know how to count in different number systems, we can see that we can represent the same value in different ways, depending on the number system we're working. Let's take a look at the number 13. So, when I think 13, I have to be more specific because 13 can be a decimal number, can be an octal number, can be a base five number. They all use the digits one and three. So, I have to say what kind of a 13 I'm referring to. Now that we have a lot of number systems, the way to do that is to subscript the number system we're referring this number with the number thirteen. We enclose it in parenthesis and say subscript ten, basically meaning the decimal number system.",
                    "",
                    "Let's try to figure out what's the equivalent value for the decimal thirteen in the octal number system. So, let's try counting up to thirteen. So one, two, three, four, five, six, seven, octal would after seven would group the ones into a ten, so we would have ten, eleven, twelve, thirteen, fourteen, fifteen. So a decimal thirteen is equivalent to a one five, fifteen, octal. Let's try to figure out what's the equivalent value in the base five number system. One, two, three, four, after four would come ten, eleven, twelve, thirteen, fourteen, after fourteen, we have another group so it would be twenty, twenty one, twenty two, twenty three. So thirteen decimal is equivalent to fifteen octal, and to twenty three base five. Let's try to see what this number is equivalent in the binary number system. Again, let\u2019s count: one, ten, eleven, one, hundred, one, 101, 110, 111, 1000,  1001, 1010, 1011, 1100, 1101. So the decimal thirteen is equivalent to 1101 binary. Let's do the same for the hexadecimal: one, two, three, four, five, six, seven, eight, nine, then would come A. B. C. D., so it would be a D. hexadecimal.",
                    "",
                    "So, a 13 decimal is equivalent to 15 octal to 23 base five to 1101 binary and D. hexadecimal. Once again, you see that a lower number system, a binary number system, is a longer number and groups more often. So, we get a longer number. And a bigger number system, sixteen for example, is a shorter number, it's one digit in this case. It would be very interesting to have a method to be able to convert a number of presentations in one number system to its equivalent in a number of another number system, to translate N. in base b1 to N., the same amount representing the same value, but in base b2 and that's what we're going to do next. ",
                    "",
                    "Base Conversions 3.1 ",
                    "Okay, let's see how we can convert a number in one number system representation into the equivalent number but in a different number systems representation. Actually, I'm not going to show you the general form of taking a number in base b1 and converting it into an equivalent N in b2, I\u2019m going to show you two techniques to do some other kind of a translation.",
                    "",
                    "So, first one would be taking a number \u201cN\u201d and in an arbitrary base B. and converting it into decimal that would be a very important thing to be able to do. Cause a lot of times you'll get numbers in binary hexadecimal and you want to understand what they mean, basically, so you convert it into decimal.",
                    "",
                    "And the other way around is also something important to be able to do. Taking a number in decimal, your natural number system, and converting it into, I don't know, the computer's number system, the binary, or the hexadecimal, or whatever. If you think of it a bit deeper, you see that these two translations actually allow you to convert from any number system to any other number system. Just compose them pmrafter one of the others: start with N and b1, convert it into a decimal, and then take the decimal and convert it into b2. So, basically going from b1 to b2, stepping in the decimal somewhere in the two-step process. But I'll describe each of them separately. Let's start with how to take a number in an arbitrary number system and convert it into decimal. ",
                    "",
                    "Base B to Decimal 3.2 ",
                    "So, for that let's start with the decimal number system itself; understand exactly what the positions of each digit mean, and then try to generalize it to other number systems as well. So, for example, number 375, decimal 375. So, we have three digits here: the ones, tens, and hundreds. So, the one basically stands for the five, that is in the one's position. Each value here, represents a single object, right? Each value in the tens position represents a group of ten objects, and each value in the hundreds position represents a group of one hundred objects. We like to write it as powers of ten, so one hundred is ten to the power of two, ten is ten to the power of one, and one is ten to the power of zero. So, now we can look at 375 as a weighted sum of these weights of the digits in their position. So, we have five times one object: ten to the power of zero. Plus seven tens: seven times ten to the power of one. Plus three hundred, which is three times ten to the power of two. If you add that, you get to 375, not surprising at all, but that what we mean when we say 375 decimal. ",
                    "",
                    "Now, let's look at 125 octal, for example. Once again, each digit here has its own weight, its own amount of objects that it represents. So, the ones digits represent a single object here as well. The ten digits represent a group of eight, right? And the hundredth digits represent eight eighths or eight octal tens, which means 64 objects. Once again, let's write it as powers of eight. So, sixty four is eight to the power of two, eight is eight to the power of one, and one is eight to the power of zero. So, 125 octal, we can look at it as the sum of: 5 times 1 times 8 to the power of 0, plus 2 times 8 to the power of 1, plus 1 times 8 to the power of 2. If you add them all up, you get 85. Basically, 125 octal represents an amount of 85 objects, decimal eighty five by the way. Same thing would work with the binary number system. ",
                    "",
                    "So, again each digit has its own weight, right? The ones digit is 1, the tens digit is 2, the hundreds digit is 4 and the fourth digit, the thousandth digit, represents an amount of eight. Again, as powers of 2 to the 0, 2 to the 1, 2 to the 2, and 2 to the 3. And the sum would then be 1 times 2 to the zero, plus 1 times 2 to the 1, plus 0 times 2 squared, and 1 times 2 to the three. You add them, you'll get eleven. So, 1011 binary is equivalent, represents, 11 objects, 11 decimal, right? ",
                    "",
                    "Same thing for hexadecimal 3b2. Each digit has its own weight: 16 to the 0, 16 to the 1, 16 to the 2, and then when you add it up, you get 2 times 16 to the 0, 11 times 16 to the 1, right? B. is one after the A: A is ten, B. is eleven. So, 11 times 6 into the 1 to 3 times 16 squared. All together it adds up to 946. If you look in a general form, you look at the number An to a 0, like you have n. plus 1 digits here. In the number system, base B., each digit would have its own weight: b. to the 0, b. to the power of 1, b. to the power of 2, up to b. to the power of N. And this number, if you add the digits with their corresponding weight, you'll have the formula: A0 times B. to the power of 0, plus A1 times b to the power of 1, and so on up to An times B. to the power of N. If you use this technique, you'll be able to figure out what\u2019s the decimal value that is represented, in this case a base B. number.",
                    "",
                    ""
                ]
            },
            {
                "title": "Decimal to Base B 3.3",
                "data": [
                    "Okay, so we know how to take a number in an arbitrary number system in base. B. and figure out it's decimal value. Now, let's try to do the other way around: take a decimal number and try to find its representation in base B, an arbitrary number system. ",
                    "Actually, I want to demonstrate it only on base 2, on the binary number system, basically to convert the decimal number into a binary number. We can then generalize this method, translating decimal into other number systems as well. ",
                    "",
                    "Let's take, for example 75, the decimal 75, and try to find its representation, its binary equivalent representation. So let's think how we can get started here, so obviously the binary number is going to be a sequence of digits, each one is going to be a zero or one. We just have to figure out which digits is a zero and which one is the one, since it's going to be a binary number, each digit has its own weight. So, we have the 1s, 2 to the zero, 2 to the 1s (the 10s), 2 to the 2s (the hundreds), 2 to the 3, to the 4, and so on.",
                    "",
                    "Just to be more explicit, let's write these powers of two with their decimal value: so, 2 to the 0 is 1, 2 to the 1 is 2, 2 to the 2 is 4, and so on, 8, 16,32, 64, 128, and so on. So, each digit here has this weight and we need to figure out for each position, whether it's going to be a zero or a one. ",
                    ""
                ]
            },
            {
                "title": "Let's start with this position here, with the digit in with the weight 256. Should that be a zero or a one? I believe most of you said that it must be a 0. Let's try to argue why this is indeed right, or the only possible value for this digit. Actually, it makes a lot of sense because if we would put a 1 here, only this digit would give us the weight of 256 which is greater than the total weight we're trying to represent, the total value of 75. So, there can't be a one in this position. This position, therefore, must be a 0. Actually, not only this position of 256, but all digits that their weight is greater than 75: the 128, 256, 512, and so on. All of these digits must be a zero, right? The real question is what would be the value with the rest of the digits. So, all of them together should represent or should add up to the value of seventy five. And, let's try to go over them one by one and figure out whether they should be a 1 or a 0. We\u2019ll go over them from left to right, starting with this digit here, with a 64.",
                "data": [
                    "",
                    "What do you think, should this digit here be a 0 or a 1? I believe most of you said that it should be a 1, and by the way I agree it would be a 1, but I want to argue why it must be a 1, why it can't be 0? Because my concern is that maybe this digit would be a 0 and the rest of the digits, some way, would add up to 75, even though this digit here is a 0. So, this is not possible but maybe it is. How can we argue that this scenario here is not valid? So, a good argument would say that the rest of the digits, or using the rest of the digits, the largest value that they can represent all together is having them all be ones, right? Taking all of their weights, that would be the greatest value we can have using only these digits. And then, when we add 1 plus 2 plus 4 plus 8 to 16 plus 32, it add up to 63, which is by the way even less than 64. It's obviously less than 75. So, having the 0 at 64 won't allow us to add up to 75 as total, which means that this digit would have to be a 1. ",
                    "",
                    "But, before we say that, I just want to you to note a very important thing regarding the sum of geometrical progression. So, if you add up the geometric progression of 2, for example 1 plus 2 plus 4 plus 8, up to for example, 2 to the power of K., you know what this sum adds up to? So, this sum adds up to 2 to the power of K. plus 1 minus 1. Basically, 1 less than the next power of 2 in that sum. So, in this case, 1 plus 2 plus 4 plus 8 up to 32, is one less than the next power of 2, one less than 64, which is exactly 63. So, there is a very important property, we're going to use it a lot in computer science, so just make sure you know the sum and memorize it. ",
                    "",
                    "So, in this case we agree that this digit here with the weight of 64, must be a 1. After having a 1 here, out of this 75, we already used 64, we already represented 64, which leaves us with 11. So, the rest of the digits must add up to 11. If we need to add up to 11, 32 must obviously be a 0 and so is 16. The weight of 16 is greater than 11 itself, right? The question again comes when we are in a weight that is less than 11. 8, in this case, should this be a 0 or 1? I believe most of you said 1, but let's give this argument again. If for some odd reason, we have a 0 here, the greatest value we can have with the rest of the digits would be 7, right? 1 less than this 8, which is obviously less than 11. So this bit here, must be a 1. Having the 1 in this position out of the 11, we already represented 8, which leaves us with 3 for the rest of the bits. So, the four must be a 0, and the 2 must be a 1, which leaves us 3 minus 2 with the 1 for the rest of the bits. And that is exactly the value that we're going to put for the 1 digit. ",
                    "",
                    "So, you see that this method here figures out the 75. Now that we can copy these bits up there and see that it is 1001011, whatever, and more importantly, this method exposed the bits from left to right. There is an alternative method that would expose the bits from right to left, and won't talk about it now but you can look it up on Wikipedia.",
                    ""
                ]
            },
            {
                "title": "Binary & Hexadecimal Base Conversations 3.4",
                "data": [
                    "Okay, so now we know how to translate a number in an arbitrary number system to decimal, enough to take a decimal number and turn it into a binary. You could generalize it and turn it into any other number system as well. I want to show you, and basically this is enough to convert from one number system to any other number system. If you do these two things as a two step process, but I want to show you another kind of translation and that is to convert a binary number to its equivalent hexadecimal number. It's going to be a direct way, where a direct process that does this translation is going to be a very fast and simple one, I hope. And the question that we ask ourselves is why do we care about the hexadecimal number system? I think I have a better, or I have a good answer for that in one of our future models but for now, let's just see how we can convert a binary to hexadecimal number back and forth. ",
                    "So, for example, let's take a 3b9, a hexadecimal number, and try to figure out its binary representation. So, first I'll show you the technique, how technically you take a hexadecimal number and turn to binary and binary to hexadecimal. I\u2019ll just show you, without explaining why it works just how to do that, and after that, I\u2019ll try to justify it in some sense. ",
                    "",
                    "So, for example, when you get this number 3b9 hexadecimal and you want to convert it into binary. What do you do is actually, with time you memorize it, but for now let's have a table. That one side would be the hexadecimal digits and the other would be the 4 bit binary representation of these digits, so that the hexadecimal digits are 0, 1, 2, 3, up to F, right? And their 4 bit binary representation goes like that. So 0 would be four 0s, 0. 1 would be 0001, 2 would be 0010, 3 would be a 0011, and so on. If you keep on going, you'll see that basically this 4 bit binary representation gives us all the combinations, all the possible values or options of four bits, and it's not a coincidence. It's part of the reason why this method kind of works. So, assuming we have this table, on a piece of paper or we just memorize it, again with time you memorize it. The process of converting hexadecimal number to a binary one works like that, we work digit by digit individually, independently, and write its 4 bit extension instead of the digit. ",
                    "",
                    "So, for example 9 is 1001, and that what will have instead of the 9. b is 1011, that's what we'll have instead of the b. 3 is 0011, that's what would have instead of the 3. So, we have 3b9, each one extended to 4 bits. All of these together gives us the binary equivalent of that number. So, it's kind of simple if we memorize this table, and it's kind of surprising because we are actually translating the hexadecimal number digit by digit, individually and independently. We didn't do that when we wanted to convert 75 decimal to its binary presentation; we didn't convert 7 on its own and 5 on its own and together it was 75, we had to do something that is much more global in order to convert a decimal number into a binary one. But hexadecimal, maybe because it's 2 to the 4, it\u2019s a power of two, allows us to have a much easier process to make these conversions. ",
                    "",
                    "By the way, the other way around taking a binary number and convert it into hexadecimal, would be very similar. We'll have split the numbers into groups of four and each one to match to its hexadecimal digit. Just one thing to note when we group the numbers by fours: we need to start from right to left, because if one of the groups or if the number of digits doesn't divide evenly by four, it is good to add a 0 to its left without changing the value of the number. And then we would get like a whole number of groups of fours. In this case, the first group is a 3, the second group is a d, the third group is a 6, and all together we get 6d3.",
                    ""
                ]
            },
            {
                "title": "Binary and Hexadecimal 3.5",
                "data": [
                    "Okay, so now that we know the technique of how to convert the hexadecimal to binary and backwards binary to hexadecimal. Let's try to say a few words justifying why this magic here works, that we can translate each digit individually, independently without caring with its surrounding. I'll try to give you the intuition of the argument, basically demonstrating it on a specific binary number and how to convert it into hexadecimal. Hopefully you would be able to take this calculation here and generalize it into formal proof, general proof for all cases. So, let's look at this binary number and I'll try to make some mathematical equations, or quantities, that eventually would end up with the hexadecimal equivalent. But, as we go, you\u2019ll see how we are relating back to the table and converting each 4 bits independently to its hexadecimal digit. ",
                    "",
                    "So, let's get started: so we have this number here, it's a binary number, therefore we can write it equivalently as a weighted sum of powers of 2. So, it is 1 times 2 to the 0, plus 1 times 2 to the 1, plus 0 times 2 to the second, and so on. We can go up to, I think it goes out to 0 times 0 to the power of, raised to the power of 11, and now let's look at this sum here, with 12 errands. And probably, not really surprisingly, we\u2019ll divide it into groups of 4 errands. So, let's start with the first 4. In this case, let's take the first 4 errands and just actually copy them. So, we have 1 times 2 to the 0, plus 1 times 2 to the 1, plus 0 times 2 to the square, plus 0 times 2 to the power of 3. So, obviously our first 4 errands in the upper side are equal to the first 4 in the lower one. It won't make any difference if I also multiplied by one, right? Or in other words, instead of one I can just write 2 to the power of 0, exactly the same, right? So that's what we do with the first 4 errands, we kept them equal just wrote it a bit differently. ",
                    "",
                    "Let's take the next 4 errands, in this case, instead of just copying them as is, we'll factor them by 2 to the 4. So, instead of 1 times 2 to the 4, we do 1 times 2 to the 0. And later we'll just multiply it by 2 to the 4 and 0 times 2 to the 5 would be 0 times 2 to the 1 and 1 times 2 to the second, and 1 times 2 to the third. So, taking these 4 errands in the parenthesis and multiplying them by 2 to the 4, would give you the second 4 errands up there.",
                    "",
                    "For the last 4 errands, we\u2019ll do something very similar. We\u2019ll write them but will factor them in this case by 2 to the 8. Right. So, the first one would be 0 times 2 to the 0, plus 1 times 2 to the 1, and so on. You can see that each parenthesis here has some combination of the first 4 powers of 2. ",
                    "",
                    "Let's take a look at these ones for starters. Let's go back to the table and if we take a look here, at this line here, this number 0011 actually equals to 1 times 2 to the 0, plus 1 times 2 to the 1, plus 0 times 2 to the second, plus 0 times 2 to the 3, which is equivalent to 3. So, this thing here is equal to 3, same thing. So, we can write it 3 times, to do the same thing with these 4 errands in the second parenthesis. Again, it's a combination of the first 4 powers of 2. Going back to the table, it would match exactly this line there with 1101. Because 1101 basically stands for 1 times 2 to the 0, plus 0 times 2 to the 1, plus 1 times 2 to the second, plus 1 times 2 to 3, which is equivalent to 13, hexadecimal D. but it's equivalent to 13. So, this thing here is 13 times 2 to the power of 4. Same thing with these 4 errands, we\u2019ll look at the table. It matches this line and it adds up to 6. So, we can write 6 times 2 to the 8. So, again, as we said it's not a coincidence that our table there gave us all the combinations of 4 bits. Because when we do the math, we would get some combination of the first 4 powers of 2: 2 to the 0 through 2 to the 3. ",
                    "",
                    "Now, let's take a look at the powers of 2 that we have there. 2 to the 0, 2 to the 4, and 2 to the 8, so 2 to the 0 is basically 1, which is also 16 to the power of 0. You see like I'm getting us towards a hexadecimal representation, something times powers of 16, so this would be 3 times 6 into the power of 0. Fortunately for us, actually it's not very fortunate, it just works, 2 to the 4 is 16, which is 16 to the power of 1. So, this digit here is also 13 times a power of 16, and 2 to the 8 is 2 to the 4 squared, or in other words, 16 squared. So, we get 6 times 16 squared. So, we can view that as a hexadecimal representation because it's 3 for the ones digit, 13 or D for the tens digit, and 6 for the hexadecimal hundreds digit. So, that very long binary number is equivalent to 6d3 hexidecimal.",
                    ""
                ]
            },
            {
                "title": "Addition 4.1",
                "data": [
                    "Okay, so now that we know how to work with other number systems, we know how to a convert from one system to another. Let's see how we can do some math; how we can do some arithmetic in different number systems. Let's start with addition: how we can add numbers to one another. So again, let's start with a decimal number system. And make sure we get exactly what we do there, and then, try to generalize it to other number systems as well. ",
                    ""
                ]
            },
            {
                "title": "So, for example, let's try to add 325 to 692, two decimal numbers. We start, I think third grade, our teacher said start with the ones digit. So, we add 5 and 2, we get 7. And then, for the tens digit we had 2 tens and 9 tens, that would give us 11 tens, which would, 11 tens would be grouped into 100 and an additional ten. That's why we write 1 in the tens, and carry over an additional hundred. Then we have 100 plus 300 plus 600, it adds up to 10 hundreds. And we have, therefore, 0 for the ten hundreds and 1 for the thousand because these 10 hundreds were grouped into one group of a thousand. So, we have 0 hundreds and 1 carried to the thousands, which would then be dropped down, and all together we have 1017.",
                "data": [
                    "",
                    "Let's try to do the same when we add, or we try to do some additions, in the Octal number system. For example, a 365 plus 243 Octal. Again, we start with the ones digit, 5 plus 3. When we add 5 plus 3, we get 8. But we don't just write 8, because in the octal number system, 8 ones are grouped into a single ten. So, after grouping these eight ones to a ten, we have 0 additional ones and a carry over of a ten. Then, we add a 10, another 6 ten and another 4 ten, that would give us 11 tens, right? Maybe let's try to count it; count it up starting at 6. So, we kind of have to add: 6 plus 5. So, let's add 5 to 6. Let\u2019s count. After six comes seven, after seven we have ten, eleven, twelve, thirteen, so we have thirteen in the tens position. So, we group ten of them to hundreds, we are left with 3 tens and one hundred is carry over. Then, we have 3 plus 1 that's a 4, plus 2 that's a five, a six, that would give us 6. So all together, 365 octal plus 243 octal, gives us 630 octal, right? Same thing as we've done with the decimal, we are working in the octal. We just keep in mind to group items when we have eight individual ones. ",
                    "",
                    "Same thing would work when we try to add binary numbers: 0 plus 1 would be a 1, 0 plus 0 would give us 0, 1 plus 0 would be 1, 1 plus 1 that is 2 obviously. That would be 0 plus 1 of a carry over, 1 plus 1 plus 1, that's a 3, that would leave 1 plus 1 of a carry over. 1 plus 0 plus 0 would be a1, 0 plus 1 is 1, and 1 plus 1 is basically 2, or 10. Or a 0 plus a 1 carry over, which then drops down. And that is the result of adding these two binary numbers. Try a few on your own, and make sure you get how to add the numbers in terms of alternative number system.",
                    ""
                ]
            },
            {
                "title": "Okay, now that we know how to add numbers in different number systems, let's see how we can subtract numbers in a different number system. Again, let's start with decimal for practice, and then try to generalize it to other number systems as well. So, let's try to subtract 427 minus 192 decimal. So, we start with the ones right. 7 minus 2 that would give us a 5, then we try to subtract 2 minus 9 and that's a problem because we can\u2019t subtract 9 from 2, so we would need to borrow an additional 10 tens, actually in this case. So, instead of having 2 tens, we'll have 12 tens, and that by splitting the hundred, the 4 hundreds into 3 hundred and 10 tens. That would leave us with 3 hundred and 10 tens additional 2 tens that would give a 312 ten. So, instead of saying 42 tens or 420, we say 312 tens. Then we have these 12 tens minus 9 that would give us a 3 and 300 minus 1, that would give us a 2. So 427 minus 192 is 235.",
                "data": [
                    " ",
                    "Let's try doing the same for the octal number system. Let\u2019s try to subtract 351 from 536. Same thing, we first subtract 1 from the 6 that would give us a 5, then when we try to subtract 5 from 3, that is not possible. So, we need to split one of the hundreds, one of the 5 hundred into tens. So, instead of having 530, we would have 4 hundred and 13 ten. So therefore, we have 13 tens that we need to take 5 out of them. So, let's count back out of thirteen, so it's twelve, eleven, ten, seven, six. So, we have 6 there, and 4 minus 3 is 1. So, 536 minus 351 is 165 octal.",
                    ""
                ]
            },
            {
                "title": "Signed Numbers 5.1",
                "data": [
                    "Okay, so now we know that we can represent numbers using the binary number system, basically base two, so numbers can be represented using zeros and ones. But then, let's see how we can represent not only positive numbers but also negative numbers. So for example, 26. The decimal 26 is equivalent to 11010 binary, right?  Because if we add up the weights, 16 plus 8 plus 2, it would add up to 26. Check it yourself. ",
                    "",
                    "How can we represent negative 26 decimal in binary? So obviously, we can write negative 11010 but then inside a computer, we don't have, we only have zeros and ones. We don't have the negative sign, we can\u2019t have that represented. So, we would need to figure out a different way to represent negative numbers, signed numbers, using only zeros and ones. ",
                    "",
                    "The first approach I want to show you is called \u201cSign and Magnitude.\u201d It's a very intuitive approach and it says it's a slight variation of a binary number system, the formal binary number system. It said that not all of the bits have their weight depending on their position; the left most bit is used only to say what the sign of the number is, the rest of the bits would have their weighted value depending on the position. So, for example, a positive number is 0, a negative number is 1 for the sign bit and twenty six would be whatever 26 looks like. 000 and then 11010 so negative 26 in the sign magnitude representation, would start with the 1 as the sign and the rest of the bits would just represent 26. This way using only zeros and ones, we can represent negative numbers as well. Just know that the one in the signs bit position doesn't have a power of two as its weight, it represents something else, its represents the sign of the number whether it's positive or negative. So, that's a very intuitive way to represent signed numbers. But then, that's not the way computers typically represent signed integers. The typical way that computers represent signed integers is called \u201cTwo\u2019s Complement\u201d and I'm going to explain this representation method right away.",
                    ""
                ]
            },
            {
                "title": " Two\u2019s Complement 5.2",
                "data": [
                    "Okay, let's see how the \u2018Two\u2019s Complement\u2019 representation method works to represent positive and negative numbers. So, when you want to represent the number, assigned number, in the Two\u2019s Complement representation method it kind of matters how long, how big you want, how many bits you want in this representation. So, let's let me give you a general definition here: so, for presenting a number in k-bits, it has two major properties that you should know about. The first is that if your number that you want to represent is positive, then the first k1, (k. minus 1) bits are represented as the unsigned binary representation, basically the regular base 2, and then the last bit would just be a 0. So, in order to represent a positive number just take its base 2 representation in (k. minus 1) bits, and the k-th bit would be a 0, on its left. And another important property for the Two\u2019s Complement representation method is that if you add a number and its additive inverse, its value with the sign flipped, then the sum would be 2 to the power of k. I suggest you write these two properties down when you look at the next example. ",
                    "",
                    "So for example, if we want to translate the decimal value 26 into Two\u2019s Complement using 8-bits, okay so our k now is 8. So it's 8-bit Two\u2019s Complement representation. Then, since it's a positive number we should look at 26 as in a binary representation, basically in seven bits so the first seven bits would be the binary representation of 26, like that. And then the left-most bit would be 0. Very, actually, very straight forward. ",
                    "Let's try to figure out how negative 26 would look like, in once again, an 8-bit Two\u2019s Complement representation. So, we said that adding a number and its additive inverse should give us 2 to the k. In this case, adding 26 to negative 26 should give us to 2 in this case, k is 8, 2 to the 8. So 26, we know how it looks like, right? 2 to the 8, if you think about it, it is 1 and eight 0s; just as 10 to the 8 decimal is one and eight zeros, in binary 2 to the 8 would be one and eight zeros. You can figure out the weight of each digit and the ninth digit, basically, is 2 to the 8. But now let's try to figure out what negative 26, would, should look like. "
                ]
            },
            {
                "title": "So we know that if we add 26 to negative 26, they should add up to this 1 and eight 0s. So let's try to figure out negative 26, digit by digit. So zero plus what, would give us the zero? Zero, obviously. One plus what, would give us a zero? It could either be zero or one, obviously, it's not a zero. So it must be a one, but not only that it gives us a 0, it also carries another 2 to the next position. So 1 plus 0 plus what, would give us a zero? That's 1 plus 1 would give us a 0 and another carry over of 1. 1 plus 1 plus what, would give us a zero? 1 plus 1 plus 0 would give us a 0, with the carry over; 1 plus 1 plus 0 with the carry over. 1 plus 0 plus what, would give us a zero? 1 plus 0 plus 1 would give us a 0, right? It's basically a ten so it's a 0 and the carryover of another 1; 1 plus 0 plus 1 and 1 plus 0 plus 1. So, taking this number here of 11100110, this thing here, that's the 8-bits Two\u2019s Complement representation of negative 26.",
                "data": [
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 3,
        "module_name": "Hello World Script",
        "file_name": "Module 3 Hello World Script.docx",
        "transcript": [
            {
                "title": "Two\u2019s Complement 5.3",
                "data": [
                    "",
                    "Process of Executing a Program 1.2 ",
                    "Hi there. Today we are going to implement our first C++ program. Typically a first program in a programming language is a Hello World application that reaches out to the user saying \u2018Hey I am here\u2019 or printing Hello World. We are going to write a slightly different first program but before we do that let\u2019s take a closer look at the process at how  an application is executed inside a computer. Let\u2019s focus on three main components inside the computer we\u2019ve talked about a few. Let\u2019s focus on these three the memory the main memory the ram and the secondary memory whether it is a hard disk or a solid disk drive. And obviously the CPU which is the central processing unit that does all of the job inside a computer. So both memories ether it is a second memory or the RAM are basically contains a lot of zeroes and ones. Everything inside a computer is zeroes and ones as you all know. And these bits of zeroes and ones encode a lot of information. The bits are collected into collections of eight bits each collection of eight bits is called a byte and they are stored inside the memory. For example the first byte that was eight bits located in a physical address of zero the second one would be one two and three and all. So there is a very long sequence of bits each one containing zeroes and ones. This is used to represent all types of data with zeroes and ones we saw how we could represent numbers but using numbers we can represent audio we can represent video we can represent documents spreadsheet. All types of data can be represented with zeroes and ones. One other kind of data that is represented is a program or an application. An application is basically a set of instructions. For example these 95 bytes here are a sequence of instructions for a program named prog.exe. Typically that\u2019s the file extension for an application it is executable .exe. So assuming these bytes here are a program prog.exe we can execute it and make the magic of this application happen. What happens when we double click an application? When we want to start executing a program? So there are a few steps that are happening first thing that happens is obviously the program is stored inside our secondary memory so the first thing that happens is this program is copied into our main memory in order for the computer to have fast access to this set of instructions. And then these instructions are starting to execute one after the other. Obviously the CPU is responsible for that. The CPU has a program counter register that says where or what\u2019s the next instruction that would be executed. It is initialized to 100 which is the first instruction in this program and then the fetch decode executes cycle is starting to happen. Each cycle the instruction that the program counter points to is fetched from the main memory into the CPU. Then the CPU decodes it understand what the meaning what it has to do with this instruction and then it executes it. And over and over so the program counter is increased and the next instruction is fetched decoded and executed. Program counter is increased and the instruction is fetched decoded and executed. This happens literally million times each second and each the instructions are just executed one after the other. Whatever the program is intended to do basically happens. ",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 4,
        "module_name": "Data Types and Expressions Part 1",
        "file_name": "Module 4 Data Types and Expressions Part 1.docx",
        "transcript": [
            {
                "title": "Data Types and Expressions 1.2",
                "data": [
                    "Hi there today we're going to talk about data types and expressions in quite a detail. Before we do that I want to say that each programming language has three fundamental constructs. There are data expressions and a control flow. In order to master in a programming language you have to know a lot in each one of them. You have to know a lot in data a lot in expressions a lot about control flow and actually takes some time in order to understand so much. So we are going to take step by step and you have to be kind of patient.",
                    ""
                ]
            },
            {
                "title": "Data Types and Expressions 1.3",
                "data": [
                    "Let\u2019s take a second look at a problem we already implemented the program that reads from the user two numbers and print their sum and see if we can find some of these constructs some data expressions and control flow. So here is the program we implemented you can see that when this program is executed it is executed line after line. First there is cout that prints an instruction to the user please enter two numbers. Then there is the cin that reads from the user these two numbers then we add them store them into sum and eventually we print out the message that whatever the sum is. You can see that the program is executed line by line we call this flow a sequential flow. This is a default flow of a program that lines executed one after the other. You could also see that we have here some data we have three variables numOne numTwo sum they are all integers in C which is a C++ which is a strongly type language. Each data must have a type of its own in this case both or all of our variables are integers int. There are also expressions here in our program there is numOne plus numTwo there is an assignment expressions there is a cout a cin an io expression. So there are all the kinds of these constructs here in this program. ",
                    ""
                ]
            },
            {
                "title": "Data Types and Expressions 1.4",
                "data": [
                    "There is a sequential flow IO expressions arithmetic expressions and data of a type int. Today we are going to focus mostly on some data types there are several data types that are used in C++. There is the int we already talked about. We are going to talk about float and double which are going to be used to represent real numbers numbers with a fractional part. There is a char that is used for characters there is string for textual information and there bool for Boolean values True and False. We are going to talk in much detail about each and every one of these data types. We are going to start talking about int we are going to do it right away.",
                    ""
                ]
            },
            {
                "title": "The int Data Type 2.1",
                "data": [
                    "Ok so let\u2019s get into the details of int of the data type for integers. So integers as we said are used to store integer numbers so every time we want an integer number we use the type int. Let\u2019s see how this int data is represented inside the memory of our computer. What is the inner representation of an int. So there are basically two options here. Either each data would be of a different size for example smaller numbers would take less space less bits in order to represent them. And larger number would take more space in order to represent them. That would make integers be of a different sizes. Another option would be that every int data would have a fixed amount of bytes in its representation. There are advantages and disadvantages for each one of these options and C++ the data types the int datas are all of the same size. Each int data is four bytes long which is 32 bits each byte is 8 bits so four bytes is 32 bits. So each integer is 32 bits long. For example if we have an integer x then inside our memory there would be four bytes for this x. Let\u2019s say starting at address 100 until 104. If we have an integer y after x would come this four bytes for the y from 104 to 108. So each int data uses four bytes. So I think the greatest disadvantage of a fixed size variables or fixed size data is that we can say that 32 bits is quite a lot we can represent two to the power of 32 different values in 32 bits. Two to the power of 32 by the way is around four billion different numbers so there are a lot of numbers we can represent in an int variable. But then if we were thinking about that integer number domain it is an infinite domain and if our program would like to represent a number that are very big for example astronomical distances then 32 bits won\u2019t be enough for that kind of data. So that\u2019s the biggest disadvantage of using a fixed size data but then there are also a lot of advantages of using fixed size data and we will talk more about them in more detail when we speak about arrays later on in this course. Ok so inside the memory each integer takes four bytes let\u2019s think how the integer is represented inside these four bytes. What bits what zeroes and ones would be used in order to represent an integer data. So when we talked in one of our previous modules about number systems we saw that we can represent numbers using zeroes and ones using the binary number system base two. And then since integer can also be negative numbers we talked about the 2\u2019s complement representation method and that\u2019s how integers are represented in C++ the numbers are basically represented in 32 bit 2\u2019s complement representation method. So for example if we set x to 6 obviously 6 decimal is 110 in base 2 but if we are thinking about the 2\u2019s complement representation of 6 basically we are padding it with zeroes so 6 then is this representation in a 32 bit 2\u2019s complement. So these 4 bytes these 32 bits are then placed inside the 4 bytes of x. If we set y to -6 then again -6 as a decimal are represented like in a 32 bit 2\u2019s complement representation method and these 4 bytes are then taken inside the y\u2019s location in the memory. So the numbers are represented in four bytes and using the 2\u2019s complement representation method that\u2019s a very important thing to know about integers. ",
                    ""
                ]
            },
            {
                "title": "The int Data Type 2.2",
                "data": [
                    "Let\u2019s talk about some more stuff regarding integers. For example what are the literals that are built in inside C++ for integers. Maybe let\u2019s take a step aside here and understand what do we mean when we say C++ literals. So when we are talking about data basically data can come into two major forms either variables or constants. Variables are as we can understand by their name can change their values where constants are values that are constants cannot be changed. So if we look at these lines of code here we see that we have x and y which are variables we can see that we can set their values x equals 6 and then set it to 7 or to 8 or whatever they are variables their value can change. And then there are also some constants here in this program. As we said every data in C++ is has a type of its own C++ is a strongly typed language. So 6 -6 even 0 way down there are all of some type in this case they would be of type integers. So we can see that they are variables we can declare them let\u2019s say int x later on we can define double y or whatever type we want to see for each variable. And then there are also contants and then for constants we talk about two kinds of constants one that are built in inside our language when we name C++ literals. Basically we don\u2019t need to define 6 or -6 or 7.3 or abc we don\u2019t need to define what these values are the compiler already recognizes them they are built in inside programming language. But we can also define our own constants we can create a program and define contants. The syntax for that goes something like cons int max = 5 and in this case we are creating a constant of type integer its name is max and the value is set to 5. We will talk more about it later on but the major idea here is once we set max with 5 this value cannot change cannot set max to 7 after a few lines of code. Once we created a constant that value is there to stay. So let\u2019s get back to literals of type int what kind of built in data the compiler recognizes as integers. So as we can expect writing numbers in their decimal representation just like 3 4 -6 3954 or whatever number you want the compiler would recognize them as integers. So we can use them and we know that they are considered to be integers.  ",
                    "The int Data Type 2.3\nOne more thing I want to talk about are our operators that can be applied on integers. Using our arithmetical operators basically we create expressions so we have the literals or the variables which are atomic expression but combining them using arithmetic operators we create more compound expressions. For example plus we can add two integers and then create a greater or a bigger expression. Expression basically is a thing that has a value in this case for example if we have x and y and then x is set to 5 we create an expression x plus 5 basically 5 plus 2 or 7. If we cout x plus 2 the value of this expression would be printed in this case 7 would be printed. But plus is an arithmetical operator that can be applied on integers x is an integer 2 is an integer it is a C++ literal of type int and plus can come in between two integers and create a compound expression in this case x plus 2. So we can print the value of an expression we can set a variable to have the value of an expression. Y equals x plus 2 basically is an assignment when an assignment is evaluated first the write inside is evaluated x plus 2 and then this value which is in this case 7 is set into the variable y. So given an expression we can print it we can assign it to a variable. Actually we can also just type it X plus 2 obviously it is a useless line of code here because no one does nothing with the value x plus 2 but the x plus 2 on its own is a legal expression in C++ and we can write just like that. So plus is arithmetic expression arithmetic operator sorry but then we can have some more arithmetic operators. For example minus and star for times so we can subtract integers x minus 2 we can print it and it could print 3. We can multiply x times 2 5 times 2 which is 10 we can assign it to y y would be 10. So we can apply different arithmetic operators on ints. Plus minus times. We can also apply the divider operator but here it is a little bit tricky. What do you think would happen if we for example cout x divided by 2 5 divided by 2. I would say that it would probably print 2.5 because 5 divided by 2 is two and a half. But then it behaves a little bit differently.\n\nThe int Data Type 2.4\nLet\u2019s stop for a minute and roll back to I don\u2019t know second grade. And recall what our math teacher used to say when we divide 13 by 5. So when we divide 13 by 5 we get something like 2 with a remainder of 3. Basically means 5 fits into 13 two full times and there is still a remainder after these two full times of 3. There are two designated operators that give us these 2 and 3 they are called div and mod. For example if we do 13 div 5 we get 2. Which basically div means how many full times times 5 fits in 13. So 13 div 5 results to 2. And 13 mod 5 results to 3. Basically meaning what\u2019s the remainder when we are dividing 13 by 5. In C++ we don\u2019t use div and mod in the textual writing there are specific symbols to do div and mod. For div we have the slash operator so if we have int slash int that would do a div operator in this case that would be 2. And for mod we will use the percent symbol so 13 percent 5 would be 3 for the remainder when we divide 13 by 5. ",
                    "The int Data Type 2.5\nIf we print or cout x divided by 2 since x and 2 are both integer the slash here is in the context of div. So it would print how many full times 2 fits into 5 which is 2 in this case and if we cout x mod 2 that would print the remainder when we are dividing 13 by 5 or 5 by 2 sorry and that is one. Because this is the remainder when we are 5 by 2. So when we are speaking about arithmetic operators in the context of int we have the plus minus multiplication div and mod. One last operator I want to talk about here is the assignment. Formally an assignment is considered to be an arithmetic operator. Basically by using it we create an arithmetic expression. And when we set x to 6 there is a side effect that basically sets the value of x to 6. But this expression basically also has a value. The value of this expression is the value that is assigned to the variable. So for example if we cout x equals 7 so first the side effect of x getting the value of 7 happens but then the value of this expression 7 is also in this case printed. Later on if we assign y to be x equals 8 once again x equals 8 when it is evaluated sets x to the value of 8 but the value of this expression 8 goes into y. So using assignment as arithmetical operator we can for example create multiple assignments in a single line of code. So we can do y equals x equals 8. Actually we don\u2019t need the parentheses we can just do y equals x equals whatever in this case 9. So these are the major or the fundamental arithmetic operators when we are using integers. And if we recap what we said about integers about the int data type so they are representing they are used represent integer numbers. Each one has a fixed size of 4 bytes. The data in these 4 bytes is represented using a 32 bit 2\u2019s complement method. C++ has built in datas that are considered to be integers basically the C++ literals. We programmers just write integer numbers in their decimal representation and C++ would recognize them consider them to be integers. And we can create arithmetic operators using integers with arithmetic operators in between them. "
                ]
            },
            {
                "title": "Weeks and Days 2.6",
                "data": [
                    "Ok so let\u2019s try to use all the syntax we talked about integers to implement a simple problem. We name it weeks and days problem. So let\u2019s write a program that reads from the user number of days they traveled. And then the program would print what\u2019s the travelling time in the format of how many full weeks they traveled and how many additional days are there. For example if we executed this program the program would ask the user please enter the number of days you traveled. The user would say for example 19 and then the program would respond 19 days or two weeks and five days. Because 19 days are two full weeks and extra five days.",
                    ""
                ]
            },
            {
                "title": "Weeks and Days 2.7",
                "data": [
                    "Before we get into implementing it let\u2019s stop a second and think how we can figure out how many full weeks and extra days are there in a some amount of days. So if we divide let\u2019s say 19 by 7 we get two with a remainder of 5. Two would be how many full times 7 fits into 19 which is the number of full weeks. And 5 is basically the days remaining after these two full weeks. So once again 19 div 7 is the full weeks and 19 mod 5 would be the days remaining. After we have that figured out it will be quite easy implementing it using C++. Let\u2019s go ahead and do that.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 4,
        "module_name": "Data Types and Expressions Part 2",
        "file_name": "Module 4 Data Types and Expressions Part 2.docx",
        "transcript": [
            {
                "title": "Weeks and Days Implementation 2.8",
                "data": [
                    "Float and Double Data Types 1.2 ",
                    "Ok so we talked in quite a detail about the int data type. Let\u2019s start talking about float and double. These two types are quite similar to one another so we will talk about both of them together. And we will just say the small differences between them are. So float and doubles are used both to store real numbers in them. Real numbers meaning number that can have fractional parts. Let\u2019s say 7.5 or 3.65 some decimal point in them. Let\u2019s try and think about the inner representation in of a data that can have fractional part. So as integers both float and doubles also have a fixed size it is not that their size changes depending on the number they are represented. Each float has a size of its own and each double has a size of its own. They differ from one another with their sizes. So for doubles each double data takes 8 bytes where each float data takes only 4 bytes. So double is double the size of a float but again each double data takes 8 bytes no matter what\u2019s represented in it and each float data takes 4 bytes no matter what represented in it. ",
                    ""
                ]
            },
            {
                "title": "Float and Double Data Types 1.3",
                "data": [
                    "For example if we have an integer x and a double y ints takes 4 bytes so x takes let\u2019s say from 100 to 104. And double y y is a double so y takes 8 bytes from 104 to 112. If we assign x with 6 and y with let\u2019s say 7.658 so we are thinking that x would contain 6 and y would contain 7.658. But then we know that it is not we really write the number 6 and the number 7.658 in the memory. So how can we represent floating point numbers or real numbers with fractional parts using zeroes and ones. ",
                    "",
                    "Float and Double Data Types 1.4 ",
                    "One way of trying to do that is decide that if we have let\u2019s 8 bytes that the first 4 bytes would be for the integer part and the other four bytes would be for the fractional part. And this is what we call like where we put decimal point in a fixed position. After four bytes we have fixed position for the point. This is not how doubles and float are really represented inside the memory. I am not going to get into the details of the bits of how to represent floating point numbers you can read in the following link some more information about the floating points representation method it is called IEEE 754. But then I would say that the decimal point it is not in a fixed position the point can float around inside a bunch of bits that\u2019s why it is called the floating point representation method.",
                    ""
                ]
            },
            {
                "title": "Float and Double Data Types 1.5",
                "data": [
                    "So just for an example if we have x equals 6 and y equals 7.658 so we know that 6 is represented in a 32 bit 2\u2019s complement because it is an integer and it would look like that. Y or 7.658 would be represented using the floating point representation method in case of a double in a 64 bit and it would look something like that. And therefore it would be stored in y just like that. So this is the way doubles and floats are basically represented using zeroes and ones. Let\u2019s talk about the built in datas that are considered to be doubles and floats. C++ literals of doubles and floats. So for doubles we just write 3.4 -8.975 or whatever decimal number we want to write. If we want to write an integer number but to be considered as double we have to write 6.0 to add a .0 to it if we just write 6 the compiler would treat it as an integer. If we want to write the double 6 as a C++ literal we just write it 6.0. So for doubles we basically write the decimal representation of real numbers. For floats we just add a prefix of f at the end of the numbers so the compiler would know that when we write 3.4f it is the float literal with a value of 3.4 and not the double literal of 3.4. That\u2019s also for -8.975 and so on. So that\u2019s how we create C++ literals regarding arithmetic operators in this case very very expected. We have a plus minus multiplication division. In this case the slash operator would do real division it won\u2019t do div as it had in integers. When we place divide operator between two doubles or two floats it would make the real division of these two values and assignment as we have talked before.",
                    ""
                ]
            },
            {
                "title": "Float and Double Data Types 1.6",
                "data": [
                    "Ok let\u2019s use these types in order to implement the following program. Let\u2019s write a program that reads from a user the radius of a circle and then the program would calculate and print the area of this circle. So for example we ask the user to enter radius the user would say I don\u2019t know 2.6 and then the program would respond the area of a circle with radius of 2.6 is in this case 21.2372. Let\u2019s take a minute and think how we would implement it. So kind of straight forward we can just use the formula to calculate the area of a circle. So if we have a circle with a radius of r the area of the circle is just pi times r squared. Let\u2019s use this formula in order to make our calculation. Let\u2019s go ahead and implement it.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 4,
        "module_name": "Data Types and Expressions Part 3",
        "file_name": "Module 4 Data Types and Expressions Part 3.docx",
        "transcript": [
            {
                "title": "Float and Double Data Types 1.7",
                "data": [
                    "The char Data Type 1.2 ",
                    "Ok so we\u2019ve talked about ints that represent integers. Float and double that represent real numbers. Let\u2019s talk about the char type. Chars are used to represent characters each char data represents a single character. Let\u2019s see how we represent internally a character in the memory. So for example if we want to represent the letter A which is a character inside the memory obviously we won\u2019t just write a curly A inside the memory because the memory contains only zeroes and ones. So we should figure out a way to represent textual information in this case a character with only zeroes and ones. A common way to do that is to map a letter to a number that corresponds to it. So there would be a number mapped to the letter A a number mapped to the letter B a number mapped to actually lower case letters upper case letter digits some symbols the dollar sign question mark all the other punctuation marks and so on. Let\u2019s see how many different numbers we would need to in order to represent these characters that would give us how big should be each character data. So it seems like one byte that contains 8 bits actually contains 2 to the 8 different values 2 to the 8 is 256. And 256 seems more than enough to represent all the lower case letters upper case letters digits punctuations and stuff like that. So a character data each is takes one byte in the memory. Now the mapping the mapping function that gives each character its number its value is called the ASCII table you can see here that upper case a is mapped to 65 upper case b is mapped to 66 lower case a is mapped to 97 lower case b is mapped to 98. The dollar sign is mapped to 36 you can see that each symbol has its own value that is mapped to it. So for example if we want to write the data A inside the memory using the ASCII table the ASCII value corresponds to A is 97 and then we write 97 in a binary representation in this case would look like 011 four zeroes and a 1. And these 8 bits are now stored in the memory representing the letter A. So characters are mapped to numbers by the ASCII table which are then represented in binary and stored in the memory.",
                    "",
                    "What\u2019s my ASCII Value? 1.3 "
                ]
            }
        ]
    },
    {
        "module_number": 4,
        "module_name": "Data Types and Expressions Part 4",
        "file_name": "Module 4 Data Types and Expressions Part 4.docx",
        "transcript": []
    },
    {
        "module_number": 5,
        "module_name": "Branching Part 1",
        "file_name": "Module 5 Branching Part 1.docx",
        "transcript": [
            {
                "title": "Let\u2019s try to write a program that reads from the user a single character and then prints what\u2019s the ASCII value that corresponds to that character. For example the program would first ask the user please enter a character the user would say upper case T and then the program would respond by saying the ASCII value of upper case T is 84.",
                "data": [
                    "",
                    "Computing the Absolute Value 1.1 ",
                    "Hi there. Today we're going to talk about the branching statements. Let's start with a problem that we're going to try to solve. So, let's write a program that reads from the user an integer and then prints its absolute value. So, for example, the program can prompt the user to enter an integer, the user can enter let's say negative seven, and then the problem would respond by saying that absolute value of negative seven equals seven. ",
                    "",
                    "Let's see how we can implement this behavior. So, it actually depends, in case the user enters a positive integer then the absolute value would remain the same as the input. But in case the user enters a negative number then we need to calculate the additive inverse of that negative number and that would be the absolute value. Mathematically it is kind of easy to calculate the additive inverse of an integer: you just multiply it by negative one so negative seven times negative one that would turn out to be seven, the absolute value. ",
                    "",
                    "Let's take a closer look on how we can implement this kind of a behavior. So up to now we have seen some features of a C++. We've talked about a lot of data types and how they are represented; we've talked about several kinds of expressions, such as arithmetic expressions, boolean expressions. In terms of control flow, the only flow we are used to working with is a sequential flow, which is the default by which programs are executed, basically meaning that instructions are executed one after the other in a sequential order. ",
                    "",
                    "Let's see if we can use all that in order to implement the absolute value or calculate the absolute value of an integer and then if we take a closer look here it seems that we need some more power in order to do that. Because it seems like that in some cases we want to keep the input as it is in case the input is positive in other cases we want to multiply it by negative one in case the input was negative and a sequential flow, that all instructions are kind of executed one after the other, won't be good enough. Because sometimes you want to do one thing in one and sometimes want to do another things. For that, we will introduce a more complex kind of a control flow and that would be branching flow. That would allow us to execute some instructions on one execution and to execute other set of instructions in another depending on the input or depending on some stuff. Let's show you a few kind of branching statements. The first one would be an IF and let's take a closer look on an IF statement.",
                    "",
                    "Syntax and Semantics 2.1 ",
                    "So, let's take a look at some IF statement. So, actually there are few kinds of IF statements, there would be: a one-way if statement, a two-way if statement, and also a multi-way IF statement. Let's start with a one-way if statement. For each kind of control flow I'll show you two things: first, I'll talk about the syntax, how we can create a legal kind of statement in C++ of that form, and then we'll talk about the semantics, what does it mean or how the compiler or the C.P.U. would execute this kind of statement. ",
                    "",
                    "Let's start with the syntax: the syntactic rules of creating such statements. So, we have the program that goes up to the IF keyword. Then, we have the if keyword followed by a condition and closed in parenthesis. Now, it's very important that we put the condition inside parenthesis, the compiler won't allow us to write it differently. It is a very strict rule here. The condition should be a Boolean expression, basically evaluating to True or false. ",
                    "After we have the IF and the condition, next line comes the body of the IF statement. It is taken one tab to the right. And after that the program just continues. So we have our program before the IF statement then the word if followed by a condition in parenthesis and then pushed one tab to the right we have the body followed by the remaining code of our program. One thing syntactically I want us to note is each expression we know in C++ ends with a semi-colon. So the expressions before the IF statement obviously end with a semi-colon, the expressions after the IF statement obviously end with the semi-colon. Also the IF body, the statement inside the IF, also ends with a semi-colon. But the first line where we have if and the condition in that line we don't have a semi-colon and it's very important that we don't have this semi-colon over there. So that's basically the syntax rules how we can create a legal one-way IF statement. ",
                    "",
                    "Let's talk about what happens when we write this kind of a statement: how the compiler is going to understand and interpret this kind of a statement. So the program would be executed up to the IF statement. When we reach the IF statement, the first thing that happens is the condition would be evaluated. Then since the condition is a Boolean expression, there are two cases: in case one case is where the condition evaluates to true, the other is when the condition evaluates to false. ",
                    "",
                    "Let's see what happens in each of these cases. So, in the case of the condition is evaluated to true, the body would be executed and then the program just continues. In case the condition evaluates to false, the body would not be executed and the program just continues with its original execution. So, basically the body of the IF here is conditioned to be executed by the value of the Boolean expression that comes in these parenthesis. So, if the condition is true the body is executed, if the condition is not true the body is not executed. So, it is not a sequential flow were each of the expressions are anyway evaluated, the body of the IF is conditioned to be evaluated depending on the value of the Boolean expression.",
                    "",
                    "Computing the Absolute Value 2.2 ",
                    "Now that we have the one way if statement, let's try to implement this problem here that reads from the user an integer and prints its absolute value. Let\u2019s go and implement it.",
                    "",
                    "Computing the Absolute Value Implementation 2.3 ",
                    "Okay, so let's implement this program. Let's first prompt the user to enter an integer so C-out: \u201cplease enter an integer.\u201d And let's also break the line here. Then, let's read this integer from the user, so let's have an integer variable. Let's name it, I don't know, maybe \u201cuser input,\u201d because this is what the user inputs. So, let's just c-in into this user input variable. And now we need to calculate the absolute value of a user input. ",
                    "",
                    "So, as we said there are two cases: one is when the user input is positive, in this case the absolute value just remains whatever the input was, and in case the user input is negative, in this case we need to multiply it by a negative one. So, let's do something like IF and then that's have a Boolean expression here. \u201cUser input\u201d is less than zero then; in this case we need to multiply \u201cuser input\u201d by negative one. So let's do \u201cuser input\u201d times negative one, and let's set it back into the \u201cuser input\u201d variable. So, in this case we're kind of multiplying \u201cuser input\u201d by negative one and then setting the result back to the variable \u201cuser input.\u201d",
                    "",
                    "So, \u201cuser input\u201d basically changes its value from its original value to its additive inverse. So this case we have \u201cuser input\u201d equals \u201cuser input times negative one\u201d. And now we can just C- out, in this case, the user input value. Let's try to execute it. Please enter an integer: so, if we have negative seven, it would a print seven. By the way, it printed the program ended with exit code zero right here after the seven because we didn't break the line. So, let's break the line also, it would make it appear better. And let's also test a positive value, in this case seven and then we get the value seven back.",
                    "Okay, so it seems to be working fine. I want to improve it in two ways: the first one is not a great improvement it\u2019s just a syntactic shortcut here. Where instead of having user input equals user input times negative one, we can make it shorter and just have the user input times equals negative one. It's a syntactic shortcut of obviating variable value by applying some operation on its previous value, in this case user input would be multiplied by negative one and set back into the same variable, so the times equals operator does that. You can use this shortcut of an operator; there is also plus-equal, minus-equals, div-equals and so on. Sometimes it is easier to write it in this way. So that's one thing I wanted us to use. ",
                    "",
                    "The second thing is the output I was expecting out of this program was not just to print the absolute value of the input, to a print something more of the form like the absolute value of whatever original input was equals the absolute value. Not just a seven, but something like the absolute value of seven equals seven For that, we should print something like the absolute value of something and then another pipe here equals and whatever the absolute value is. So this is kind of the format I'm expecting here, like  pipe symbol and then the original input and then another pipe and then equals and then the value of the absolute value of the original input. The problem here is that we kind of change, in some cases, the original input. So if you do if it was positive it would remain the same, but if it was negative originally then we changed its value and in this point in our program; we don't have its original value anymore. In order to resolve this we can use two variables: one for the original input and one for the absolute value.",
                    "",
                    "So, in this case let's declare another variable. So we would have user input and, for short, \u201cAbs Value\u201d and then the user input would remain with the original input the user entered and the absolute value would contain the absolute value of the original input. So, to do that I would first initialize absolute value with input value or the user input. And then, in case the user input is negative then I would set the absolute value to be the user input times negative one. If we take a closer look here, we'll see that originally I set absolute value to be the user input. Let\u2019s add a semicolon here. And if it is a negative, it would change its sign, if it is not negative it would remain with the original user input. Now at this point, we can use user input, original input, it didn't change, and absolute value down here. ",
                    "",
                    "You can see that the names we chose for the variables represent exactly what they're holding. So, the user input actually does hold whatever the user entered that's why it is a good name for it; it was read from the user, input did not change or its value did not change. And then we're printing the absolute value of the user input and absolute value, actually contains the absolute value of this user input; it is better than what I\u2019ve have done in my previous version, where the user input changed its a value to be the additive inverse of its original value and then it does not contain the user input anymore. ",
                    "",
                    "Let's try to execute it now, make sure it works. So let's enter an integer, let's say negative seven and then it just prints the absolute value of negative seven equals seven. I see I'm missing the space here. Let's test it with a positive value. Let's say seven, works fine, let's test it with a zero; though I'm not expecting any issues. Yeah, seems to be working fine. ",
                    "",
                    "One-Way if Statements 2.4 ",
                    "So, we have the syntax of a one way IF statement, I want to show you an extension of the syntax. So originally or formally, we can condition the execution of a single expression. You can see that the body here contains a single expression. In case we want to condition the execution of several extensions, we use this syntax here: where we have a compound expression that is basically a set of instructions that are enclosed in curly braces. So, the syntax would go IF parenthesis and the condition in it, and then open the curly brace, we have a set of instructions, again pushed one tab to the right and close curly brace and then the program just continues. So, we kind of group together a set of instructions into a single compound expression using these curly braces syntax and this way we can condition the execution of a set of instructions. A lot of times we use this kind of syntax.",
                    "",
                    "Determining Parity 3.1 ",
                    "Let's try to write another program now. Let's try to write a program that reads from the user a positive integer and determines its parity: whether it's even or odd. So, for example the program can prompt for the user to enter a positive integer, the user would I know say seven, and then the program would respond by saying seven is odd. Let's see how we can, how we can implement this program. So, now that we have if statements it seems much more reasonable we can use the one way if statement in order to implement this program. But it would be better to use a two-way if statement in this case. I\u2019ll talk about the if else statement which is a two-way if. We\u2019ll implement this program and afterwards with a compare the if else implementation verses of a one-way if implementation of the same program. But let me first introduce you to the two-way if statement, the if else statement.",
                    "",
                    "Syntax and Semantics 3.2 ",
                    "Okay, so in order to use the if else statement, the two-way IF statement, the syntax would go this way; once again, I'll first talk about the syntax, then show you the semantics for how the compiler understands this kind of statements. So, the syntax is very simple it goes like that; we have our program, then comes the if keyword followed by a Boolean expression and enclosed in parenthesis, and then we have the if body, again pushed one tab to the right. After that we have the ELSE keyword and else's body pushed one tab to the right, after the if else statement the program just continues in its original alignment. That's basically the syntax. ",
                    "",
                    "The semantics is very straightforward here as well, so when we get to the if else statement, the first thing is the condition is evaluated. As we said the condition is a Boolean expression, so it has two values possible either its value is true or false. In case the condition is true, the if body would be executed and then the program would continue in its original execution. But if the condition would be false then the else's body would be executed followed by the execution of the rest of the program. ",
                    "",
                    "So basically based on the value of the Boolean expression, we either evaluate the IF body or the else body. That's why it's called the two way if statement; we either go one way with the IF body or the other way with the Else body, based on the value of the Boolean expression, based on the value of this condition. Once again this is the simplified version of the if else; if we want to use the more compound version, where we want to execute a few of instructions, a set of instructions in the true case, and the set of instructions in the false case, then we need to compound or to group together a set of instructions using curly braces. So, after the if in the condition we can have a set of instructions enclosed in curly braces as after the else we can have a set of instructions enclosed in curly braces.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 5,
        "module_name": "Branching Part 2",
        "file_name": "Module 5 Branching Part 2.docx",
        "transcript": [
            {
                "title": "Determining the Parity 3.3",
                "data": [
                    "",
                    "Classifying a character 1.1 ",
                    "Let's use all of these statements in order to implement the following problem; let's write a program that reads from the user a character and classifies it to one of the following: either lower case letter, or an uppercase letter, a digit, or a character that is not alphanumeric So, in the execution of the program could behave something like that first the user's prompted to enter a character, the user then says, I don't know says upper-case D. and then the program would respond by saying uppercase D is an uppercase letter. Let's try to implement.",
                    "",
                    "Classifying a character Implementation 1.2 ",
                    "Okay. So, let's first prompt the user to enter a character, so C-out: please enter a character, end-l and then let's read this character so let's have char variable, User-C-H think would be good name here. So User-C-H is the input the user entered. And now, we should figure out whether this character is an uppercase letter, a lower case letter, a digit, or a character that is not alphanumeric. In order to do that, first let's decide in what control flow we're going to use. It seems as if we have to choose one out of four cases here. So a multi-way if seems very reasonable so if else if else something like that should work, so I would definitely use a multi-way if here. So, that's one thing we needed to figure out, the other thing is the how to determine whether our user C.H. or input is an upper case or lower case or a digit.",
                    "",
                    "So if you recall characters are represented using ASCII values; let's take a second look here at our ASCII table. If you remember we said that all the lower case letters are continuous in this table, all the upper case letters are continuous and even all the digits are continuous. We know that we don't need to remember or to memorize the values of the lower case letters in the upper case in the digits. The only thing we need to know is that they are continuous and we can check whether our ASCII value is in the range of the lower case A and lowercase Z, or in the range of the Upper Case A to the upper case Z, or in the range of zero and nine and so on. Let's try to do that so. Let's do something like if and now let's try to check if our user C.H. is in the range of the lower case letters.",
                    "",
                    "So, let's do something like user C-H. is greater or equal to, and now I don't want to use the value of lowercase A I don't want to use ninety seven in this case. I can use the C++ literal for lower case A and that would be something like that. So if user C H. is greater or equal to lowercase A and user C H is less or equal to lowercase C that means that our ASCII value is in the range of the lower case letters. In this case, let's print that the user CH is a lower case letter and let's do the same or something similar, to the upper cases. So else and then I won't break the line here I would continue that if in the same line because I want to use the multi-way if and once again I'll check if user CH is greater or equal to upper case A and user CH is less or equal to upper case Z, then let's c- out that our user CH is an uppercase letter. And now let's also check if it is a digit. So, else if our user CH is greater or equal to zero and the user CH is less or equal to nine then we want to C-out that our user CH value is a digit. And, eventually if it is not either one of these then we should just print that this character is not an alphanumeric character.",
                    "",
                    "Okay, so let's try to test it out. Let's first enter a character, so let's do lowercase D. seems to be working fine, these lowercase letters. Let's try uppercase D also seems to be working fine let's try, I don't know 4, it says four is not alphanumeric character. I was expecting 4 to fall somewhere here and print that four is a digit. Try to think what's wrong here. So here we checked whether user CH is greater equal to zero or and less or equal to nine, User CH is basically the ASCII value of our character but then the ASCII values zero is not zero is forty eight and the ASCII value of nine is not nine, it\u2019s fifty seven.",
                    "So in all of these cases here up here this case here in this case here, we compared a character to a character, once again a character to a character. And it basically compared the ASCII value that is represented in here to the ASCII value of a lowercase A and so on here we are comparing a character to an integer. So that's fine with the compiler it would just implicitly cast this character into an integer and it would compare the ASCII value of this character to the integer value zero and the integer value for nine. So, in order for it to work we need to give here the ASCII value of zero and the ASCII value of nine, basically forty eight and fifty something. Again we say we don't want to memorize these values, so I can just use the character literal for zero and the character literal for nine and it would look something like that. Once again I want you to know that the integer zero and the character zero are two different things. The integer zero is a 4 byte 2s compliment representing the value of zero, where the character zero is a single byte representing the digits or the character zero that is represented by the ASCII value. In this case zero is forty eight so the binary representation of forty eight would be stored in this case. So let's try to execute it now. First let's see that this change didn't affect all other cases of lower case works and uppercase also works and now let's see what happens with digit four. Also works. And finally let's check something that is not alphanumeric, let's do the dollar sign, and that also seems to be fine. Okay. ",
                    "",
                    "Convert 24-hour to 12-hour 1.3 ",
                    "Let's try to write a program now that reads from the user a time, entered in a twenty four hour format and then the program would print the equivalent time in a twelve hour format. So for example the user would be prompted to enter a time in twenty four hour form, that it would they would say I don't know fifteen",
                    "colon thirty seven, and for that input the program would respond by saying that fifteen thirty seven is three thirty seven PM. So, basically we're converting time in a twenty-four hour format to twelve-hour format adding the AM PM and stuff like that. I'm sure you're familiar with these two formats, but let's take a second look here how these formats basically work.",
                    "",
                    "So in a twenty four hour format, we have all the hours going from running from zero to twenty three. The first twelve are considered to be A.M., so zero to eleven are considered to be AM. Where twelve to twenty three are obviously considered to be PM, so that's the period value. Regarding the twelve hour format an hour or so in the first half of the day the time basically remains, so one in the twenty four hour format is one AM, two is two am, three is three am, and so on up to eleven which is eleven am. Where in the second half of the day then the time doesn't really stay the same, we kind of shift it twelve hours back so thirteen is considered to be one pm, fourteen is considered to be fourteen minus twelve which is two pm, fifteen is three pm and so on up to twenty three which is eleven pm. ",
                    "",
                    "So it seems that the first half of the day the hour basically stays the same and the second half of the day the hour is shifted back twelve hours, twelve numbers before. That's almost true, there are two exceptions here, in the first half of the day, zero is not zero A.M. So, zero doesn't stay the same just as one stayed one A.M. and two stayed two AM. Zero is twelve A.M. So it has different behavior here, same thing regarding twelve or noon, twelve doesn't become zero P.M. is not shifted back twelve hours. Twelve kind of remains twelve P.M., so most of the hours in the first half of the day remain the same besides zero.",
                    "Most of the hours in the second half of the day are shifted back twelve hours, besides twelve P.M. So, this is basically how the twenty four hour and twelve hour format works. Let's go ahead and implement this program now.",
                    "",
                    "Convert 24-hour to 12 hour Implementation 1.4 ",
                    "So let's start by prompting the user to enter a time in a twenty four hour format so C-out: please enter your time in a twenty four hour format. Let's break the line here, and now let's read the time so the user would print something like fifteen colon thirty seven. So we would need an integer to read the fifteen an integer to read the thirty seven and then we would also need to get rid of the colons here. We're not going to use it but the user would definitely print it. So we would need two integers. So let's say integer: the fifteen would be an hour in a twenty four hour format so I'll name it hour twenty four, the thirty seven would be minutes so I'll name it minutes twenty four and we\u2019ll also need a character, so let's define character; I'll just name it temp because it's not going to be used besides just to be read. Let's C-in this into hour twenty four into temp and into minutes twenty four. So, we have our input hour twenty four and minutes twenty four and now we're basically supposed to convert it into a twelve hour format. ",
                    "",
                    "For that we would need the initialize actually three parameters: we would need the new hour, the new minutes, which would basically be the same minutes, but we would also need the AM PM. So, for the hour and the minutes we would have, let's say hour twelve and minutes twelve. For the AM PM, we would store it in a string variable, for that I would include the string library and then I can create a string variable; I'll name it period, so we have the period. So we have basically all the variables to hold the result, the hour twelve the minute twelve and the period. And now we should decide in what kind of a control flow we're going to use to set all these variables.",
                    "",
                    "Let's take another look here at the conversion table. And I'm kind of wondering whether we're going to use multi-way IF, a two-way if, and so on. At first it seems like we have, maybe four different cases the case of zero, the case of one to eleven, the case of twelve, and the case of thirteen to twenty three so maybe a multi-way if would be a good choice here. But if you recall how we kind of thought of it when we introduced this conversion table. We first kind of determined whether it is am and pm so it's kind of a two way choice here, and then for each one of these we thought if we were in this range or in the zero if we're in this range or at the twelve. So it seems more logical to first determine one out of two cases, the AM PM, and then inside it nested in it, to determine what would be the twelve hour format for the hour. So my choice here would be a two way if instead of a multi-way or four if else statement but then a multi-way if would also work and be fine. But I would implement it using two-way IF statement. So for that I would do something like if and then I would ask if the hour twenty four is in the range of greater or equal to zero and hour twenty four is less or equal to eleven so if we are in the first half of the day this is what we're going to do.",
                    "",
                    "It\u2019s going to be a lot of stuff so I'm creating a compound expression. Otherwise, we'll do some other stuff.",
                    "So this would be the basic structure of our program; one thing is that the minute in the twelve hour format is basically the same as the minutes in the twenty four hour format so before even testing and determining in which part of the day we are, I can set the minutes twelve to be the same as many twenty four so that goes anyways. If we're in this part of the day let's set period to be AM, if we're in the second half of the day let's set period to be PM. So, basically up to now we said something like, we read the time in a twenty four hour format and we said the minutes in the twelve hour format to be the same as the minutes we'd just read from the user and then we checked in which half of the day we are setting the period variable accordingly.",
                    "",
                    "Now we just have to set the hour twenty four based on which part of the day we are so let's do something in order to determine what the hour twenty four value is. So let's take a second look here. So in the first half of the day the hour twenty four is the same the hour twelve is the same as the hour twenty four besides the case of hour twenty four equals zero. So let's ask if hour twenty four equals zero then hour twelve is set to twelve, otherwise our hour twelve is the same as hour twenty four right. So in the first half of the day only the case where hour twenty four is zero then hour twelve is twelve, otherwise it's the same as hour twenty four in the second half of the day.",
                    "Once again, second of all the day basically hour twelve is shifted back twelve hours before the hour twenty four value. So let's one again once again check if hour twenty four equals twelve then our hour twelve would be set to twelve, otherwise our hour twelve would be hour twenty four minus twelve or shifting it back twelve.",
                    "",
                    "So it seems like we're setting minutes twelve anyway before the IF statement the, if statement determines whether period would be set to AM or PM. And that if the main if statement also determines whether we're going to set our twenty four using this logic or using that logic. Each one of these logics decides whether to set hour twelve one way or another. After we're done doing all of that we are on the ready to create the output message. So we want to say something that I don't know fifteen thirty seven is three thirty seven P.M. So hour twenty four colon minutes twenty four is and then hour twelve colon minutes twelve and let's break the line. Okay, let's try to execute it. So please enter a time in a twenty four hour, let's do fifteen thirty seven, and then it says fifteen thirty seven is three thirty seven. ",
                    "",
                    "We forgot to print the period value. So that after printing minutes twelve let's also add space and then print period which would be AM or PM. Let's test it now. So fifteen thirty seven, now it says that fifteen thirty seven is three thirty seven P.M. Let's try to do all four cases. Let's do twelve thirty seven twelve, thirty seven is twelve thirty seven P.M. That's true.",
                    "",
                    "So we've tested a value in this range, we've tested a value here in twelve, now let's do two AMs; one in one to eleven and one with the zero, basically testing all the branches, all the possible branches of this program. So let's do zero thirty seven, it would say that zero thirty seven is twelve thirty seven am. ",
                    "Three thirty seven and then it would say that three thirty seven is three thirty seven AM. Okay, so it seems that all four branches work properly and yes, that's it.",
                    "",
                    "Switch Statements 2.1 ",
                    "We have a few branching statements: we have a one way if, a two way if, multi-way if. I want to show you another kind of a branching statement; it's also a multi-way branching. It's called a switch statement and it goes something like that. Again, let me first talk about the syntax the rules of how to create a valid switch expression, and then let's see how the compiler basically interprets and executes this kind of statements, what\u2019s the semantics of this expression.",
                    "So let's start with the syntax we have our program up to the switch statement, then we have the switch keyword and then we have a numeric expression basically an arithmetic expression enclosed in parenthesis. Then, we have a compound expression basically a body enclosed in the curly braces. ",
                    "That has a few case clauses and the default clause; each case clause starts with the keyword case followed by a constant value and colons, and then shifted another tab to the right we have the statements that are condition followed by a break and the semi-colon ending this case. And then another case, another constant colon, body break, and so on. We can have as many cases as we want and then finally would come the word default and that again some statements pushed one tab to the right, with the body and the break. ",
                    "",
                    "You can see that here we are kind of conditioning execution of some statements but we're not grouping them in the curly braces, we start their group with colons and a group with a break. This is how we kind of specify the instructions that are conditions. So this is the syntax, the semantics goes something like that. When the execution reaches a switch statement, first the numeric expression is evaluated. A numeric expression is not a Boolean expression; its value is not true or false, it's a numeric value to be seven six point five or whatever. So we have a numeric value, and then this value is compared to the constants that come in the different cases. So first the numeric expression value is compared to constant one, if they're equal if the numeric expression is the value of constant one, then these expressions would be executed. The break would break out of the switch statement and the problem would continue. But if the numeric expression is not equal to constant one, the numeric expression is compared to constant two if it is equal to constant two this statements of the body would be executed break would break us out of the switch statement and so on. So the numeric expression would be compared to the different constants one after the other. If none of the constants are equal to the numeric expression, then the default body would be executed break would break us out of the switch statement. So you can see that we have a muti-choice here depending on the value of the numeric expression and the values of the constants. One of them would be executed; either the constant one body, or the constant two body, or maybe the default body would be excellent. So it's a multi choice branching statement that we get here out of the switch.",
                    "",
                    "Computing Value of a Simple Expression 2.2 ",
                    "So let's try to use a switch in order to solve of the following problem. Let's write a program that reads from the user a simple mathematical expression. We will allow only an addition difference division or multiplication expression and then we'll print the value. For example, the user would be prompted to enter an expression of the form argument operator argument, and then the user would say I don't know five point two times four and the program would respond by saying twenty point eight which is five point two times four. Let's try to implement this program using switch. Now we don't really need to use switch here we can definitely use the if-else statement or other kind of branching statements, but let's try to use switch anyway. Let's go ahead.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 6,
        "module_name": "Loops",
        "file_name": "Module 6 Loops.docx",
        "transcript": [
            {
                "title": "Motivation 1.2",
                "data": [
                    "Hi there hope you are having a great day. Let\u2019s recap what we\u2019ve seen so far. So we\u2019ve talked about three fundamental constructs of C++. We\u2019ve talked about data expressions control flow. Regarding data we\u2019ve covered a few built in data types such as int unsigned int float double char bool all of that. Regarding expressions we\u2019ve talked about arithmetic expressions we\u2019ve talked about Boolean expressions. And when talking about control flow so we\u2019ve seen the default sequential flow where statements are executed one after the other and we\u2019ve also talked about branching such as IFs IF ELSE and so on. Let\u2019s try to use all of that in order to solve actually it seems a very simple problem. We would read positive int n from the user and print the numbers from one up to n. So for instance if the user enters I don\u2019t know 4 then the program would respond by printing 1 2 3 4. Ok let\u2019s try thinking how we can do all of that. So it seems like sequential flow can\u2019t really work so we can\u2019t do cout 1 cout 2  cout 3 cout because we don\u2019t know where to stop. And actually branching also seems kind of impractical. I don\u2019t know maybe we can do something like if n equals 1 cout 1. Else if n equals 2 cout 1 and 2. Else if n equals 3 cout 1 2 and 3 and so on but since our program needs to have a finite amount of lines of code we can\u2019t make it work for all inputs. So what we have so far seems not enough in order to implement this kind of simple requirement. And to do that we would need a new kind of control flow that would help us iterate and repeat statements over and over. And that\u2019s basically what we are going to be talking about today. I am going to show you two statements for repetitive executions for iterative executions. The first one is going to be a while.",
                    ""
                ]
            },
            {
                "title": "While Loops 2.1",
                "data": [
                    "Ok so the syntax of the while statement goes like this. So you have a few statements then comes the while keyword and then enclosed in parentheses you have a condition. And as a compound statement you can have a body of statements that are tabbed one tab to the right. After the while you have your program just continues at the original alignment. But that\u2019s basically the syntax of a while expression. Let\u2019s talk about the semantics how a while expression is basically a while statement is basically executed. So the program is executed up to the while statement. When it reaches the while statement the first thing that happens is that condition is evaluated and then actually there are two cases. Either it is true or false. If the condition is true the body is executed and the condition is reevaluated. Again two options either true or false. If it is true body is executed condition evaluated once more. True body condition. True body condition evaluated. When the condition turns false the program just continues in its original path. So basically while the condition is still true the body is executed over and over and over. That\u2019s why it is called the while statement. While the condition is true we keep repeating the execution of the body over and over. When the condition turns false we break out and continue with the flow of our program. So that\u2019s basically the semantics of a while statement. This is syntactically the extended version of a while. The simpler version just as we had with the Ifs doesn\u2019t require a compound statement. You can have a single statement body then you don\u2019t need the curly braces to enclose this statement in it.",
                    ""
                ]
            },
            {
                "title": "Solve Let\u2019s Count Program (using While) 2.2",
                "data": [
                    "Ok let\u2019s then try to use this while statements this iterative capability that we just talked about in order to solve our counting program. Let\u2019s try and think how we can use the capability of repeating some statements over and over in order to print numbers let\u2019s say from 1 to N. So we can maybe maintain some kind of a variable a counter that would be initialized to one printed and incremented. Printed and incremented. Something like that we will start counter as one and we will print one and increment counter to two. We will print two and we will increment counter to three. Will print three and increment counter to four. So basically we are repeating two statements here print the value of counter and increment it. Print the value of counter and increment it. If we do it a right amount of times we will basically print numbers from 1 to wherever we decide to go. Let\u2019s implement it using C code now. ",
                    ""
                ]
            },
            {
                "title": "For Loops 3.1",
                "data": [
                    "Ok so that was kind of cool being able to repeat stuff over and over. We can do that using a while statement. There is another control flow for iterative statements which is called the for statement. Let me again show you the syntax and the semantics of for statements. So syntactically it goes something like that you have your program and then you have the for key word. Enclosed in parentheses you have three parts that are separated by semi colons. You have the initialization statement semi colon condition statement and semi colon and increment statement. So you have these three parts that are enclosed in parentheses. After that you have compound statement of the body of the for loop which is again spaced one tab to the right that\u2019s basically the for statement. After that we have our program that continues at the original alignment. Semantically the way a for loop basically works is something like that. You execute your program up to for statement and then when the execution reaches the for statement first thing it does is executes the initialization statement one time. After executing this statement the condition is evaluated two options either it is true or false. If the condition is true the body is executed followed by the increment statement. Once again the condition would be evaluated. If it is true body increment. Condition is evaluated if it is true body increment. Condition is evaluated if it is false the for basically ends and the program just continues with its original execution. So that\u2019s for syntax we have the simplified for where you don\u2019t need the compound statement you can have a single statement that would be repeated each iteration. Then you won\u2019t need the curly braces. At first it seems like for statement are much more complex than while as we practice to use fors you will see that they have their advantages. They are sometimes more readable and preferred over whiles. But we will have to get used to that.",
                    "",
                    "Solve Let\u2019s Count Program (using for) 3.2 ",
                    "So now let\u2019s try to solve the same problem here of counting from 1 to N but this time let\u2019s do it using a for loop.",
                    "",
                    "Counting up using for.mp3 ",
                    "Ok then let\u2019s try to count up now using the for loop. So once again let\u2019s first ask the user to please enter a positive integer let\u2019s read this input so we declared an n and read into that n and now we\u2019re supposed to start counting from one to n. Once again we\u2019ll create this counter variable but now that we have a for loop as we said there are three parts of this for statement there is the initialization the condition and the increment. So we can initialize counter to one we want to keep repeating as long as counter is less or equal to four. And then we want to repeat two things we want to print the value of counter and we want to increment counter. Instead of incrementing counter here inside the for loop we can increment it in the increment position so counter plus plus goes over here. Let\u2019s try to trace this execution so once again we have our two variables n and counter. Let\u2019s assume the user enters four for n. And first we initialize counter to one we check whether the Boolean condition is true or false if counter is less than four in this case it is. So we are printing one the value of counter. And after executing the body we execute the increment statement basically increasing counter to two. We then check whether the Boolean expression is still true it is so we print value of counter and we increment counter to three. The Boolean condition is still true so we print the value of counter and increment counter basically turning it to four. The Boolean expression is still true so we are printing the value of counter printing four and incrementing the value of counter basically incrementing it to five. Now the Boolean condition is false we are breaking out of the for loop we can once again return zero and end this execution. ",
                    " ",
                    "Let\u2019s Count Problem- Comparing the Two Solutions 3.3 ",
                    "If we take a minute to compare these two implementations of the counting up program. One using the while loop the other using the for loop we\u2019ll see ok so obviously they both read an n from the user. But we see that they both implemented the same idea of maintaining a counter initialize to one and repeating print statement of the current value of counter and increasing it over and over four times. The difference as you can see is the location or the way this counter is basically managed in each program. So they both initialized counter they both go up to when counter is less or equal to four. They both repeat printing the value of counter and incrementing it. In the while loop the management of this counter is split into three locations one for initializing one for checking the condition and another for the increment. Where in the for loop they are all kind of binded together in the for loop header where we do all the management of counter in a single location in a single place. We initialize we check the condition and we increment all at the same place. That is one of the advantages of a for loop over a while that it is in that way more readable to a programmer that you can easily see how this loop is basically managed. ",
                    ""
                ]
            },
            {
                "title": "Counting & Summing Digits 4.1",
                "data": [
                    "Let\u2019s try to solve another program using iterative statements. Let\u2019s try to solve a problem that we first read a positive integer from the user. And then try to calculate the number of digits in this number and what the sum of these digits. For example we can ask the user to enter a positive integer. The user would enter let\u2019s say 375 and the program would respond by saying that 375 has three digits. Yea because 375 has three digits and their sum is 15 and that\u2019s because 3 plus 7 plus 5 equals 15. So given a positive integer we would have to figure out how many digits are there in this number and what their sum is. Let\u2019s try to think how we can do that kind of a thing. So for that I think we should try to iterate over the digits each time we can count a digit and add it to some accumulator that would hold the total sum. So we would need two variables count digits variable and a sum digits variable. We would initialize both of them to zero and then we would start iterating over the digits one by one. We will start let\u2019s say with 5 and for each digit we will count it and we will add it. So counting 5 would turn count digits to 1 and adding 5 would turn sum digits into 5. And then we would go over the next digit 7 we will count it and add it would make count digits 2 and sum digits would be 12. Five plus seven. Next digit we will count it makes count digit 3 and we will add it makes sum digits 15. We will repeat this process over the digits each time counting it and adding it. Two things that are basically repeated. That\u2019s the basic idea of counting and summing up digits in the number. The only challenge we have left is figuring out how to iterate over the  digits. We can\u2019t access each digit separately so we can think of mathematical property of integer numbers that can come in handy for this. And that\u2019s for example if we take 375 and divide it by 10 we will get 37 and  the remainder of 5. We can then take 35 mod it by 10 to get the 5 and 375 div 10 would give us the 37. Basically splitting the ones digit from the rest of the digits. This way we can isolate the ones digit work and accumulate its data and then remove it and keep working with the rest of the digits. This trick here works not only for three digit numbers it works for any length of a number. For example let\u2019s take a four digit number 2375 divided by 10 we will get 237 and the remainder of 5. Once again 2375 mod 10 would give us the ones digit the 5. 2375 div 10 would remove this 5 would of the number and we will be left with 237. This thing here as we said works for any length of a number. Taking n plus 1 digit number an to a zero mod 10 would isolate the ones digit a0. And div it then by 10 would isolate the rest of the digits basically removing a zero from the number keeping us left with an to a1. Let\u2019s take all of that together in order to better explain or better understand the process of counting and summing up the digits. So we said we have our number 375 both accumulators count digits and sum digits that would be initialized to zero. We would also maintain the current digit for each iteration. And then each iteration we would isolate the digit count it add it and remove it. For example first iteration we would isolate ones digit the 5 we will count it into the count digit we will add it to the sum digits and we will remove it from the 375. Keeping us or having us left with 37. And then again these four statements would be repeated. Isolating the ones digit the current digit would be 7. Counting it adding it and removing it. And the four statements would be repeated again taking out the ones digit counting it summing it and removing it. If we keep on the repeating these four statements over and over basically we are iterating over the digits one by one counting each digit and accumulating its sum. Let\u2019s try to implement that using C++.",
                    "",
                    "Counting and Summing Digits 4.2 ",
                    "Ok so let\u2019s try to write this program. So let\u2019s have our main let\u2019s keep some space for variables. First we need to read the input from the user let\u2019s ask the user to enter a positive integer so enter a positive integer let\u2019s also break the line. And then let\u2019s read the user\u2019s input so let\u2019s have a num integer variable let\u2019s cin into this num. And now we should start iterating over the digits of num accumalting their sum and counting each digit. We said we were going to use two accumulating variables one for the amount of digits one for the sum of the digits. So let\u2019s declare sum digits and count digits these would be our accumulating variables. Before we start iterating we have to initialize them so let\u2019s set sum digits to zero and also count digits to zero and now we should start iterating. We said since we don\u2019t know how many iterations we are going to have we\u2019ll use a while loop here so while again let\u2019s keep the put in condition that controls the while loop for a later stage. Let\u2019s first figure out the statement there are going to be repeated each iteration. So we said that each time we want to extract the ones digit add it to the sum digits and count it over and over. So let\u2019s have a current digit variable that would hold the current digit we are dealing with in this iteration so here is the current digit. So first thing we want to do is take out the ones digit. We said we were going to use mod 10 for doing exactly that so current digit gets num mod 10. So now that we have the ones digit we should count it and add it to sum. So we should increase count digits let\u2019s increment it plus plus and let\u2019s add a current digit value into the sum digits variable. Sum digits plus equals our current digit. After doing that we need to remove the ones digit from num we said we\u2019ll do that by diving num by 10. So let\u2019s set num to the new value after removing the ones digit. So let\u2019s set nums digit equals num div 10. So these are basically the four statement we want to repeat each we want to isolate the ones digit we want to count it we want to add it and we want to remove it from them. Over and over this way we are accumulating count the digits fro num. So that basically seems to be the statements inside the body of the while in order to figure out what is the Boolean condition that controls this while loop let\u2019s try to trace this execution and during this trace we\u2019ll try to figure out how to phrase the Boolean condition. So let\u2019s have our memory we have a few variables we have num sum digits count digits and current digit. First we ask the user to enter a positive integer the user enters let\u2019s say 375 then we set some digits to zero and count digits to zero and now we start iterating. So let\u2019s assume the Boolean condition is true and start repeating these four statement over and over. So first iteration current digits gets num mod 10 375 mod 10 with a remainder when we are dividing 375 by 10 would be five. So our current digit would be five which makes sense that\u2019s exactly what we wanted to do. Then we are counting this digit basically incrementing count digits and we are adding five to the value of sum digits basically turning sum digits to be five. After we are doing that num becomes num div 10 so first we evaluate num div 10 which is 37 and num then is assigned to 37 basically removing the ones digit from 375. That ends the first iteration. Second iteration once again current digit is 37 in this case mod 10 that would be 7 right the remainder when we are dividing 37 by 10 is 7. We are counting this digits basically incrementing count digits and we are adding seven to the sum digits that would make sum digits be 12. Last thing is num div 10 so when 37 is divided by 10 we get 3 and we still want to have one more iteration for adding this 3. So current digit would be num mod 10 just so we won\u2019t make any mistakes when we divide 3 by 10 we get 3 so the mod would return 3 and that\u2019s exactly what we want current digit to be. That\u2019s the digit we are going over this iteration we are counting it basically incrementing count digits and we are adding it to sum digits making sum digits 15. Finally we are assigning num to num div 10 and num div 10 in this case is zero so that completes the third and hopefully the last iteration of the while loop. And I think at this point we should be able to say what the Boolean condition for the while in order to control this statement to have only three iterations in case of 375. So it was supposed to be true up to now and it is a Boolean condition should turn to false exactly at this point because we don\u2019t want anymore iterations. So we see that when num turns to zero that\u2019s where we want to stop we don\u2019t want to have more iterations basically when num is positive we want to repeat these three statements. When it was 375 we want to make our first iterations when it was 37 we wanted to make our second iterations and finally when it was 3 we wanted to make our last iteration. When it turned zero we wanted to break out of this while loop so while num is positive we want to repeat when we break out of this loop now we have the number of digits and their sum at our possession so we just need to announce it to the user. So let\u2019s cout let\u2019s say something like 375 has three digits and their sum is 15. So it would be num has count digits digits and their sum is now let\u2019s just strain the value of the sum digits variable sum digits let\u2019s break the line here return zero and that basically concludes this execution. ",
                    "",
                    "Computing the Average 5.1 ",
                    "We\u2019ve talked about for loops while loops iterative statements. Let\u2019s look at another problem now. Let\u2019s talk about how to compute the average of a collection of numbers. For example let\u2019s assume the user enters a sequence of grades and calculates and prints the average of the sequence. Let\u2019s try to aim for a behavior that looks something like that. At first the program would ask the user please enter the number of students you have in the class. The user can say let\u2019s say 4 and then the program would ask the user please enter the students\u2019 grades separated by spaces. The user would then enter four grades for example 71 86 68 94. And eventually the program would respond by saying what\u2019s the average of this sequence. In this case the class average is 79.75. So in order to implement this problem here we need to calculate an average. And in order to calculate the average basically you need to sum up all the numbers and divide it by the amount of numbers. That would give you the average. So to do that we know how many numbers are going to be that\u2019s the first input we got from the user. The number of students in the class but in order to figure out the sum of the numbers the sum of the grades we would need to accumulate them in some kind of sum variable. Just as we did in the calculating the sum of the digits of a number. So basically we are going to repeat here two things over and over. We will read an input and add it to sum. Read an input and add it to sum. Read an input and add it to sum. Over and over for each grade the user enters. After we will finish up reading and accumulating the sum we will have a variable containing the total sum which we can then divide by the amount of numbers to figure out what the average is. Let\u2019s get into the details of this implementation.",
                    ""
                ]
            },
            {
                "title": "Computing the Average Screen Share 5.2",
                "data": [
                    "First we need to read the number of students. So let\u2019s create a variable num of students. And let\u2019s ask the user please enter the number of students in class and end the line. And just read it into this num of students. Then we should announce to the user to start entering the sequence of grades. So let\u2019s say enter the students\u2019 grades. Let\u2019s also ask the user to separate them by space. Ok I think that\u2019s good enough instructions here for the user. Ok so now we should start reading the numbers and adding them up. For that we would need some kind of iterative process. Since we know exactly how many iterations we are going to have. We are going to have num of students iterations. A for loop here is a good choice. So let\u2019s create some counter variable initialize it to 1 and each iteration will just increment this count. And if we want to do num of students iteration starting a 1 incrementing it by 1 each time we just need to make sure that count is less or equal to num of students in order to keep on going. Let\u2019s just declare this count variable. So this for loop header here basically controls process that does num of students iterations. And then we said that each iteration we want to read an input. Let\u2019s read it into current variable and to add it to some accumulator. So we should let\u2019s first declare current. We also need some variable that actually should be initialized before we start iterating. So first it is initialized to zero and then each iteration we just add to sum the value of the current element we read. So if we take a second look at this piece of code here we first initialize sum to zero and then we start iterating num of students times. Each time we read an input and add it to sum over and over. Read an input add it to sum. At the end we would have the total sum of all the grades the user entered. We can then calculate the average as divide sum by a number of students. Let\u2019s create a variable for the average. Even though the grades are integers and number of students is integer. The average could be a number with a fractional part so for that I would define the average variable as a double. And then I would just assign average to sum divided by num of students. Then again a closer look here would suggest that the slash operator in the context of dividing two integers won\u2019t do a real division but would basically do integer division div. So in order to make it a real division we would need to cast both sum and number of students to double. Now average is the result of really dividing a sum by number of students. We should just now announce that the average is average is and then we will just print the value of the variable average and end the line. That basically should be the program. Let\u2019s test it out. Compile it and execute it. No bugs here which is good. Enter the number of students so we have four students. Enter the grades let\u2019s just do I don\u2019t know 89 75 93 and 78. And then the average is 83.75 I believe that\u2019s the average here. Looks good.",
                    "",
                    ""
                ]
            },
            {
                "title": "Computing the Average 5.3",
                "data": [
                    "We\u2019ve calculated the average which was a cool thing. But taking a closer look here the behavior of this program we can see that we are asking the user to say in advance how many numbers are going to be in the sequence. Which is maybe fine in this case but in a lot of other cases it is a not so user friendly and sometimes it is basically impractical for the user to know in advance how many or how large it is the input. A lot of times we just want the user to keep on entering the input as they find it and to signal in some way that they are done entering the input. Let\u2019s revise our program a bit to allow this kind of behavior. Let\u2019s ask the user to enter a sequence of grades and just type negative one which is obviously not a valid grade to signal that the input is over. So the behavior should be something like that. First the program would say what the instructions are. Enter the grades separated by space and your sequence by typing negative one. Then the user would start entering the sequence of grades in this case 71 space 86 space 68 and so on 94 and then negative one which is just saying I am done entering the grades. The program would then know to announce the average which is in this case 79.75. Let\u2019s take a minute and think how we can implement this kind of a behavior. It seems like we can\u2019t use a simple for loop here as we have used before going with a count from one to the amount of numbers in our sequence. Just because we don\u2019t know how many numbers there are going to be in the sequence. But we still want to do some kind of repetitive process of reading and accumulating reading and adding reading and adding. In order to calculate  the total sum of the numbers. So it seems like a for loop won\u2019t be the control flow we are going to choose. In this case we are going to choose a while loop. And another technique I want to show you here is the use of variables which are named flag variables. Flags are basically two state variables they are either down or raised or up. We can use them in this type of situation something like that. We will initialize the flag to be down and then each time we read an input we will add it by the way we also need to count the number of elements in order to or be able to divide the sum by the amount. In this case we would need to accumulate both the sum and a count of how many elements there are. So each iteration we will read a number add it and count it. Read a number add it and count it. The flag would remain down as long as we read grades. We read a grade add it and count it. Read a grade add it and count it. As we see negative one we just raise the flag basically signally to break out of this repetitive process and to calculate the average. At that point in time we would have accumulated the total sum we would have accumulated the number of elements that the user entered. And then we would be able to divide them one by the other. So that would be the big idea here of our implementation. Just one thing before we move on really implementing is how to implement this two state variable. And it seems very reasonable to use Boolean typed variable because a Boolean also has two states false and true. Basically corresponding to the two states of a flag, down and up. So the down would be let\u2019s say false and up would be true and then using this Boolean variable we can use it in the while condition to control this repetitive process. Let\u2019s go down and implement this here.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 7,
        "module_name": "Algorithm Analysis",
        "file_name": "Module 7 Algorithm Analysis.docx",
        "transcript": [
            {
                "title": "Computing the Average Screen Share 2 5.4",
                "data": [
                    "",
                    "Primality Testing 2.1 ",
                    "Hi there hope you are having a great day. Today we are going to talk about how to analyze algorithms. We talk about correctness of algorithms and how to assess the resources algorithms used. Whether it is time memory network or stuff like that. We will focus mostly on time but same concepts would work for other resources as well. We will start by talking about the problem of testing whether a number is prime or not. Before we look at the problem definition let\u2019s look at some other two definitions. First is defining what is a prime number. I believe you probably know that but let\u2019s make it formal. So assuming we have an integer num that is greater or equal to two. We say that num is prime if its only factors are one and num itself. So if it divides only by num and one that would make it a prime number. For example 13 is prime because 13\u2019s only factors are 1 and 13 no other number divides by 13. On the other hand 12 is not prime because besides 1 and 12 for example 6 is a factor of 12. That would make it not a prime number. Another definition I want us to note here is complimentary dividers. The definition goes something like that again let\u2019s have an integer number num greater or equal to two. And let\u2019s assume d and k are two dividers of num. We would call k and q complimentary dividers of num if when we multiply them we get num. So if d times k equals num. For example for num equals 100 4 and 25 are complimentary because if you multiply 4 by 25 you will get 100. And so are 5 and 20 both are dividers if you multiply 5 by 20 you will get 100. That would make 4 or 25 a complimentary couple of dividers and 5 and 20 complimentary dividers. For example 2 and 10 which are also two dividers of 100 are not complimentary dividers because if we multiply 2 and 10 we won\u2019t get 100. So let\u2019s have these two definitions we will use them later on. And let\u2019s take a look at the problem of testing primality. So let\u2019s write a program that reads from the user an integer let\u2019s assume that it is greater or equal to two. And the problem would need to determine whether this number is prime or not. So an execution could look something like. Assuming we have we will ask the user to enter an integer greater or equal to two. The user would enter 911 and then the program would respond by saying 911 is a prime number. You can try it out and check that 911 is indeed a prime number. Ok let\u2019s go ahead and solve it.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 8,
        "module_name": "Functions Transcript",
        "file_name": "Module 8 Functions Transcript.docx",
        "transcript": [
            {
                "title": "k-Combinations Problem 1.2",
                "data": [
                    "Hi there hope you are having a great day. Today we are going to talk about functions. Let\u2019s start by first calculating the k combinations. Now let\u2019s start with the definition what are k combinations. So let n and k be two integers non negative integers. And let\u2019s assume that k is not greater than n namely k is less or equal to n. So the k combinations are the number of unsorted selections of distinct elements from a set of elements. So assuming you have n elements in how many ways can you take k elements out of that. For example if n is five and k is three what are three combinations that case. So assuming you have a set of n elements in this case five balls a green one orange purple blue and red. Let\u2019s see in how many ways can we select three balls out of these five balls. So the first one can be I don\u2019t know take a blue purple and a green. So they aren\u2019t ordered it means that there is no difference between blue purple and green and green purple and blue. It is the same it is choosing these three balls. And that\u2019s another way. And there are no duplicates here. So we can\u2019t choose two purples or something like that. So we can do it this way that way we can do it in a lot of ways. The number of ways we can choose three balls out of these n out of these five are called three combinations. So we would like to figure out what are three combinations from a set of five. Luckily for us by the way it is also noted as n choose three that\u2019s another way to call it. And we can write it as like this big parentheses and then you have a n and a k under it. Luckily for us there is a theorem that tells us the value of n choose k. So let n and k be two non- negative integers such that k is not greater than n. The number of k combinations of a set of n elements equals to n choose k that equals to the factorial of n over the factorial of k times the factorial of n minus k. In our example five choose three or three combinations would be the factorial of five over the factorial of three times the factorial of two. Basically the factorial of five is one twenty over factorial of three is six times two that is one twenty over twelve and that is ten."
                ]
            },
            {
                "title": "k-Combinations Problem 1.3",
                "data": [
                    "Now that we have the definition of what k combinations are let\u2019s try to write a program that reads from the user two positive integers n and k. Assuming that k is not greater than n. And it would print the value of n choose k. So an execution can look something like that. First we would ask the user to enter n and k. The user can say five and three for example and then the program would respond by saying five choose three is ten. ",
                    "k-Combination Solution 1.4 ",
                    "So let\u2019s implement this program. So let\u2019s have our main. We are going to calculate n choose k by the formula of factorial n over k factorial times factorial of n minus k. So let\u2019s start first declaring n and k. Let\u2019s ask the user to enter n and k and read these variables n and k. Now we will need to calculate the factorial of n the factorial of k factorial of n minus k and do the calculation there. So in order to calculate the factorial of n we will define an accumulating variable n fact that would initially be one. And then we\u2019ll just multiply more and more values to it right. So we want to do one times two times three times four so on up to n. So we will use a for loop that would iterate one through n and each time it would multiply our accumulator this n fact by i. So first it would be n fact times one then n fact times two times three times four up to n. So eventually it would accumulate the entire multiplication of one through n. So at the end of this execution n fact would hold the factorial of n. Same thing we would do for k fact to calculate the factorial of k. So again k fact is initialized to one and then we would iterate one through k this time multiplying each value there. And same thing we will do for n minus k. So I named a variable n underscore k fact just because I can\u2019t use a dash in the variable name. But same thing n underscore k fact is initialized to one and then we are multiplying one through n minus k all the values in that range. After doing that we are collecting we are calculating the n fact over k fact times n minus k fact and we name that value as k combinations. Now a point here is that I made k combinations be an integer but then I am dividing here some values maybe it could be a double. So if we think about it a bit more deeply we will see that it cannot be a double since we are using the division of two integers it would be counted as div. But then it would not be a double since mathematically we can prove that this value here represents n choose k or in other words the number of ways to choose k elements out of a set of n. And that is always a whole value that is not can\u2019t be 3.5 ways to choose k elements out of n. That is an integer value so we can feel safe to declare k comb as an integer and do the math here using div in this case. So after calculating k comb we can print that n choose k is k comb. So we\u2019ve got that all figured out now take a look at these three parts here when we calculated the factorial of n the factorial of k and the factorial of n minus k. They look very similar to one another in one way and they are doing the same kind of calculation the same kind of process. Basically initializing an accumulator and iterating over a range each time multiplying the accumulator by the current value. Not only that it is basically calculating the factorial so you know let\u2019s that would be cool. Let\u2019s include the math library let\u2019s do something like include c math. And then instead of having n fact equals one and all of the for loop let\u2019s call the factorial function of math. Let\u2019s do n fact equals factorial of n. And same thing for k. K fact equals factorial of k. And same thing for n minus k. N minus k equals factorial of n minus k. And that would make our code first much more clear it basically speaks what we want to say. It says n fact equals the factorial of n we don\u2019t need to analyze a for loop and an accumulator to understand what they do. Basically we just read the words there. And yes so it makes it a lot clearer and we don\u2019t need to get into the details of implementation here. Unfortunately the c math library doesn\u2019t have a factorial function that\u2019s kind of unfortunate here. Luckily for us we are going to define our own factorial function just now. So yeah let\u2019s see how we do that.",
                    "Flow of a Program That Calls Functions 1.5 ",
                    "We want to create or define the function of fork Tauriel so we can use it three times here. No no we've already used functions we've been callers of function quite a few times already. We've called the square root function. You've called Also some other functions as well but that's going to be our first time that we're going to define our own function in this case it's going to be the fork Tauriel function.",
                    "So we want to define a function named fork Tauriel this function should get an integer as an input a number as an input should do some stuff and eventually return the fork Tauriel of number which is also an integer value in order to define this thing here a function that gets an integer and returns some integer we first defined the header of the function in a simple slot it looks like that in fact Oriya and then and closing parenthesis in parenthesis. We have in numb. Basically the name of the function is factorial the second word here in this line that's the place where we have the name of the function in the prentices we say what are the variables of this function in this case a number which is of type integer. So we are expecting to get an integer value locally will name it numb and on the left here we have the integer which is the type of the value returned by this function the calculation result of this function is an integer. So you have like if you read it in order. It starts this function is evaluated to be in into the return an int its name is for Korea and it would get a numb as an input and then you have a block of curly braces that basically init you have to say what. Is the implementation how do you take this input numb and calculate the fork Tauriel of numb. Out of it and here we have to write the code to calculate numb for Victoria given a value to numb. So the code is very similar to what we had previously Let's have two local variables factor is and I initially the factor as the cumulate are here would be set to one and then we'll iterate over the range from one to Naam right we want to calculate number for Tauriel So we'll have to rate from one to numb. Each time Multiplying factor is by the current value that we are iterating over. So first will multiply by one and then by two than by three than before and so on up to number. So after having these few lines executed factories would hold the factorial of numb. We want to return that as a result for that we use the keyword return that basically announces that as the output of the function so return fact or is that is the value the integer there is returned from this function so that is the definition of the factorial function. First we say that interface how to use this function call it for Tauriel pass an integer to it and then it should return an integer inside the curly braces we say what the implementation he is or basically how to calculate how to create this factorial resource for total integer out of the input number using this for loop eventually returning that value."
                ]
            },
            {
                "title": "Calling a Function Code 1.6",
                "data": [
                    "If we have our main here in addition to the main since we are calling factorial in this case even three times. We will need to give the implementation of factorial the function factorial. So now we have two things in our program. The main which is by the way seems very much syntactically as a function we will see a few words about it in one of our future modules. So we have the main function and then we have the factorial function. And let\u2019s see how this program basically executes. So first we read n and k from the user cout please enter n and k cin n and k. And then we have a call to factorial basically jumping to the code of factorial assigning num the local variable num with the value of n doing the calculation of n factorial. And returning res back to the main back to n fact. After doing that we call factorial again with the value of k associated to num. Jumping to the factorial call doing the calculations and going back to assigning the resulted value to the factRes back to k fact. And then we are calling factorial for n minus k jumping to factorial calculating the for loop a third time now and returning the result back to n minus k fact. And then we can calculate the combination there of n fact over k fact times n minus k fact into k combinations and printing the output. So you can see that we are calling this function factorial several times. Makes it very tempting to define this factorial function. We implement the algorithm of calculating factorial only once but we can use it over and over. One of the advantages of using a function. Also the name factorial is another great advantage here because instead of having a for loop three times we are having word name factorial appear three times. Make it much more readable and clear exactly what we are doing here. And this way we call this function over and over and calculate the factorial of n of k and of n minus k. ",
                    "",
                    "(Supplemental) ",
                    "Let\u2019s take a closer look here at this program. It seems like we are calling factorial over and over which is a great thing. Each time passing a different value as our local num. First time we are calling factorial for n second time factorial with k third time factorial with n minus k. So each time num our local variable and the factorial function gets a different value. For example if n and k are five and three first time we will call factorial with num equals five. Second time we will call factorial with num equals three and next in the third time we will call factorial with num equals five minus three which is two. And the factorial would be executed the right amount of iterations and each time when factorial is done it goes back to where we called it from and do the assignment. So basically each time we call factorial we jump to the beginning of the factorial function same position. And each time we return from factorial we go back to where we came from. So first time we called factorial we came back to the first assignment. Second time we called factorial we went back to the second assignment. And third time we called factorial we went back to the third assignment. You can see that using functions calling functions is some other way of controlling the flow of our program. It is not sequential right so when we are calling factorial we are not going right back or right to the next instructions. We are jumping to a totally different position in our code doing some instructions over there and then jumping back to where we came from. That happens or in order to do that we would need to figure out a way to manage this kind of control flow. So jumping to the beginning of the function that\u2019s easy. We always each call jumps to the exact same position in the code to the beginning of the function. But in order to return to the right place we would need some kind of mechanism to save and to store the position where we would need to jump back to. In a few minutes we will see exactly how we do that.",
                    "",
                    "Data Types, Expressions, Control Flow 1.7 ",
                    "So to put it in context you can see that functions are a new way of a control flow. So if like we have the sequential flow that our instructions are executed one after the other in a sequential order. Then we introduced branching where we could some executions do one lines of code and another execution jump to do a different set of instructions. Iterative statements that allow us to repeat the same instructions over and over. To jump back to a position we\u2019ve already been in our code and execute the instructions over and over. Function calls are none of these control flows are a different order that we are executing our instructions. When we call a function we jump to a totally new position in our code. Do or execute the instructions over there. And when the function ends using a return statement we are jumping back to where we came from. So that is a totally new control flow and in order to get a bit deeper understanding of what is going on inside the computer there. Let me introduce you to an execution model named the run time stack. And that is basically what happens inside of the computer during the execution time."
                ]
            },
            {
                "title": "Runtime Stack Execution Model 1.8",
                "data": [
                    "So let\u2019s take a look at this program here that we have same program that we had before. And let\u2019s execute or trace this execution using the runtime stack model. So assuming you have your memory here as we do a sequence of bytes a sequence of data. Let me show you how the runtime stack model works for executing this program. I believe you will get the hang of it as we go and you will be able to apply it to other executions as well. It would help you predict exactly what the output or how programs basically behave. So ok so we have our main program when we start executing the main we first have all our local variables stored in the frame of the main function. In this case we have n k kcomb nfact k fact n minus k fact. All of these variables we declared at the head over our main. So we have a place for all of these variables. You will see that the runtime stack helps us maintain the variables and manage the variables that are accessible in each in each position we are in the code. In the case of main these are the variables. Then for example let\u2019s start executing. So line four would print please enter n and k. Then we get to line five cin n and k let\u2019s assume the user enters three and five. So n would be equal to five. K would be equal to three. Then line six we have a call we have an assignment basically n fact equals the factorial of n. And in an assignment expression we first evaluate the right hand side and then the result would be stored in the left hand side of the assignment. So basically it is a compound expression that has two steps here. An evaluation on the right hand side and then an assignment to the left hand side. So when we are calling the factorial of n when we have a function call let me show you what happens in our runtime stack model. So there would be a few steps each time we call a function. First time is creating a frame for the factorial function. The frame would include the parameters the function has what is inside the parentheses in this case num. The local variables factRes and I. And one additional data which is the return address right. We said that the function when we call function we always go to the same place but when we return each time we need to return to a different position in our code. So we need to store we need to keep where we want to return when this function ends. So that\u2019s why in addition to the data the function uses it also stores where to return when the calculation is done. So first step when calling a function is creating the frame for the function in case of factorial we have num as our argument. Local variables factRes and i. And then the return address. Step two would be to evaluate the arguments in our calling environment. In this case n. So when we are in main n is five we are calling factorial with a value of five. So step two is evaluate the argument. Step three is associating the argument with the parameters. So in this case associating num the factorial num with the argument value five. Also updating the return address to be six to the assignment here. After we created the frame evaluated the arguments associated the parameters with the argument values update the return address. Step four would be to jump to the beginning of the function to execute the code over there. So in this case we will set fact Res to one. By the way when we are inside the function block the only variables we can access are the function\u2019s frame variables. We can access the factorial variables we cannot access the main variables at this position. So the factorial body cannot access the main n k and so on. Ok so factRes is initialized to one and then we start iterating so initially I is one. Then we multiply factRes by one then I increases to two. We multiply factRes by two. I advances to three we multiply that by three. So factRes is six and we keep on doing that until factRes holds the one hundred and twenty or when we are done iterating over the entire range from one to num. In this case five. And when we come to the return statement return factRes. So that\u2019s a new statement. Let me show you how this statement is evaluated in our runtime stack model. So we first evaluate so again a few steps here. First we evaluate the return value the factRes in this case one hundred and twenty. Then we figure out where we need to return to. That is address six. And then we pop out the current frame and jump with a return value in this case one hundred and twenty back to line six. So we assign n fact with the return value with one hundred and twenty. So n fact would get one hundred and twenty.",
                    ""
                ]
            },
            {
                "title": "Runtime Stack Execution Model 1.9",
                "data": [
                    "Kind of complex but let\u2019s do it in this case two more times. So line seven again an assignment expression. First we divided the right hand side. So that\u2019s evaluate factorial of K. That is a function call right. So we are expecting four steps here. First create a frame for this function. In this case it would contain the parameters num the local variables factRes and I. And return address right. Step two is evaluate the argument in this case K. K is three. Step three is associate the argument the parameter with argument value in this case num is three. Also associate the return address in this case we want to return to seven. Not six as before because this call should return to line number seven. And the last step step four is jump to the beginning of the code start executing. So factRes would be one and than we will iterate and iterate and iterate until we will have factRes with a value of six. And then when we come to the return statement we will need to return the value. Again a few steps there. First we will evaluate the value we want to return factRes which is six. Then we will figure out where we want to return to address seven. Then we will pop out the local frame the function that we\u2019ve just ended. No need for that data anymore. And then we will jump back with the return value to the return address. We were jumping back with six that\u2019s our return value to line number seven. We have to complete the assignment so k fact would get the value of six. That ends the execution of line number seven. Line number eight I hope you get it after this third function call. So again an assignment we have a call to the function factorial. So we are creating a new frame for factorial containing the parameters local variables and a return address. We are evaluating step two evaluating the argument n minus k. So under the main\u2019s scope n is five k is three n minus k is two. Step three is associating the parameter with the argument value so num would be two. Return address would be updated also to be eight. And step four we are jumping to the beginning of the code. Same location right the beginning of the factorial function. We will do the execution there initializing updating factRes I and advancing all of that. And when we want to return the value here let\u2019s do the steps we need to take when we are returning a value. So return factRes we first evaluate factRes which is two. Then we figure out where we want to return to that is eight. Then we pop out the current frame and jump back with the return value two to line number eight. So we assign n minus k fact with the value of two. Then line number nine we would assign k comb with one twenty over six times two. Basically which is ten. Then we print that five that is the value of n choose three that is the value of k here in our local scope. Right we are in the main so we are using the variables of the main frame. Is and then we print k comb which is in this case ten. That basically when we are returning zero we are basically just as all other return popping out the current frame and jumping back to where we came from. Back to the operating system. We will talk about operating system in one of our future modules we\u2019ll get into that in much more detail.",
                    ""
                ]
            },
            {
                "title": "Program Implementation 1.10",
                "data": [
                    "Let\u2019s try to write this program on our computer and make it executable we need to do one more tiny thing there. So let\u2019s go to the computer and make it work.",
                    "So let\u2019s take a look at our code here. So we have the main function just as we had it previously. We also have the factorial function we implemented. I just added include IO stream and using namespace STD so we can use the IO capabilities cout and cin and all of these guys there. Let\u2019s try to execute it. Ok so I press the play button. Oh my God build failed. Ok something is not fine here let\u2019s take a look what the problem is. It seems like that problems are in line nine and ten and eleven. Basically the lines where we are calling the factorial function or our factorial function something is not properly ok there. But let\u2019s see it says use of undeclared identifier factorial. Sounds like the compiler for some reason doesn\u2019t recognize the factorial function and says that we are using something that is undeclared or undefined. But then it is defined right. Our factorial is defined down here. The problem is that when the compiler builds the executable it translates it line after line starting line one two three four and so on. So when we compiler reaches for example line number nine it doesn\u2019t recognize the factorial right. Because factorial was not defined before line number nine. It was defined only in line number nineteen. So at that point the factorial the compiler says wait a second I don\u2019t know what factorial is how can I translate it and make an executable a machine language code for calling the factorial. I don\u2019t know what factorial is. So there are two ways to solve this issue. First one is defining factorial before using it. Basically let\u2019s take the definition of factorial up here before the main right. Something like that. First you see that there are no these errors are gone. If we execute the program ok build succeed which is good. I can enter five and three and then it would say that five choose three is ten. As we intended it to be. That is not a recommended way to solve this issue because when we are going to have programs with many more functions it will be difficult to figure out what is the order we need to define the functions so they would be defined before we call them. When we have tons of functions it becomes a very complicated task to do. Sometimes it is also impossible when we have circular calls. Function one calls function two function two calls function three and function three calls function one back again. So in this case we don\u2019t have a right order to play the function so each definition would become before the function call. So the preferred way to solve this issue is not to mind about the order of the way we are defining the function. So in this case I can define the main before factorial this case call the function before I define it. But in order for assuring the compiler that the definition of the factorial function is head to come we are adding function declaration up here before the main. The function declaration basically contains the header the prototype of the function. Basically the header line of the function. The interface for using this function. Basically saying that the function is named factorial it is expected to get one single argument of type integer and return an integer as a return value. When we are copying this header line this prototype the syntax is to end it with a semicolon. In this case it would look something like that. So now when the compiler translates this code after including or extending with the definitions of IO stream and using namespace STD. We are telling the compiler you can be sure that we are going to define a function named factorial later on. And just for your information this function would be named factorial it would get an integer num as an input and it would return an integer as an output. Then when the compiler reaches let\u2019s say line number eleven and it has to compile and translate this function call factorial of n. It knows that it would eventually find a definition for the function factorial. So it knows that there would be a function with the same matching name factorial and passing n an integer as a single argument that\u2019s also fine because this function is expecting to get one integer as an input. And assigning the value factorial n into a variable of type int that is also fine because we said that this function is returning an int. So basically this prototype tells all the compiler needs to know for using for calling for interacting with this function. So by having or by supplying this prototype up here the compiler can know that the syntax of this function call is valid. So it is ok and the compiler just keeps on translating the entire main function. And then line twenty one this promise we made is coming to live. So we are basically implementing the factorial function. So that is fine and we can execute this code. Now we can enter five and three and get the value back. So now that we know that we can call functions we can define functions and we can declare functions let\u2019s continue doing that. We said that there are a few advantages of using functions. For example defining the factorial function first made our code more clearer more readable right. We have the word factorial n instead of some for loop here. We can reuse this function over and over. In this case we used it three times. By the way another point we can make here see that we returned factRes. We didn\u2019t print we didn\u2019t cout factRes. It is a big difference. If in some way instead of return we cout it factRes that would be not a good behavior because it would print these three factorial values. We don\u2019t want that we want to assign it to variables right. Also if we print it we can then not assign it to a variable. So make sure that you see that we intended to return the value not to print it. Later on if you wanted to print it we could have printed it back in the main. You\u2019ll see that a lot of times we would return values from functions. Typically that\u2019s the way we use functions where we return the value. It is very rare that you see that functions do the interactions with the user and print some values to the user. It happens sometimes but most of the times we just return the values back to the caller and the caller would decide whether it wants to assign it to a variable or it wants to print it to the user. What about defining another function here? You see that this calculating here that calculates the k combinations here we are doing it only once. But we can think of scenarios where we want to repeat this calculation over and over. So maybe we should take this few lines of code here. These calculating of factorial of n factorial of k of n minus k and doing the math to get the k combinations and create a new function for that purpose. So let\u2019s create a function named k combinations. There are two inputs n and k so we should do int n and int k. Make sure that when you have more than one argument or more than one input for a function you separate the input with a comma. Just like when you declare more than one variable you separate it with a comma. With one exception here for functions if you have more than one of the same type you still need to write explicitly the type of each variable. So you cannot do something like int n comma k. You will need to say int n comma int k. So this function k combinations gets two inputs n and k. And given n and k it would return an int right. So we have a curly brace lock for the code the body of this function. And let\u2019s define it. Basically it would be this code here right. So let\u2019s take that from here put it here. Right we have some problems it says it doesn\u2019t know the nFact variable so let\u2019s take n fact k fact n minus k fact make them local over here. So the k combinations function given n and k basically declare three additional local variables. N fact k fact n minus k fact that are set to be the factorial of n factorial of k factorial of n minus k. And after having these values we should return this thing here. N fact over k fact times n minus k fact. That is the return value that we are returning. And then up here in our main we will see k combinations after reading n and k to be whatever k combinations would return when called with n and k. Something like that ok.  Let\u2019s take a look here so we still have one issue here that we are using k combinations that is not defined. Yeah that\u2019s basically we are defining it after using it. Let\u2019s make a declaration let\u2019s the prototype of k combinations up here. That should solve this issue. Ok let\u2019s look at our code now. So first we have the declarations of factorial and k combinations. Just so we can use it before we define it. Then the main has actually only three local variables. N and k that we are reading from the user. And k combinations k comb actually here. That would hold whatever k combinations results to. And then we print n choose k is whatever k comb is. Right and k comb would be set to the calculation of k combinations hold with n and k. So again take a look at this main it seems very readable now. So we are reading n and k we are calculating k combinations for these n and k and then we are printing the result. Right we don\u2019t need to do a lot in order to understand exactly what happens here. And then k combinations is implemented very straight forward right. Given n and k we are defining these three local variables that are set to the three values of factorial of n factorial of k and factorial of n minus k. And then we are returning this combination here. Then we also obviously define the factorial function as well. Even though we are calling the k combinations only one time it is still important to define these functions. First for increasing the readability of our program and then for reuse maybe in other programs as well. Let\u2019s just execute it and see that it works as expected. So build succeeded let\u2019s enter five and three and then we get that five choose three is ten. So it seems to be working just fine.",
                    ""
                ]
            },
            {
                "title": "Scope of Variables Example 1.11",
                "data": [
                    "Ok let\u2019s take a look at another example and see how our local scopes basically work. So for example let\u2019s have a main program that has a local variable n. It prints the value of N and then it calls a function named func. Passing n as an argument. And then it prints the value of n after this function call. And func let\u2019s define a function here named func that gets an integer n as an argument. By the way it returns void when we have void as a return value it means that there won\u2019t be any value returned. Right you can see that when we call func we didn\u2019t say some variable equals func right. Because this function doesn\u2019t return a value only does some calculations but not returning any return value. You can see that there is no return statement at the end. So a function can return whatever type it wants it can return an int a float a double just any type we want. Or it could be a void basically saying there is no value to return. Ok so we have this function this function sets n to be four and then prints the value of n inside. Let\u2019s try to trace this execution. So before the function N is three right n is set to three so just after that it would print before func three. Then we call func with three basically jumping to the function position setting n to be four. So inside the function n would be four. And just after the cout we jump back to where we came from and printing the value of n after this function call. So it would print after func but I am not sure whether the value that would be printed is the updated value of n basically four or the value of n was before which is three. So let\u2019s in order to solve this issue let\u2019s use the runtime stack model and trace this execution in that model. That would explain the behavior entirely. Ok so we start with frame for the main containing the local variable n. Initially n at line two is set to three so n gets the value of three. Then we line three cout before func the value of n is three. Then we call the function func. So a function call we have a few steps we need to take. So first we create a frame for this function containing the parameter n. Local variables there aren\u2019t any and the return address. By the way it is not a coincidence but the parameter name for the function func and the local variable up the main are both named n. And you can see there are two separate instances of variables both named n. One in the main frame and the other in the func frame. So there are two different variables I believe you probably already can guess what would be the value printed after func is evaluated. Ok so step one was creating the frame so we have n in the return address. Step two is evaluating the argument so up in the main the value of n in the argument is three. Step three would be assigning the parameter with the argument value basically assigning the local func\u2019s n to be three. Returning address would be five right after this call. And then we are jumping to the beginning of the function. Now we set n to be four. Which n would be  set to four? Obviously the local n that we are currently in the variable\u2019s scope. So our currently scope is the func frame so we are setting func\u2019s n to be four. And then obviously when we print inside func the value of n is four. When this function ends there is no return value but we do figure out that we want to return to address to line number five. We pop out the current frame and at line number five we are back to the main\u2019s frame. So the original n didn\u2019t change its value it was three then locally for the func the local n changed from three to four. So when we print after func the value of n it would print three. Ok so then we return zero pop out this frame and end this execution. So yeah so each function has its own local frame that when we are executing it the variables we access are the local variable. We have access only  to these variables we don\u2019t have access to frames that are out of variables that are out of our local frame. And we can have variables with the same name but there would be two separate instances because each of them is in a different scope. You can see that the runtime stack has dual role here. It both manages the frames the local scopes of each function. What are the variables that are accessible in that function and it also helps us to manage the flow of the program. To remember where to go back when the function ends.",
                    "Pass By Value 2.1 ",
                    "Ok let\u2019s take a look at another example. Let\u2019s define a main that would have two local variables A and B. A would be initialized to three B would be initialized to four. Then we print the values of A and B basically before function calls. So we say before A is probably three B is probably four. Then we call the swap function with A and B and then we print the values of A and B after this function call. The swap function just as its name basically implies it would swap the values. So given A and B as inputs it would swap these values. So let\u2019s see what would be printed here let\u2019s trace the execution. So initially the frame for main would include A and B the local variables that at line number two would be assigned to three and four. Then the cout at line number three would print that before A is three and B is four. And then we have the call for the swap function. Again note that the swap function doesn\u2019t return any value it is a void function that is given two integers. Let\u2019s follow the steps for a function call for the swap function. Step one is create the frame for swap basically containing the parameters A and B. Same names of variables for the main A and B and parameters of swap. They can be the same they can be different anyway they are separate instances. Local variable temp here that\u2019s also included in the swap frame. And return address so the step number one is create this frame. Step number two is evaluating the argument up there in the main. A and B are three and four that are in step number three associated to the parameter the local parameters A and B. So the local A and B are also containing the same three and four we had up in the main. Return address would be to line number five right we are currently in line number four so we\u2019ll return back to line number five. And then we just need to jump to the beginning of the swap function. So in order to swap the values we would need to use this temporary variable to store the original value of A which is three so we can override variable A with the new value the swapped value of A. Then we say A equals B basically erasing the value the previous value of A putting four in its place. And then luckily for us we stored the previous value of A in temp so we can set B to be the previous value of A basically temp. Right so as you can see A and B did swap their values instead of three and four they are now four and three. But unfortunately the swap of values was locally in the swap function so when this function ends and we pop out this frame and we jump back to where the main is. To the main frame. The swapped values are already gone and A and B the original A and B of the main have their original values unchanged which are three and four. So this thing here would print three and four. And then the main ends and so on. Not surprising basically very similar to what happened in our previous example. But a bit concerning because we do want to be able to do some thing in a function and affect other locations as well. ",
                    "",
                    ""
                ]
            },
            {
                "title": "Parameter Passing 2.2",
                "data": [
                    "Let me show you how we can deal with that. I want to introduce you a few ways a few types to pass parameters to function. There are basically two major ways for a parameter passing to functions. One of them is called call by value the second is call by reference. We\u2019ve used up to now without mentioning it that\u2019s the only way I showed you how to create parameters and pass arguments to parameters we used call by value. Now I want to show you what call by reference is and what\u2019s the difference between both of them. So we\u2019ll talk about these two types in two dimensions. First I will talk about the syntactic difference what you need to do coding wise in order to call a parameter or to pass a parameter by value. And what you need to do syntactic wise in order to pass a parameter by reference. And then I will obviously explain what is the difference. What does it mean when you pass a parameter by value or by reference. So syntactically the difference is very simple. In a call by value you do exactly what we\u2019ve done before. You have your func and then the parameter is just the type and the name of the parameter. Int x in this case. The syntax for call by reference is very similar but with one significant difference. So when you have func then you have int and then ampersand symbol x. So adding the ampsersand symbol next to the int that would make your parameter here be called by reference parameter. So that\u2019s the way you control the method you are passing your parameter. Either you just have the type before the name that would be call by value. Or type ampersand and a name that would be call by reference. So we know how syntactically to call or to pass an argument by value. How syntactically to pass an argument by reference. But what does it mean when we pass an argument by value versus passing an argument by reference? So the semantics is something like that the meaning is something like that. When we are passing a parameter by value we are evaluating the value of the argument and that value is passed. Where when we are passing an argument by reference we are evaluating the reference the position the location of the argument and the link or a reference to that position. That is what passes. ",
                    "",
                    "Example 2.3 ",
                    "I think it would be much easier to understand if we\u2019ll take a look at the swap example. So as we initially wrote it as you can see A and B these two parameter of the swap function were passed by value right. We have int A int B. Let\u2019s see what happens if we pass A and B by refenrece. Note that I added the ampersand sign symbol just for both A and B so now we have void swap and int ampersand A int ampersand B. Basically saying both our parameters would be called by reference. And let\u2019s see what the execution would look like. I would need to update the runtime stack model to support or to explain also call by reference. So it looks something like that. Again we start with the main frame containing the local variables A and B. Line number two assigns A to three and B to four. Line number three prints before A is three and B is four just as we had before. Then we have the call for swap and see what is different here when we call by reference ok. So again there are going to be a few steps. First step same thing we create a frame containing the parameters in this case A and B. Local variables in this case temp. And return address. Step two is a bit different in step two basically we evaluated the arguments. But since now we have two types of passing the arguments or two ways to pass the argument it matters whether we are passing an argument by value or by reference. In case we pass an argument by value we evaluate in step two the value of the argument. In case we are passing an argument by reference we won\u2019t evaluate the value we would evaluate the position the reference of the variable. In this case A and B are called by reference right so we won\u2019t evaluate the values of A and B which are three and four. We\u2019ll evaluate references links positions to A and B. The places where A and B are. And when step three we assign the parameters with the arguments we will assign a link to the main\u2019s A for A and a link to the main\u2019s B for B. So we are passing the references to the variables A and B to main\u2019s A and B. So the parameters the local parameters A and B of the function swap contain references basically links that\u2019s why I colored them in blue just to make it look like a link. We are passing a link a reference to the main\u2019s A and B. Which when we look at this image now it seems that the swap function by using the local data in the frame of swap that has A B the local A B and temp. Now A and B have access to the main\u2019s A and B and maybe do the swapping the changes over there. So it seems to be good right. So again step one was creating the frame. Step two was evaluating the arguments in this case since they were called by reference the references to the variables to A and B. Step three is assigning the local variable the local parameters to these references. And step four the return address would be five. But step four would be jumping to the body of the function. And then we have first instruction temp equals A. What do you think would happen? Would temp contain a reference to the main\u2019s A or temp would contain the value pointed by the reference to A? So the semantics of C++ says that when you use reference variable in this case A and B. When you look at the value of this variable you always follow the reference. So temp equals A the value of A would be three. So temp would get the value of three. When we say in line twelve A equals B. What do you think would happen? Would the local A obviously it would affect the local A the swap A the swap\u2019s A. Would the local A contain a reference a link to B or would the main\u2019s A followed by the reference would contain the value four followed by the reference of B? So once again the semantics of C++ when we use referenced variables we affect the variables referenced by these variables. So the main\u2019s A would get the value of main\u2019s B that would set the main\u2019s A to be four. Again by using the local references we affect variables that are out of our scope. Which is great that\u2019s exactly what we wanted to do. And eventually line number thirteen would change the referenced B basically the main\u2019s B to be the value of temp which is three. So these few lines of code updated swapped the main\u2019s A and B values to be four and three. Obviously the fact that both the main\u2019s variables names are A and B and the local swap function variables A and B are the same that is just a coincident. It doesn\u2019t have to be like that. When the swap function ends this frame is popped out but when we go back to line number five we print that after the function call the value of A now is four and the value of B is now three. So it is a very powerful tool now that we have that passing by reference that when doing that would basically give the function permission to access variables and data that originally it doesn\u2019t have access to. Right when the main ends that pops out and everything basically ends. ",
                    "Analyze Digits 2.4 ",
                    "Let\u2019s try to write another program. Let\u2019s write a program that reads a positive integer num and prints both the numbers of digits in num how many digits. And what\u2019s the total sum of all of these digits. We\u2019ve done that a few times but now let\u2019s try to do it using functions. For example an execution could look something like please enter a positive integer. So user says three seventy five and then the program would respond by saying three seventy five has three digits right. Three seven five three digits. And their sum is fifteen three plus seven plus five."
                ]
            },
            {
                "title": "Analyze Digits Introduction 2.5",
                "data": [
                    "When we implement this program we\u2019ll probably want to define a function analyze digits right. And let\u2019s think of the interface the input and output this function should have. So as an input for analyze digit I would expect some number num that\u2019s the input of this function. And this function should given the num do a lot of calculation a lot of yeah calculation there. And tell us two things first what is. Or not first one is what is the number of digits and the second is what is the sum of these digits. So there is one input and two outputs. And that is a bit of a problem in C++ because if you recall the syntax of a function we have the function name we can have as a parameters as many parameters as we want. Just separate them with commas say what\u2019s the type of each of them and you can have as many inputs as many parameters as you want. So for the input we can as many as we want but before the function we have the type of the return value. Basically saying that we can have only one value returned from a function. So when we use the return statement or the return expression we can only return one value. Which can then be assigned to a variable or whatever. But we can have only one in this case we want to. So in order to implement such behavior we can work around that by using call by reference. Since call by reference basically can update a value in some other scope in for example the caller\u2019s frame scope. We can maybe instead of returning a value can update the variable that we want to set this return value to. So basically we can have we can write we can implement this behavior in a few ways. One of them would be say analyze digits input would be an integer num and then since we want to return two stuff one we would return as a return value for example the amount would be the int for the return value. And the second would be returned using call by reference variable. So we would have int ampersand int by reference. And out sum that would be the returned sum. By the way I note that the num the first parameter is an integer and it is called by value. You didn\u2019t have the ampersand right. And then the second parameter is out sum and it is called by reference. In a few minutes we will implement it and you will see exactly how that works. But just naming convention when a parameter is used for a return value I like to name it out something in this case out sum. You can also say ok if we can return values using call by reference let\u2019s do it this way. So let\u2019s have analyze digits with an input of num and have both values returned this way. So we would have out sum and out count digits. Both passed by reference so int ampersand out sum and int ampersand count out count digits. This case our function would be void right it won\u2019t return anything as a return value. It would return both of them in the mechanism of passing by reference. Let\u2019s implement the analyze digits using the computer. I\u2019ll implement the first one just to show the difference of how to return both by a return value and using a call by reference. Both on the implementation of the analyze digits function and in the calling of the function as well. So let\u2019s go to the computer and write this code.",
                    ""
                ]
            },
            {
                "title": "Analyze Digits Implementation 2.6",
                "data": [
                    "So let\u2019s implement this program. First let\u2019s read the input so let\u2019s say please enter a positive integer end l. And then let\u2019s read it so let\u2019s declare a num variable here and read into num. Now we need to do the calculation here analyze digits so we would need two values to be calculated. The count digs and the sum digs. The count of the digits and the sum of the digits. We said to have a function analyze digits that as an input it would get num passed by value right. We don\u2019t want num to update anything. And it should return these two values ones as a return value as an int and the other by reference variable outsum. Ok so let\u2019s have this function here in a few minutes we will make the code here. But when we call this function it would be something like that. Let\u2019s call this function analyze digits right. The input would be this num here that we\u2019ve just read. So this num here would be passed by value to our parameter num over here. Again I named them the same name but it is a coincidence they can have totally different names. And then this function given num should return these two values. The first one is the return value right. So let\u2019s have count digits store whatever this function returns right. This function returns a value that we can then store in a variable. So that\u2019s how we get the return value in the return into our count digs variable. The second would be get as a parameter that we would call by reference. So if we pass this sum digs local variable by reference. You see as the second argument is passed by reference. If we pass it by reference then by updating the value of out sum here locally inside this function that would update this main\u2019s variable with the value. So and I am just writing some non sense here but by doing out sum equals. I am just making up eight. That would update sum digs up here in the main to contain the value eight right. Because it contains a reference to this variable. So this variable would get its value by the assignment right. The assignment would put the value into count digs. And a local assignment inside the analyze digits function would update this value with a return value. So these or both of these variables would eventually have the right values in them. By two different mechanisms for returning values. So that would be the call for analyze digits. After we do that we can just announce that cout num the value of num and then has and then we have to say how many digits. So count digs digits and their sum is and then let\u2019s have the sum digs value and end l. That\u2019s it right. That\u2019s basically it. So when calling analyze digits we assign both count digs with a return value and sum digs with a call by reference. Then we can assume that both of these variables contain the whatever the function calculated. Let\u2019s declare the function right. We have an error here let\u2019s declare this function. Semi colon at the end right don\u2019t forget. And the only thing we have left to do is implement the analyze digits function. So for that that\u2019s a very simple implementation. We basically iterate over the digits of num counting each one accumulating the sum of them. So eventually we would have a total count and a total sum. We\u2019ve done that a few times already. So let\u2019s create a local count and sum variables. We\u2019ll initialize count to zero and sum to zero and then using a while loop. While num is greater than zero each iteration would have our current digit. So current dig would be num mod ten right. Let\u2019s also create current digit variable. So we have current digit and then we count this digit so count plus plus. And we accumulate this digit sum plus equals the value of current digit. Then we need to remove this digit so let\u2019s update num to be num div ten. Basically removing the ones digit. After we\u2019ve done this calculation then these two local variables count and sum basically contain the two values we want to return. One of them should be returned. Return count. And one of them should be updated using this reference here. So outsum would be evaluated or assigned with a value of sum. Basically by this reference we are assigning sum digts with a current value of sum. One very important thing actually a return statement I don\u2019t know if I mentioned it explicitly but a return statement basically pops out the current frame and jumps back to where we came from. So all the statements that appear after the return statement basically are not executed. So the order that we return the values in is very crucial. We cannot first return this thing here and only after that update the reference variable. It is very important first to update the reference variable and only after that do the return statement right. Because otherwise this assignment wouldn\u2019t be executed. So after having both count and sum with right values we update using our reference the main\u2019s sum digits to be the sum and we return the count and this assignment here would update the coutn digits with this value. Let\u2019s just execute it make sure we didn\u2019t make any silly mistakes here. So we succeeded. Please enter a positive integer. Three seventy five and then it says that three seventy five has three digits and their sum is fifteen. Seems to be working fine. Just make sure you see how we used call by reference in order to work around the fact that C allows return only one value out of a function. We basically used call by reference in order to access our calling scope and update a value using that reference.",
                    " "
                ]
            },
            {
                "title": "Solving Quadratic Equations 3.1",
                "data": [
                    "Ok as a final example let\u2019s write a program that solves a quadratic equation. Let\u2019s write a program that basically reads three real numbers representing the coefficients of a quadratic equation. And then it prints the solutions of the equation if there are any or an appropriate message. So for example an execution could look something like please enter coefficients of the quadratic equation. The user says one negative five and six. And then the program would say the equation one x squared just uses this symbol to says power up. So one x squared plus negative five x plus six equals zero solutions two and three. In this case there are two solutions. So before we go to implement this program let\u2019s go back to I don\u2019t know seventh eighth grade where we\u2019ve talked about quadratic equations and make sure that we got the math all figured out. All the observations how to solve quadratic equations. So we know that we have this formula in case our equation A x squared plus B x plus C equals zero and it is a real quadratic equation. So A is not zero. Then we have an equation for the square root of these of the solutions of these x sub one and two are negative B plus minus the square root of B squared minus four A C over two A. See why it is important that A is not zero so we can divide by two times A. So this formula here is fine only when it is a real quadratic equation. And basically there could be different types of solutions basically depending on the sign of the discriminant. So in case B squared minus four A C is positive there could be two real solutions here. For example a one negative five and six x squared minus five x plus six equals zero. We have two solutions x one and x two. But then if the discriminant is zero then when we take the square root it would also be zero and the plus minus zero would be the same. So for that case we would have only one solution. For example when A is one B is two and C is three you can check that  B squared minus four A C would be zero and then we would get only one solution. X equals negative one. And there are also cases where the discriminant is negative in this case there is no square root real square root. So there is no real solution so we say that in this case there is no real solution. For example when A is one B is zero and C is one you will see that B squared minus four A C is a negative. In this case there is no real solution there are complex solutions but no real solutions. So so far for quadratic equations we either have two real solutions one real solution or no real solutions at all. But when we are reading these A B and C we are not guaranteed that A would be not zero. There are cases where A could be zero. In these cases the solutions can also be a bit different. So for example if A is zero B is zero and C is five so we get a zero x squared plus zero x plus five equals zero basically five equals zero no value substitute for x to get like a truth statement here. So in this case there is no solution at all not only not a real solution and you do have a complex solution. There would be no solution at all right. Or if all A B and Cs are zeroes so you\u2019ll get zero equals zero basically all values you\u2019ll substitute for x would give you a truth statement. So for zero zero zero for example you would get that all reals are solutions. So let\u2019s use these observations so there are five types of different solutions either two real solutions one real solution no real solution at all no solutions at all or that all the reals are solutions. Now that we\u2019ve refreshed that let\u2019s go ahead and implement this program.",
                    ""
                ]
            },
            {
                "title": "Implementation 3.2",
                "data": [
                    "Ok so let's implement a program that solves the quadratic equation. Let's also focus in this implementation not only making this program work but also breaking it down to functions in a correct way so that would increase the readability over a program and the usability of the functionalities we implement. So we can reuse it in other cases as well. Let's also focus on the how to document our code in this sense as well. So let's start obviously we would include iostream cmath we probably need a square root there so let\u2019s include the cmath. Using namespace std and let\u2019s also define constants for the different types of solutions. So let\u2019s add no solutions one real solution two real solutions all reals or no real solution. We\u2019ll have a number associated to each of them but the name of the constant matters the most because we are going to use the names to kind of speak what we want to say. Then we are going to define a quadratic equation right. I am before we start the main we have all of these definitions the includes the constants the prototypes here. So for example for quadratic we have double A double B D sorry double A double B and double C as three inputs all passed by value. And then quadratic should let\u2019s think what quadratic should return. It should tell us what kind of solutions we have here. Either no solution one solution two solutions and so on that is one thing it should return. And it should also tell us what are the solutions in case we have two real solutions we want to know what are the solutions. So we are expecting somewhere between one value only saying what the types of solutions we have to three. In case there are also some solutions. So in case there is no solution at all we\u2019ll get only one value back. In case there are two solutions we\u2019ll get two real solutions and then two values that are also as the solutions. So we are expecting somewhere between one to three values as a return. I think it makes sense to return the type of solution as a return value and to allow two parameter that could be passed by reference to store and to return in them the two solutions in case we have two solutions. For that I made the return value of type int and two doubles that are passed by reference so the quadratic can update the main\u2019s variables to contain or to have the solutions. That seems like a good prototype a good interface with a quadratic function. Let\u2019s also document what we kind of said over here. So that kind of document that describes the interface of with a function basically what the caller of the function should know when it uses when they use this functions should come right here where we define the prototype of the function. So we said we say first that the quadratic function solves the quadratic equation of the form A x squared plus B x plus C equals zero. We should say what the input of the function is what the output of the function is and if there are any assumptions we should also say that. So the caller would know what assumptions this function basically takes. What assumptions the caller should have or should assume for this function behavior. So for inputs obviously A B and C are the coefficients of the equation. For outputs we should say that there are three kind of outputs even though two of the outputs are inside kind of the input location input position inside the parentheses. They are still considered to be output values. So for output we will say that there is one the type of the solution and I also say what mechanism I am using to return this output that is return value. And out x one and out x two are solutions to the equations these are output parameters called by reference. "
                ]
            },
            {
                "title": "Implementation 2 3.3",
                "data": [
                    "Let\u2019s go ahead and implement the main. Now that we\u2019ve declared a function. Again in terms of documentation I am just describing what this program is supposed to do what input and output what kind of interaction this program would have with the user. So I am saying this program solved the quadratic equation input from user would be three real numbers representing coefficients of a quadratic equation. And the output to the user would be the solution to the equation if there are any or an appropriate message. The implementation of main is quite straight forward we\u2019ll define A B and C as local variables. We\u2019ll ask the user to enter the coefficients of the quadratic equation read it into these A B and C. And then we should call the quadratic function and figure out what type of equation we have here what kind of solutions we have and this kind of thing here. In this case so yeah we should start with announcing that the equation A x squared plus B x plus C equals zero has and then in this case I think the control flow that would basically branch between or choose between the different types of solutions is best implemented using a switch statement. So we\u2019ll do switch and the value we are switching over is basically whatever results with the call of quadratic. Basically the return value of quadratic. We said if you recall the return value that quadratic returns is one of the constants up there. The type of the solution right. So when we are calling quadratic with A B C x one and x two which are by the way two local variables that we defined here to store the optional solutions. But when we are switching over the call of quadratic basically the value of this call would be the return value. And that is the type of solutions. So we\u2019ll have a case for each type of solution. So we\u2019ll have case two real solutions case one real solution case no real solution case no solutions at all and case no reals. And we will also have like a default clause saying that there is an error because we are expecting to get one of them. So I hope you are convinced now that a switch makes it very readable. So we are switching over the call of quadratic and the different cases are basically the different kinds of solutions. Again the use of constants here makes our code very readable. We can basically see switch quadratic and case two real solution one real solution the different cases. And let\u2019s see how we behave in each one of these cases. So in case of two real solutions we just print the solutions are x one space x two right we said that in case there are solutions we want them updated in the output parameters. In this case x one and x two would contain these values. One real solution let\u2019s print one solution and then x one by the way how do we know that the solution is inside the x one? That\u2019s a thing we should maybe say a few words about it. No real solution by the way we just print no real solution or no solution at all or that all the real numbers are solutions.",
                    ""
                ]
            },
            {
                "title": "Implementation 3 3.4",
                "data": [
                    "But let\u2019s add an assumption as we said to say that what\u2019s the behavior in case there aren\u2019t just two real solutions. So let\u2019s add these two assumptions. We\u2019ll write these assumptions right here in the documentation so the user would know what kind of behavior to expect from the function. So the assumptions we are adding is if equation has one solution it would show in out x one. So when we in the main said that in case of one real solution we are printing x one we can be we can feel safe that x one would be the variable that would contain that single solution. Also we are saying that in case there if equation has no real solutions or no solutions at all the values of the output parameters are not defined. So we don\u2019t care what they have. And again in our main when there weren\u2019t no solutions we didn\u2019t access these output parameters. So by adding these assumptions to our documentation we basically tell our caller what type of behavior they should expect from our function to when they use it. "
                ]
            },
            {
                "title": "Implementation 4",
                "data": [
                    "I think we are ok now to implement or we can start implementing the quadratic function. So as we said the quadratic function gets three inputs as parameters A B and C. It would return the type of solution at our return value and it would also update out x one and out x two as the output parameters. For implementation basically we want to use the formula we have for the solutions of a quadratic equation but that only works when A is a non zero value. When we can divide by two A. So let\u2019s first make sure or calculate the cases where A is non zero. And then in these cases we can use the formula of x sub one two equals this thing here. Let\u2019s define the discriminant delta to be b squared minus four A C and check whether it is positive or equals to zero or else basically means negative. In case it is positive we\u2019ll have x one be minus b plus the square root of delta divided by two A. And x two would be minus b minus the square root of delta divided by two A. These are x one and x two are basically our local instances of the solutions then we want to return these two values. So we will set out x one and out x two to these two values and we should also return our type of solution for that case which are two real solutions. Right again make sure you first return the values in the output parameters and then do the return statement. Now let\u2019s take care of the case where our delta equals zero. In this case we said we are assigning out x one so let\u2019s first have x one be minus b over two A. Right the plus minus would be plus minus zero that\u2019s why we have only one. So out x one that\u2019s very important by our assumption out x one would contain x one and we return one real solution. And in case A is not zero but our discriminant is negative in this  case we just say no real solution. We don\u2019t update out x one and out x two at all and we said we are not defining what would be returned by these values in case there are no real solutions. Ok now the last thing we have to take care of is what happens if A is zero. In this  case actually our quadratic equation A x squared plus B x plus C equals zero when A is zero basically it becomes linear equation. B x plus C equals zero we don\u2019t have a quadratic element anymore. So in this case we should solve it as a linear equation. We can have the code here but actually I think it would make sense if we would create a function that deals with linear equations separately. Then we can use this function later on maybe not in this program but in another programs as well. So it makes sense to have or to define a function that not only not only to define a function that solves a quadratic equation but also to define a function that solves a linear equation. So the quadratic can use the linear but the linear can be used in other scenarios as well.",
                    ""
                ]
            },
            {
                "title": "Implementation 5 3.6",
                "data": [
                    "Let\u2019s add another function here. Linear just as we had quadratic. The input for the linear would be actually linear has two coefficients so we would have double A and double B right. And the linear function should return the type of solution right maybe all reals we said in case of a linear. There could be no solution at all or no real solution or both of the times actually there is only one solution for linear equation. A x plus B equals zero. So just move the B divide by A and get one real solution. So let\u2019s have the solution as an output parameter and the type of solution as a return value. Let\u2019s document this function so linear first we say what is it intended to do so it solves a linear equation of the type A x plus B equals zero. The input are A and B which are the coefficients of the solution. And the output is the type of the solution as a return value and out x one that is the solution of the solution of an equation or a solution of an equation as an output parameter. Our assumption here is obviously if there aren\u2019t any solutions then out x one is not defined. "
                ]
            },
            {
                "title": "Implementation 6 3.7",
                "data": [
                    "Assuming we have this function our quadratic function would now use this function. So in the else clause in case A is equal to zero then we just call the linear passing what would be the inputs for linear? So yeah so the quadratic B and C coefficients become the A and B coefficients for the linear. So we\u2019ll pass B and C and then out x one would be our reference to where we write the solution in case there is a solution. Maybe I\u2019d note here that we are passing like out x one is already a reference to a variable x one in our main. And now we are passing it to another function by reference so we won\u2019t get a reference to the reference that would give us a reference to the original value in the main. So when linear updates out x one it basically updates the original reference we\u2019ve passed from the main. So yeah so we\u2019re passing out x one and basically we are just rolling out the return value of linear as the return value of quadratic. So if linear returns no solution or one real solution or all reals we\u2019ll just keep on returning it as a return value of quadratic. That\u2019s why we are saying return linear. ",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 9,
        "module_name": "Arrays",
        "file_name": "Module 9 Arrays.docx",
        "transcript": [
            {
                "title": "Motivation 1.2",
                "data": [
                    "Hi there. I hope you're okay. Today we're going to talk about our first data structure in simplest loss. But before we start doing that, let's take a second look at one of our programs we've implemented in one of the previous models. If you want a more detailed review, you can just click on the link below. But I hope you recall that we calculated an average of a sequence of elements.",
                    "",
                    "Actually, we wrote a program that computes the average of student grades. The program first asks the user to enter how many grades. How many students are in the class of the student, the user responded; let\u2019s say four. And then he was asked to enter the grades one after the other, so seventy one, eighty six, sixty nine, ninety four, and eventually the program just responded with the average of these grades, which is seventy nine point twenty five.",
                    "",
                    "So, if you recall when we implemented this program we basically summed up all the numbers and divided it by the amount of the numbers, that's how we calculate the average. Let's take a closer look at our implementation. ",
                    "",
                    ""
                ]
            },
            {
                "title": "Computing the Average Implementation 1.3",
                "data": [
                    "Okay. So we start by reading the number of students from the user, and then we ask the user to enter the grades. Then we do number of student iterations in order to first read each grade but then, we also want to accumulate the sum. So, for that we have our accumulating variable sum that is first initialized to zero before starting to iterate and then after each time we read a grade, we also add it to this accumulation variable, we add it into sum.",
                    "",
                    "So, when this \u2018for\u2019 loop ends we have the total sum of the grades in our sum variable. So we can just divide sum by the number of students and get the average. After doing that, all we have left is just outputting this average back to the user. ",
                    "",
                    ""
                ]
            },
            {
                "title": "Above The Average 1.4",
                "data": [
                    "Okay. So now let's try to make this program a bit more complicated: in addition to just calculating the average, let's also figure out which grades are above the average.So, the program would interact something like that. First it would ask the user to enter the number of students in the class, the student would say four. Again, the user would be asked to enter the grades. So seventy one, eighty six, sixty eight, ninety four, and then the program would not only say that the average is seventy nine point seven five. It would also say that the grades above the average are: eighty six and ninety four. ",
                    "",
                    "Seems very similar, but then if we think about it a bit more; let's see how we can implement this requirement. So, we would need to know what grades are above the average. We can do it really in a single pass over the elements, just as we have for calculating the average when we wanted to sum them up because only after figuring out the value of the average we can know which grades are above the average.",
                    "",
                    "So it seems like we would need two passes over the data in order to find the grade that are above the average. So one solution could be let\u2019s ask the user to re-enter the grades after we calculated the average but that is not user friendly at all. And so, another solution could be maybe to store the grades, to store these datas in our memory, so we can pass over them more than once. It could be very impractical to store them in different variables because first, we don't know how many variables we would need and it would be very difficult to manage these variables. For that we would need some kind of a data structure, which is a single entity that allows us to store a collection of elements in it and to access and modify these elements as we need. We would use the ray data structure, a static array basically, which can be thought of as a sequence of variables that are of the same type; in this case are integers. Let's see how it works.",
                    ""
                ]
            },
            {
                "title": "Above the Average Implementation 1.5",
                "data": [
                    "Let's see how we can get it done. So let's have our main place for the variables, so let's have an integer X, a double Y. That's a sign X. with four, Y. with seven point three. And then our memory, our run time stack would look something like that. We have our X, let's say it's physically stored in the address one thousand, it's assigned to be four since it's four bytes long, then it would end at thousand and four. Then we would have Y; Y is double so it\u2019s eight bytes long, that would take it to ten twelve. Y is assigned to seven point three. Okay, so all of that we already know. We can create variables of different types, assign each one of the stored in the runtime spec, everything is as we use. ",
                    "",
                    "Now let's create our already, which is a sequential collection of variables. So if you do something like that; int arr five that's a single line of code that basically declares a sequence of five integers. So they are in the memory one after the other: one, two, three, four, five. And there are ten twelve, ten sixteen, ten twenty. They\u2019re all integers. So each of them is four bytes long: ten twenty eight, ten thirty two. So in a single line of code, we basically declared a sequence: a collection of five integers. And then let's say I want to write the value ten over here. So this array\u2019s basically zero based index; each one has a local address, a local position. And if I want to assign this thing here to be ten, I'll just have arr index two. I use a square brackets, equals ten. If I want to write a value fourteen here, I\u2019ll just go arr four. That's the index that's a position where I want to have the number fourteen in and put fourteen right over there. So as you can see, it's very easy to declare a sequence; to declare a multiple collection of variables in a single line and then we can also have our uniform naming mechanism using indexes to access the different position, the different cells or slots of this array.",
                    ""
                ]
            },
            {
                "title": "Basic Properties of Arrays 2.1",
                "data": [
                    "Let's recap the three basic properties of arrays. So let's say we have an array, so the first thing is obviously that the elements are all stored one after the other in our memory. So, the elements are stored continuously in the memory. Second thing is that all of the elements are of the same type. For instance, if they're ints, they're all going to be four bytes long but they all must be of the same type; we can't have an array that one element is an integer, the other is a double and so on. So either all of the elements are integers, or all the elements are doubles, or all the elements are chars or whatever, but they all must be of the same type. And the last thing is that we access our array using a zero-based index system so they have their local addresses starting at zero, one, two, three, and so on.",
                    ""
                ]
            },
            {
                "title": "Basic Array Properties 2.2",
                "data": [
                    "So, these three properties basically imply another very important property; let's take a look. For example if we have an array of six integers, in the memory we would have six continuous integers one after the other. Let\u2019s say it starts at address one thousand. If we want to assign let's say arr zero with four, that won't be an issue for the compiler since it knows that the array starts at address one thousand, then that's exactly the location where this four should be written. ",
                    "",
                    "",
                    "",
                    "Okay, so what happens if we want to assign arr two with let's say five. How would the compiler know where in our memory to write this five? So since it knows it starts at address one thousand, how could it calculate; how could it figure out where arr two is? So, this is obviously index zero, one, two, three, four, and five: these are the indexes of the array. Arr two should be two jumps of four bytes after this one thousand. Actually the compiler can figure out a formula that the address of arr two is one thousand plus two times the size of each element in the array, basically one thousand plus eight; that is the address of arr two, that is where five should be written. More generally, we can say that instead of one thousand, that would be the address arr begins, plus two times instead of four, let's just say the size of each element.",
                    "",
                    "More generally, let's say since we know that all the elements are continuously in the memory they're all of the same size and it's a zero based index system. We can figure out a formula that the address of, let's say arr I. Let\u2019s do a general formula here, would be this base address where the arr starts.",
                    "So the address arr begins, plus in this case I jumps of the size of each element, which is I times size of each element in arr. So the compiler can use this formula in order to figure out where each element is located in the memory.",
                    "",
                    "Basic Array Properties Cont\u2019d 2.3 ",
                    "This formula that the compiler uses makes very weird behavior. For example, if we do, let\u2019s say, arr five equals seven, that's regular behavior where the address of five would be, once again, one thousand plus five times four that would be one hundred and twenty, one thousand four, thousand and twelve, thousand sixteen and twenty and that's where the seven would be written in. ",
                    "",
                    "But let\u2019s say the programmer says arr eight equals, let's say, ten. So that's obviously a logic mistake because our arrays only of size six and we're trying to access arr eight, which is basically the ninth element in this array. There are no nine element, there are only six elements but then the compiler would just use this formula.So the address of error arr eight would be one thousand, as our base address, plus eight times four which takes it to a thousand and forty, and ten will be written over here. So the compiler doesn't care that we go out of the range of our array, it basically goes follows this formula and writes to the memory using this formula and it's our responsibility as programmers to keep ourself inside the boundaries of the array. Know that in address ten forty, there could be other variables it can be even memory that is not associated to our program; it could be associated to some other application that runs simultaneously to our program And it's very dangerous to write to locations that are not logically related to our program, so it's our responsibility to keep in bounds. ",
                    "",
                    "Another quite weird behavior we can say arr negative two equals, let's say, twenty. Again the compiler would follow the same formula: substituting negative two to be one thousand, the location is one thousand minus two times four, which is basically ninety nine two, that where the twenty will be written. So we kind of have the eight index and the negative two index, which are invalid indexes. But the compiler does allow it; it's not, it won't be in the air. So, again be safe when you use in indexes for the arrays. It's your responsibility to keep inbound of our range.",
                    ""
                ]
            },
            {
                "title": "Syntactic Notes 2.4",
                "data": [
                    "Okay, so the arrays that we're talking about are actually called static arrays. There are also other kinds of arrays which are called dynamic arrays. We\u2019ll talk in more detail about them in one of our future models. But static arrays are arrays that are stored on our runtime stack, just like other variables. For example, if we have, let's say, main with an integer, an array and a double, since they're all stored on the runtime stack X. is stored first, after that comes our five element array, one, two, three, four, five, and right after that comes our Y.. So basically the size of this array must be known at compile time so the stack can be organized; the compiler could know where exactly this Y should come. That means that we have to supply the physical size, the number of bytes basically, that the array takes in our memory at compile time. We can say something like, let's have an integer X. and then in an array of integers and the double; we must say what's the physical size of this array.",
                    "",
                    "Let's sum up a few syntactic notes regarding static arrays; so a few important things. So, the most important thing I want to say here that: the array\u2019s physical size must be known at compile time, basically means it must be a constant and it must be given at declaration. We have to say int arr six, which is a constant int literal: it is known at compile time, it is given a declaration. Or we can have, let's say, const iny X. equals seven and then create an array of size X.. Again, it's a constant and it is known at compile time. We cannot do, let's say, int arr without giving any size or we cannot have, let's say, an integer N equal seven and then create an array of size N, again it's not a constant; it's a variable, that's also not legal. So, when we create, when we declare this array we have to supply the size of the array as a constant at the declaration line. I have a few more syntactic notes so I'll show you later, but for now let's go to implement our \u2018Above Average\u2019 problem.",
                    ""
                ]
            },
            {
                "title": "Above the Average 3.1",
                "data": [
                    "Okay. So now that we have the syntax of the arrays in C++, let\u2019s go back to the problem of calculating the grades that are above the average. So, as you recall, we have our class with four students, we have the grades, and we know that, not only, that the average is 79.75, we also know which grades come about the average. In order to implement that, we would obviously store the grades in an array, but if you recall we have to provide the physical size of the array at compile time. So, when we write the code, basically we have to know how many grades, or what the maximum amount of grades we\u2019re gonna have. Think it would be a very legitimate assumption to assume that a class won\u2019t exceed, let\u2019s say, sixty students. So, let\u2019s update a bit this problem. And the program first would ask the user to enter the number of students and the grades, saying not more than sixty. And then continue as we said before. Let\u2019s go implement it. ",
                    ""
                ]
            },
            {
                "title": "Above the Average Implementation 3.2",
                "data": [
                    "Okay. So let's start with implementing this program; so first, let's ask our user, cout \u2018please enter the number of students.\u2019 And we should also mention that we're expecting to get a number that is less or equal to sixty as we said we are assuming that the number of students is less than sixty so we can use a static array physically of sixty elements. So, let\u2019s say, no more than\u2026 you know instead of writing sixty hard coded, I think it would be better if we\u2019ll just defined constant variable. So const int max class size, I\u2019ll name it, and I\u2019ll set it to sixty and then I would just say, no more than, and I'll just take the max class size value. And that would be our announcement. ",
                    "",
                    "Okay. Now let's read the number of students; so let's create a variable num of students. And let's read whatever the user enters into this variable. So now we have to\u2026 Okay, so let's create an array of physical size of sixty so we can store the elements, the grades in it. So, int, let\u2019s call it grades list, and it's going to be the size of sixty. Okay so, now we have the number of students, the array and we should start reading the grades into this array. So for that we would obviously use some kind of a loop. Let's take a for loop for that, actually when we implemented the program of just calculating the average, we simultaneously both read the input and summed all the grades up in order to calculate the average. But now since we're storing the grades in a data structure, I think we can split this task into a few steps. So, first step would be reading the grades: so we\u2019ll probably have a form for that. Second step would be calculating the average, and then as we pass over the data one more time, will print grades above the average. So we're going to have three steps and not only like two; one for reading and calculating the average and then to print the ones that are above. We\u2019ll also split it to a reading step and the calculating average step that are separate.",
                    "",
                    "Okay, so let's start with reading the grade. So, let's have like a variable for a current grade: so curr grade and each iteration we're going to read the current grade, and we\u2019ll also want to store it in our array. So for that, we would probably iterate over the indexes of the array; they basically said the location where we put this grade in. So we'll have an ind variable, initially ind would be set to zero, each iteration we would increment or increase this ind. And we'll just put our current grade inside our grades list at the ind position. This is where our current grade would go to. ",
                    "",
                    "So, basically each iteration we read a grade and we store it in our grades list in the ind position. We start with index equals zero and each iteration we increase our index for the next element. So first time, firstly iteration, we insert our grade into grades list zero, second iteration grades list one, third iteration grades list two and so on. We\u2019ll keep on doing that while our index is inside the valid range for our array, which is no more than a number of students. So as long as index is less the number of students, we want to keep on going and reading. Let's try to trace only this piece of code that we have up to now so we'll make sure we get exactly how this works, why we stopped or controlled this loop exactly like that. Small details that, I think, by tracing we would get it with a better understanding.",
                    ""
                ]
            },
            {
                "title": "Above the Average Implementation 3.3",
                "data": [
                    "Okay. So let's take a look at our run time stack; let\u2019s assume it starts at address one thousand, first variable we have is number of students and then we have our array which has sixty elements in it, so we have sixty integers one after the other. So, that goes up to here; it starts at a thousand and four. First index would be zero then one, two, three, four, five, and so on up to, it's going to be fifty nine, right? Since it starts at zero, when it has sixty elements it would go up to fifty nine. After this array we have our current grade variable and our ind variable; they're all integers so each of them is four bytes. And let's start executing the program.",
                    "",
                    "So first we ask the user how any students are going to be in the class. Let's assume the user entered four, so number of students would be four and then we start reading the grades. So let's start executing the for loop, ind would be initialized to zero. We read the first grade, seventy one, and then we assign our grade lists. This is grades list with sign index zero of this array to be seventy one. Ind increases, we read the next grade, eighty six, and assign grades list one. In this case, this element here to be eighty six. Again ind the increases, we read the next element, sixty eight, and assign grades list two in this case to be sixty eight. Ind increases, by the way it is still, ind is still less the number of students; three is still less than four. We have one more iteration: we read another grade, in this case it's going to be ninety four, and we set grades list three in this case to be ninety four. Once more we increment our ind, this time when we check the boolean condition, ind that is no longer less the number of students; four is not less than four.",
                    "So we break out of this reading step. Basically, we read the entire sequence of four elements into our array data structure, right? All the rest of the elements in the array starting index four to fifty nine remain with the same value they had before that, we didn't change it. So, the logical size will name it is only four. Physically, this array is of size sixty; logically, it has only four elements in it and these elements are in indexes zero to three. Okay, let's go on to the next step where we calculate the average.",
                    ""
                ]
            },
            {
                "title": "Above the Average Implementation 3.4",
                "data": [
                    "Okay, so let's calculate the average. For that we would need to go over the elements in the array and to sum them up. Let's create a sum variable. We'll start by setting it to zero and then iterate over the elements and add each of them into sum, so we'll use a for loop. Again index would start zero, it would increase each iteration and we just keep on going while ind is less than number of students. So this basically is the way we pass over an array; we start our index with zero, we move it, increase each iteration and go up to the logical size of that array. Each time here we just want to add the current element into sum, so sum plus equals, lets access our grades list at the index position. This code here will then just add all the elements in the array up into are some variable. After doing that we can calculate the average.",
                    "So let's have a double variable; let's call it average and we can just set the average to be sum divided by the number of students. ",
                    "",
                    "One thing we should be careful though is when we divide sum and num of students, since they are both integers this division is basically integer division; it\u2019s div. So, we should cast sum and number of students in order to turn it into real division. So let's cast some to a double and number of students to a double and then this would be real division and not just integer division. So after we have the average calculated, we can output to the user that \u2018the class average is\u2019 and then just print the value of the variable average; that basically calculate the average here. Let's try to trace this, see if it all makes sense.",
                    ""
                ]
            },
            {
                "title": "Above the Average Implementation 3.5",
                "data": [
                    "Okay. So now we have our sum variable, we set it to zero and we start iterating over the array. We first add seventy one into sum, that would make it seventy one, then our index would be two. I'm not changing the index value but the index is going to change again, zero, one, two, three, four, as we iterate over the array. So, a second iteration index is one, we\u2019re adding eighty six to our sum, that would make it one fifty seven. Then we're adding the sixty eight, it would make sum two twenty five and eventually we'll add ninety four that would make sum be three nineteen, I think. Then we have our average variable and average is three nineteen divided by four, which is a seventy nine point seven five. We just print this value as the average. Now, we\u2019ll need to make another path over the numbers in order to figure out which ones are above the average. Let\u2019s go and do that. ",
                    ""
                ]
            },
            {
                "title": "Above the Average Implementation 3.6",
                "data": [
                    "Okay then. Now let's make one more pass over the data in order to print the ones that are above the average. Let's first start with announcing to the user that \u2018the grades above the average are\u2019 and now let's start iterating over our array.",
                    "",
                    "So once again, we'll have our index starting at zero, incremented each iteration and it goes as long as its value is less than number of students. And now we have to figure out if we want to print this current element or not, so we'll just if the grades list, our current element, grades list ind that's the element we're currently looking at, is as we asked it to be above the average, so greater than the value of the average. Then, let's just print this current element: cout grades list. And let's also space after each print. Yeah, so that should go over the entire set of elements and print only the ones that are above the average, spacing them. After we do all of that, let\u2019s just break the line so cout endl, and that should be it. ",
                    "",
                    "So we have our first iteration to read the grades. Second pass, in order to calculate the average and eventually, last pass over the elements in order to print the ones that are above the average.",
                    "Let's just execute it, make sure it all behaves as we expect it to. ",
                    "",
                    "Build success succeeded. Please enter a number of students. That would be four. Oh, we didn't ask the user to enter the grades. We'll fix it in the second; it\u2019s now waiting for the grades. So let's have seventy one, eighty six, sixty eight, ninety four. And then, it just says that the class average is seventy nine seventy five, and the grades above the average are eighty six and ninety four which is great. ",
                    "",
                    "Let's just add this message here before starting to read the grades. Let's ask the user enter the students\u2019 grades, separated by a space. I think that should do it. Let\u2019s test it. Okay. Building. Number of students is four. Here is our message so let's put seventy one, eighty six, sixty eight, ninety four, and so the average is, again, seventy nine seventy five and we also have the numbers that are above the average. Great.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 10,
        "module_name": "Strings",
        "file_name": "Module 10 Strings.docx",
        "transcript": [
            {
                "title": "Introduction 1.2",
                "data": [
                    "Today we're going to talk in more detail about the class string. We\u2019ve already seen the string data type. Today we are going to see some more abilities of this class.",
                    ""
                ]
            },
            {
                "title": "Initializing & Concatenating Strings 1.3",
                "data": [
                    "So far we\u2019ve seen that in order to use the string type we need to include the string library. Then we can declare a string variable in this case string str. And then we can initialize str with some textual data which is basically a line of text or a string enclosed in double quotes. Formally the double quote ABC is not a class string type it is formally a c string type. But C++ has a default an implicit cast from c string to the string class. So for us it would be considered to be just a string so we would have str equals ABC. And then we can also use the plus operator that\u2019s also a thing that we\u2019ve seen about strings. Plus basically let\u2019s us concatenate strings one after the other. So we\u2019ll do for example str plus DEF since str is ABC str plus DEF basically concatenates ABC and DEF giving us the text ABCDEF. So when we print it we just get ABCDEF.",
                    ""
                ]
            },
            {
                "title": "Reading Strings 1.4",
                "data": [
                    "For now we know how to initialize and concatenate strings using the assignment operator and the plus operator. Let\u2019s see how we can read a string from the user. So for example let\u2019s take a look at these few lines of code. If we print please enter your name ask the user prompt the user to enter their name we can then cin into a string variable. And afterwards we can cout this value this name this str. So when we executed we ask the user please enter your name the user says for example I don\u2019t know Donald Duck. And then kind of surprisingly when we print str we get only Donald back. If we take a look what happened here basically when we used cin in order to read the string it only read the first word out of the line that the user basically entered. And that\u2019s how cin works. That\u2019s the way cin was designed. It is skipping leading white spaces then reads the word and stops at the next white space. In this case cin read Donald into str. If we want to read an entire line and that\u2019s a thing we would like to do a lot of times we would need to use some other way of reading a string and that would be by using the get line the get line function. It works something like that we just call get line and get line has two parameters that\u2019s kind of surprisingly. The first parameter is cin itself and then the string we want to update the line with. So get line cin str basically reads an entire line basically a sequence of characters until the new line is pressed into the str variable. You\u2019ll see in later modules that get line can be given a different first parameter besides cin and for now what I want you to know is that this cin basically tells get line to read this line from the standard input. From the basically the keyboard from whatever the user enters. Later on you will see that there could be other inputs where or other streams that can give us input besides cin or besides the standard input. So if we execute this code here we would prompt the user to enter their name and then when the user enters Donald Duck get line cin str would read the entire Donald Duck into str. Then when we print str we would get Donald Duck printed.",
                    ""
                ]
            },
            {
                "title": "Indexing Strings 1.5",
                "data": [
                    "Ok so we already know how to initialize concatenate read a string. Let\u2019s see some more stuff we can do with strings. So strings are basically a sequence of characters just like arrays can be a sequence of array of integers is a sequence of integers. Array of double is a sequence of doubles and so on. So it would be natural to wonder if we can use this collection or sequence of characters just as we use arrays. Basically indexing separating single elements from this collection. And apparently we can do that. The syntax for indexing with strings is very similar to what we are doing with arrays. Basically we are just having str and then in a square brackets the index of the character we\u2019re we want to access. For example if we have a string str one initialized to ABCDEFG we can then access str one at the index zero that would be A. Str one index one that would be B. Str one index two that would be C. If we print them with a space between them we\u2019d get A space B space C. Basically each element in this string is of type char. So we can for example set ch which is char variable to str one index three. So in this case ch would be D.",
                    ""
                ]
            },
            {
                "title": "Slicing Strings 1.6",
                "data": [
                    "So we can basically take separate elements from a string using the indexing system indexing syntax just like arrays. We can do something even a bit more advanced with strings we can take a slice of a string out. That is the slicing syntax and it works something like that. In order to take a slice to take a sub string out of a string we use the sub str method. The way we call this method is str one dot sub str and then we have two parameters basically saying where this sub string starts and how long is this sub string. In this case this sub string would start at index three and it would be two characters long. So index three is a D two characters long the sub string is basically DE in this case. Now I believe you\u2019ve noticed that the syntax for calling this method is a bit different than what we are used to when we\u2019ve used functions. Probably you\u2019ve you probably thought that in order to create the sub string of str one starting at index three and it\u2019s two characters long. We should use sub str as a function and pass str one and three and two as three different parameters and that would obviously a right way of thinking. That\u2019s how we\u2019ve used functions so far. But then I\u2019ve said that sub str is a method and that\u2019s a bit different. We\u2019ll talk about it in much more detail when we speak about object oriented programming and classes and you will see that methods how exactly or how formally they are defined. For now I just want you to know that sub str is a method it is not a function so we are not calling it sub str with three parameters the string the starting index and the length. We are kind of passing the str one parameter in a different way. Str one is basically the calling object of this method. So sub str is aware of str one it is operating on str on it is taking the sub str of str one without us passing it as a parameter. We\u2019ve used the dot notation in order to make str one be our calling object. So we use str one dot sub str three and two. So we are calling the method sub str on str one and the additional parameters are three and two. If we\u2019ll for example print str one dot sub str three two that would print DE. I hope it makes sense we will use methods in other cases as well. Each time we are going to use a method I will explicitly say that this functionality is not a function it is a method. In this case sub str is a method. If we\u2019ll think about the result or the value of whatever sub str returns that is basically in this case it was a DE it was of type string. So we can do something like str two equals str one dot sub str two three. Basically taking the sub string of str one once again our calling object is str one. Starting at index two in this case which is the C and it is three characters long. So str or the result of this method call would be a string of length three CDE. And that would be str two so we are assigning str one dot sub str into a string variable into str two. If we\u2019ll print str two as expected it would print CDE which is the three characters string that sub str returned. So the slicing syntax is using the sub str method we have our calling object for example it could be the str variable and then two additional parameters come in parentheses. The starting location and the length of this sub string.",
                    ""
                ]
            },
            {
                "title": "Length 1.7",
                "data": [
                    "One last thing I want to say in this context here is to tell you about a new or another method that is defined in the string class. And that\u2019s the length method. The syntax is str dot length and then empty parenthesis. Once again it is a method so in order to calculate the length of a string you need to give the string as the calling object. So it would be I don\u2019t know str one or str two or whatever the variable name is dot length and then empty parenthesis and that would result as the length of the string. The number of characters in that string. So that is also useful if we want to figure out how many characters are in a string. If we want to iterate over the string or for any other reason. ",
                    ""
                ]
            },
            {
                "title": "Printing Backwards Introduction 1.8",
                "data": [
                    "Ok let's use this syntax in order to implement the problem. So this program would read the user's name and print this name in a reverse order. So for example we would prompt the user please enter your name the user would say Donald Duck and then it would print whatever so Donald Duck in a reverse order. So let's go ahead and implement it.",
                    ""
                ]
            },
            {
                "title": "Printing Backwards Implementation 1.9",
                "data": [
                    "Let\u2019s go ahead and implement this program. So first let\u2019s prompt the user cout please enter your name and break the line. And then let\u2019s read the user\u2019s name so for that let\u2019s first include the string library and let\u2019s create a string variable. Let\u2019s name it username. And let\u2019s read whatever the user enters into the user name variable. Since the user name would be first and last name it would probably contain spaces so let\u2019s use the get line syntax for that. So get line first parameter as we said it\u2019s cin basically saying that that input comes from the keyboard from the user. And the user name that is where we want it to read it to. So now we have the user name string. What we need to do now is what I was thinking is to create a new string with the reverse name in it. Let\u2019s create another string variable or actually since we\u2019ll just need to print this name we don\u2019t need to create a string we can just iterate and print in each iterator. But then we would need to go in a reverse order we need to go from the enter to the beginning. I think it is a good practice let\u2019s use a for loop. I will have an index variable and in this case we should initialize index to the end of the string and each iterator instead of incrementing index we will decrease index. We\u2019ll do something like index minus minus. So let\u2019s think of what value the index should be set to initially. That should be the index of the last character in the string. We can definitely use the length method for that so it would be user name dot length that\u2019s basically the number of characters in this string. But then if we think about it let\u2019s say we have a string ABC the length is three but the last character is in the index two right. Because like since it is a zero based index system it starts at zero and ends at basically length minus one. So let\u2019s set the initial index to username length minus one. That\u2019s the index that\u2019s the position of the last character in the username string. So initially index is the length minus one. Each iterator index is decreased and we keep on going as long as our index is greater or equal to zero. Basically it is still a valid index we can access. Each iterator we\u2019ll just print the current character we are at. So let\u2019s cout username at the index position so first iteration would print the last character. Then we\u2019ll decrease index and we\u2019ll print the character that comes before that. We\u2019ll decrease index and print the character that comes before that and so on until index equals zero in the last iteration where we\u2019d print the first character. At the end let\u2019s just break the line so cout end l here. And seems to be fine let\u2019s try to execute it make sure we didn\u2019t make any silly mistakes. So please enter your name Donald Duck and then it just prints it in a reverse order. Great another option here would be as I started before to create a string variable with a reverse name in it it would be a great practice for you to try to implement this version on your own.",
                    ""
                ]
            },
            {
                "title": "Comparing Strings 1.10",
                "data": [
                    "Ok so we know how to initialize how to concatenate how to read strings from the user. We can access specific characters using the indexing system syntax we can slice out a substring we can figure out the length. Let's see how we can compare strings. So obviously we can use a double equal signs in order to figure out if two strings are equal to one another or not equal to one another. But how about the less than. Can we also try and find whether one string is less than the other? For example if we have str one is A B C and str two is D E. Can we do something like if str one is less than str two? For example in this case we\u2019ll print that str one is smaller than str two otherwise we\u2019ll print that str one is not smaller than str two. What do you think it would happen if we try to execute these few lines of code? So one option it would figure out or let\u2019s say first start in the second option. So one way is that that could be a compilation error. The compiler won\u2019t approve comparing strings with less than operator. Another option is that that would be legal expression in C++ but then if so what do you think it would happen? Does str one is smaller is less than str two? Is ABC smaller than DE? So let me first tell you that it is not a compilation error. It is a legal expression in C++ it is a legal a Boolean expression actually in C++. But then when we talked about the less than operator we compared integers and doubles. I didn\u2019t explain too much it was kind of intuitive if four is less than seven or five point five is less than six point seven. But in case of strings it is not very straight forward. Is ABC less than DE? How do we compare strings? By their length? ABC is a three length string or DE is a two lengths strings so in this case ABC is not less than DE. Three is not less than two. But then if we try to execute this code we\u2019ll see that it would print that ABC is smaller than DE. In other words ABC is less than DE. So it is not comparing the strings by their length it has other criteria in order to determine whether one string is less than the other. And the criteria C++ chooses and it is common in other languages as well not only C++. The criteria by which we compare strings is by a lexicographical order. Other words in alphabetical order what string comes before that in let\u2019s say in a dictionary. So obviously ABC is before DE therefore ABC is less than DE. So in this case str one was less than str two that was true therefore it was it printed that str one is smaller than str two. So when comparing strings we can use the less than the greater than less or equal to greater or equal to operators. And it would do some kind of lexicographical ordering or in alphabetical ordering. In more detail the way we compare the way C++ compares strings let\u2019s say we have two strings ABCDEFG and ABCDXYZ. So when C++ tries to compare these strings it kind of iterates over the strings simultaneously and figures out the first location where they differ from one another. So first A and A are equal it goes forward to B and B C and C D and D and then the first position where they differ from one another is E and X. That\u2019s how or that\u2019s the point where C++ determines whether one of them is less than the other and the criteria is basically comparing the ASCII value of E and X. In this case E is less than X so the entire string ABCDEFG is less than the entire string ABCDXYZ.",
                    ""
                ]
            },
            {
                "title": "First Word Introduction 1.11",
                "data": [
                    "Let\u2019s try to write a program that reads from the user three words and then prints the one that comes first in an alphabetical order. For example let\u2019s prompt the user please enter three words separated by space. The user can then enter I don\u2019t know dolphin cat tiger. And then the program should respond with word that comes first in a lexicographical order. In this case it is cat. Let\u2019s go ahead and implement it.",
                    ""
                ]
            },
            {
                "title": "First Word Implementation 1.12",
                "data": [
                    "Let\u2019s start by prompting the user to enter three words separated by space. So please enter three words separated by space and let\u2019s break the line. And then let\u2019s read these three words. Let\u2019s first include the string library and then let\u2019s declare three string variables. So let\u2019s have word one word two and word three. And now we need to read three words into these variables. Since each of them is supposed to be a single word actually cin is a good way to read these words into these variables because cin separated by space. So we can just do cin into word one into word two and into word three. So now we have the three words in these variables. Now let\u2019s try to determine which one is the smallest. So I was thinking of having a multiway if statement here three cases. Checking if word one is the smallest if word two is the smallest or otherwise it is obviously word three. So let\u2019s have an if statement and let\u2019s figure out a condition that determines whether word one is the smallest of the three. So we can do something like if word one is less or equal to word two and also word one is less or equal to word three. If this is true basically it means that word one is both less than word two and less than word three. It is thus the smallest of them all in this case we\u2019ll just cout word one let\u2019s break the line. Otherwise let\u2019s do the same to check if word two is the smallest so if word two is less or equal to word one and word two is less or equal to word three. That means word two is the smallest of the three so we should print word two break the line. And otherwise I will just comment that word three is the smallest in this caser we\u2019ll just print word three. That\u2019s supposed to be good enough let\u2019s test and make sure we didn\u2019t do anything wrong here. So please enter three words separated by space so we have dolphin cat and tiger. And that it just prints cat and yea that is the smallest word of these three.",
                    ""
                ]
            },
            {
                "title": "Searching in a String 1.13",
                "data": [
                    "Ok so we know a lot of things about the string class. One more thing I want to talk about now is searching in a string. So assuming you have a string and you want to search if you have some substring contained in it. The syntax to do that is by using the find method. You call the find method on an original str string so you do str dot find. And then you pass as a parameter the string you are searching for in this case s. Let me show you how it works for example if we have str the original string ABCDEFG whatever you can call str dot find of DE. Basically searching for DE inside the original string the original str. In this case you can see that DE is a substring of the original string the find method would just return the starting index of DE. So A is zero B is one C and so on DE starts I think in index six so if we print whatever str dot find returns in this case it would just print six. Let\u2019s take a look at another call for the find method for example if we\u2019ll call str dot find on a string XYZ. In this case XYZ is not a substring of the real original str so find needs to tell us needs to return some value that says that XYZ is not found in str. Find returns in this case a constant named npos that is stored in the string namespace. In order to access the string namespace basically we use the syntax string double colons and then npos that\u2019s the constant name. So we can do something like if str dot find XYZ equals to the npos constant we can just print not found otherwise if it doesn\u2019t equal to npos basically it returns a valid index it is found. In this case when we are searching for XYZ obviously it would return that it didn\u2019t find the XYZ in the original str and therefore it would print not found. Let\u2019s take another look here. What do you think it would happen if we\u2019ll call the find method to search for CD in the original str? I believe you noticed that CD appears more than once in this original string. It appears ABCD and then B and another CD so we have two occurrences of CD. Any guesses on what find would return when we call it to search for CD? So maybe it would return arbitrarily one of the positions maybe it would return a collection of the entire indices. I\u2019ll just tell you that find the find method just returns where the first occurrence of the string we are searching for appears in the original string. In this case CD first appears at index two therefore it would print two. ",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 11,
        "module_name": "Pointers and Dynamic Storage Script",
        "file_name": "Module 11 Pointers and Dynamic Storage Script.docx",
        "transcript": [
            {
                "title": "Introduction 1.2",
                "data": [
                    "This module we're going to look at C++\u2019s pointers and how we can use them to expand the capabilities of what we do in terms of programming. And using heap dynamic variables using heap dynamic arrays. We're going to create some very useful code here and this code is going to be the basis for a lot of what we're doing in the future for both future programming and for data structures. It also leads into discussions on operating systems so this is a rather important topic that's a gateway to a lot of other things in the rest of the semester.",
                    ""
                ]
            },
            {
                "title": "Pointers \u2013 Why? 1.3",
                "data": [
                    "So the first question is why do we need pointers at all? And a pointer is a way that we can store a reference to an object that's either created on the heap or created on the stack. Now the stack is what we've been using to create all of our variables since we started programming. When we say something like index that's a variable that's created on the stack and we're going to talk a lot about what the heap is and how we can use it a little bit later on. But for right now there's something else that we can create and that is a heap dynamic variable. When we work with pointers we can have a pointer either point to a stack variable or a heap dynamic variable. And we can use the pointer as a way to store a large amount of information that's not associated with the function. One of the problems is when we start a function all of stack dynamic variables are allocated. And when that function ends all those stack dynamic variables are deallocated. But what if we wanted to have a variable that existed without having a function being called. We want the variable to continue to exist even after the function ends. We can create those variables on the heap but the heap doesn't have a name there's no way to name a variable that's on the heap. And the only way to do this the only way to have a reference to it is to have a pointer to that variable that's on the heap. So when we get into heap dynamic variables pointers are going to be really really critical. Also we can use a pointer to link two objects together. So the usual scenario that I like to use is imagine that we have some sort of object like a person object and we'll talk a lot about object orientation in the next module. But for now you understand that we can have some data structure that represents a person. Well what happens if one person gets married to another person? This means that we have to have an association relationship between two person objects and in order to do that we have to have a pointer from one person to another person. I would probably call that pointer a spouse pointer.",
                    ""
                ]
            },
            {
                "title": "What It Looks Like 1.4",
                "data": [
                    "In C++ we have to specify the data type. C++ is very type specific so we always have to specify the data type. And a pointer is going to be specified as pointing to a particular data type. So a pointer as we're going to see later on is really just a number that we store in main memory to represent where this pointer is pointing to. It's actually just a memory address but the problem with this is that all memory addresses are the same size. Unfortunately C++ doesn't let us take an integer pointer and make it point to a double you can't do that. You can't make an integer pointer point to anything other than an integer. So if we took an integer variable and we found out its address which we\u2019ll figure out how to do later on we could take that which is just simply a number it's an address and we could store it in a pointer. Now that has the effect of making the pointer point to the integer and that's exactly how we do this. The stored number represents the memory address of the item that we're being that's being pointed to. We can create pointers using a very simple construct it's just int star ptr. In the same way that we can create integers like int x we can create integer pointers by just saying int star and then giving it a variable name.",
                    "At that point on construction the pointer doesn't point to anything reasonable it's pointing to garbage in the same way that if we said int x we don't know the value of x. If we say int star ptr we don't know the value of ptr. It's pointing at garbage.",
                    ""
                ]
            },
            {
                "title": "Getting Pointers to Point 1.5",
                "data": [
                    "So we'd like to use these pointers to point to something and we can do that but we need to have some address that we make it point to. In another words we need to find an address of a variable and take that address and store it in the pointer and we can do that by using what's called the address of operator. It's a new operator that we haven't seen before but if we place it in front of variable that we have existing we're going to get back the address of that variable. Now the data type for the address of an integer the data type for that would be a pointer to an integer which is perfect because now we know how to create pointers. So we have a little bit of code here which is just simply creates a variable called x. Sets its value equal to one hundred and then we have a pointer and the pointer is set to point to x. Now that allows us to work with the x value but not use the variable name x which we're going to see later on is very important. We can use pointer ptr and access x as if we're accessing it through the pointer which is exactly what we're doing. But we're going to need to get the address of x first and we do that using the ampersand or what's called the address of operator.",
                    ""
                ]
            },
            {
                "title": "Accessing Data From a Pointer 1.6",
                "data": [
                    "Of course it would be pretty useless if all they could do was point to something. Really we want to be able to access the data that the pointer is pointing to and we can do that thankfully we have another operator known as the dereference operator and that's just the star. It's a unary star operator we place it in front of the pointer and what that means is we're going to follow the path that the pointer is pointing to. If you sort of imagine that the pointer is a signpost if you will on a highway. What we want to do is follow that sign to get to a destination and we can do that using the star operator. So here I have a little bit of code which I'm going to show you creates a variable x sets its value equal to one hundred makes a point makes the pointer point to x  and that's the same as in the previous section. But now we're going to go ahead and print out the value that the pointer is pointing to. Now this is not printing out the address of the pointer. If we simply said cout arrow arrow or output operator or whatever we\u2019re going to call it. Cout ptr we would get the address that ptr is pointing to. And the easiest way to think about this is if we printed out x we would get the value that x was pointed sorry the value in the x variable. If we printed out ptr we get the value in ptr but unfortunately that value is a pointer value which is just an address. If we dereference ptr as we have here in the code we are going to see that the value that's being printed out is actually the value that's in x. Because what we did was follow the signpost that ptr is pointing to and that took us to the variable x and now we're printing out the value in the variable x. And of course this falls on both sides both an L value on an R value we could talk about this being printed out or we can do an assignment statement where we actually change that value. That last line of code on your screen is actually going to change the value of x as if it were to say x equals twenty. And now we're seeing star ptr are equals twenty. It's the same thing because ptr is pointing to x.",
                    ""
                ]
            },
            {
                "title": "What if a pointer doesn\u2019t point to anything? 1.7",
                "data": [
                    "So when we first create a pointer just like when we first create any variable. We don't know the value that\u2019s in it. And that's a bad thing if we're talking about an integer. But it's a really bad thing if we're talking about a pointer because when we create a pointer it has some garbage value it has something in it it's not zero it's not unknown value. It's something and we don't know what that something is so effectively the pointer is now pointing somewhere where we didn't decide where it's pointing. And if we use that we have the potential for crashing our program or doing horrible horrible things inside the computer. So we don't want to do that. So whenever we create a pointer or whatever we have a pointer that's not pointing at something valid we should make that pointer point to a special place. And that special place is known as NULL or null ptr. It's capital N U L L or null ptr all lowercase. But it's really important that we always make sure that the pointer is either pointing at somewhere valid like a variable or somewhere on the heap or it's pointing at NULL or null ptr. If it's ever pointing somewhere else then we're never going to be able to check to see if it is pointing at something valid because some arbitrary location in main memory will look exactly like a variable on the heap or a variable in the stack. There will be no way to differentiate between something that's valid and something that's invalid.",
                    "So we use null ptr as a safety mechanism. Whenever we have a pointer and it's not pointing at somewhere that we know we make it point to NULL or null ptr.",
                    ""
                ]
            },
            {
                "title": "Defining Multiple Pointers 1.8",
                "data": [
                    "So there's one little problem with C++ that's a little bit of almost like an eccentricity of C++ it is a little strange. If we define multiple pointers on the same line the star doesn't associate with the data type it actually associates with the variable name. And I know this makes absolutely no sense but if you take a look at this sample code you'll see that here we're creating three pointers and one x one integer. So the three pointers need to have their stars immediately preceding the variable name and this actually doesn't even matter if the star is before the space immediately adjacent to the int or if it's to the right and immediately adjacent to the name of the variable. But it's important to recognize that the star does not make the line create all pointers it makes only that variable a pointer. So in this case we have ptr one ptr two and ptr three are pointers to integers. But x is just an integer on its own.",
                    ""
                ]
            },
            {
                "title": "Let\u2019s Get Dynamic 1.9",
                "data": [
                    "So let's talk about dynamic memory. We've already talked about pointers and having them assigned to static variables or stack dynamic variables. But now we're going to talk about what pointers are really useful for and that is creating heap dynamic memory. We're allocating heap dynamic memory and having a pointer point to that heap dynamic memory so that we can use it. Now heap dynamic memory when we create it it has no names. So we have to use pointers in order to access that heap dynamic information. And there's no way to find anything on the heap once it's created and allocated you'll be given a pointer to that memory location and then you have only that pointer to use to access it. If you ever lose track of that pointer you've lost track of that memory. And that's a bad thing it's called a memory leak. So pointers can point to heap dynamic memory heap dynamic memory is allocated whenever you ask for it and it remains allocated until you destroy it. Which means that it can survive a function call what we can do is create this heap dynamic memory or allocate this heap dynamic memory in function A. And then function A can end and we go on to do function B and heap dynamic memory is still allocated. What we can do is pass a pointer between function A and and its parent function and then back to function B. And the variable still remains allocated in that same memory location and the only thing we need to do is keep the memory location keep the pointer. Now the unfortunate fact here is that while there's a lot of capability in heap dynamic memory there's also a lot of danger. If you don't deallocate your heap dynamic memory then you have what's known as a memory leak. What this means is that for every variable that you allocate on the heap you must remember to destroy that variable. It is not destroyed automatically and this is a huge downside to C++ and languages does that have support for heap dynamic memory. There are newer languages which prohibit the use of of certain features like heap dynamic memory. And there are other programming languages that do so in an more automated way like newer languages like Java will allow you to create heap dynamic memory but it keeps track of all of it and destroys it on its own time when you're not using it anymore. So C++ leaves a lot in your hands you have to do a lot of the work and it's very useful but you have to be very very careful. Memory leaks themselves won't cause your program to crash at least not immediately but over time a small amount of memory that's wasted by your program will add up. And if your program runs continuously for months and months or even weeks and weeks. It's quite possible that you're going to run out of memory eventually and then your program will crash. But it's insidious it's going to take a really long time before that happens.",
                    ""
                ]
            },
            {
                "title": "Well, That\u2019s New 1.10",
                "data": [
                    "So how do we create new memory on the heap? How do we create these heap dynamic variables? Well there's a function that we can call it's actually an operator technically in C++ and it's called new. All we do is say new and then the data type that we want to be created and we get back a pointer to that new variable that's created on the heap. It doesn't have a name it will never have a name. So the only way that we can refer to it would be via a pointer. Again once memory is allocated on the heap it's not deallocated until you deallocate it or the program ends. If the program ends all the memory for that program is deallocated. If you ever lose track of a pointer of a pointer to a heap dynamic variable or if you ever lose track of the dynamic variable it immediately becomes garbage on the heap and you can't use it anymore. And that's your memory leak. But for creating a new variable the only thing they have to do is call new and tell it the data type that you want in terms of integer float double or any of the other data types. And it is going to give you back a pointer to that data type so you just need to store that pointer. And then you have all the access that you need by the dereference operator you can make copies of the pointer that doesn't make copies of the data necessarily. And you can use the pointer to access the variable that's created on the heap. Remember that it doesn't get destroyed until you specifically say it should be.",
                    ""
                ]
            },
            {
                "title": "For every new, there must be delete 1.11",
                "data": [
                    "For every new there has to be a delete. If memory is allocated it's going to have to be destroyed and once you destroy it you can only destroy it one time. So you can destroy it once and only once. Now here's the double edge sword that we have the problem with. We have to destroy it once and only once if we don't destroy it that's a memory leak and if we try to destroy it twice that's double delete. Memory leaks we know they're going to take a long time to show up as problems. The program will continue to run. It'll just take up more and more and more memory eventually running out of memory and then it'll crash. But double delete is much more critical. As soon as you do a double delete the first time you delete the object the first time you delete the integer or whatever it is you're doing. The memory is returned to the operating system and the operating system says it can be reallocated to someone else but that second double delete tells the operating system that it can take back a memory that it already owns. And the operating system has a real problem with this it says but I already own that memory something's wrong with your program and your program will immediately crash.  Your program will end and on a Windows machine for example you'll get a pop up box that says Microsoft is sorry but your program has performed and illegal operation and that's the end of your program. Now we can delete a memory location or we can delete a heap dynamic variable I should be specific and say by just calling the delete operator. So we say delete ptr that does not do anything to the pointer itself. Delete ptr doesn't change the pointer all it does is return the memory that's being referenced by that pointer returns it back to the operating system. The pointer still points to the same location it's just you don't own that location anymore. So again it's important to then take the pointer and make it point to NULL or null ptr right after you do the deletion so you don't accidentally double delete. If you accidentally do a delete on NULL or delete on null ptr It has absolutely no effect. There's no crashing there's no problem. The operating system and C++ just simply says that has nothing to do so I'm done. And deleting null ptr is perfectly safe. So there are certain circumstances that we're going to see over the course of the rest of the semester that you'll see where we delete null ptr and that's perfectly ok. What's not ok is deleting the same pointer twice. ",
                    ""
                ]
            },
            {
                "title": "What About Arrays 1.12",
                "data": [
                    "So now that we understand that we can create variables on the heap. There's not really a whole lot of benefit in creating a single variable on the heap but what there is a huge benefit is creating heap dynamic array. If you remember back to our discussion on arrays we said that the size of a stack dynamic array would have to be known at compile time and has to be static. So when we created our arrays back in the previous modules you created them with a known size like twenty five or fifty it was some constant values literal value. But what if we didn't know what if we didn't know how many elements we needed to create until we got to the point that it was time to actually create them. So for example we might ask the user how many things are you going to tell us how many objects do you need to store in this array? Or we might pretest how many objects we're going to put into an array. We might read in a file and see how many integers there are before we're even loading them into the array. And if we do that then we have to create a heap dynamic array because a stack dynamic array does not allow us to create it with a dynamic size. It has to be a static size. So working with a heap dynamic array means that we are going to have this since it\u2019s created on the heap. It means we're going to have to have a pointer to point to that array. Since we're working with pointers another big benefit is that if later on we decide that now we don't have enough space in this array and we need to make it bigger that's not really possible by the nature of the way that arrays work. But what we can do because this is just a pointer is we can temporarily create a new array that's slightly larger. Copy over our values into that new array we can delete our old array and make our old pointer point to our new array. And that effectively resizes our array because the pointer will still be the same pointer. But now the size of the array will be significantly larger and that's an important factor we're going to see this later on. We're going to create objects which store information and they're going to expand on how much information is stored based on the inputs. But for now what we want to see is just a simple format for asking the user how big the array should be remembering that number and then creating an array. And we can create arrays on the heap just by specifying the data type we call new again we specify the data type and then we use the array operator the square brackets operator and tell it the size. But now the big difference is the size doesn't have to be static the size can be decided at run time. When we create the array of course we have a new operation we're going to have to have a delete operation and the delete operation is a little bit different just slightly different. Because when we are working with arrays we\u2019re going to have to tell C++ that we don't just want to delete one element we want to delete the array. And we do that with the square brackets operator after delete. So this would be delete square brackets array. Now don't get ahead of ourselves and start thinking that we can delete individual objects inside of an array. We really can't do that it's not really safe to do that. What we want to keep in mind is that if we're deleting a single object we use delete ptr. If we want to delete an array we use delete square brackets arr.",
                    ""
                ]
            },
            {
                "title": "What can we do with heap-dynamic arrays? 1.13",
                "data": [
                    "So what can we do with heap dynamic arrays? Thankfully everything that we can do with normal arrays. All the normal arrays we've been working with are actually just pointers to stack dynamic arrays. So even when we created those old stack dynamic arrays that you did in previous modules you actually had a pointer to the base of a stack dynamic array. Well now we have the pointer to the base of a heap dynamic array and we can do everything that we could do with the old stack dynamic arrays. There's really no difference heap dynamic arrays work just the same way as stack dynamic arrays. We can still use the square brackets operators to access each of the individual elements. We also now if we wanted to could use the star operator dereference operator to access just the first element. So we see that there's really no difference between a heap dynamic array and a stack dynamic array.",
                    ""
                ]
            },
            {
                "title": "Pointer Arithmetic 1.14",
                "data": [
                    "One of the fun things that we can do with pointers is pointer arithmetic and this is especially useful if we have a pointer that's pointing at an array. C++ allows us to manipulate pointers using standard arithmetic operators like the plus sign and the minus sign and the plus plus and the minus minus sign. And what we can do is add on to the pointer. Now C++ understands enough about what the pointer is pointing to to make some interesting extrapolations. What it does is for example if we added zero to the pointer that would obviously move it no positions. But if the pointer is pointing at in an array then we can move the pointer up one element of the array by adding one to it and that's possibly very helpful. If you want to use pointer arithmetic to access the elements of the array you can make a temporary pointer and then progress through the array in a loop adding one to the pointer each time and that would take you through all the elements of the array. You don't have to worry about how large a single element is. If this for example is an integer each element is going to be four bytes. If it's a double each element is going to be eight bytes. But that's not something that you have to concern yourself with because with pointer arithmetic the pointer data type is known because we can only have an integer pointer pointing to an integer. So when we add one to that integer pointer it's going to move up one element which means either four bytes for an integer or eight bytes for a double. If we add five to it it moves of five elements. But the important thing to recognize now is if we change where the pointer is pointing to then the zero element or the square brackets zero element the first element in the array will now be different. So if we have an array with the number of elements and we add five to the beginning of the pointer to the pointer at the beginning. It's going to move up five elements which makes the zero element now what is actually the fifth element in the real array. So that has to be taken into consideration you should always remember where the beginning of the real array is. If you want to move up just one element of the time we have the plus plus operator and the minus minus operator and those can just move us forward or backwards an individual element of time. So something like ptr plus plus is a really useful tool for accessing the array.",
                    ""
                ]
            },
            {
                "title": "A Real Example of a Growing Array 1.15",
                "data": [
                    "Hello everybody I wanted to show you a simple example of how we could create a dynamically resizing array or rather how we can dynamically resize an array. And I wanted to do that by starting out with just a simple array its size is 10 and it holds some stuff you can see the code on your screen. I hope we're looking at just an array that holds 10 things and we've gotten to the point where we'd like to add one more element. So we'd like to add that one more element into the array and unfortunately because the array is already full we don't know how it's possible that we could add any more elements into this array. So what we're going to do is write a very simple function which is going to resize the array and we're going to give it the array itself as well as the current size and the new size and we're of course going to have to go ahead and write that function. So it's not going to return anything and it's going to be called resize array and it's going to take the array as a pointer now we have to recognize that the array will change. So it's important that we pass that array by reference or rather that pointer by reference because in this case the array is going to have to be resized and the only way that we can resize an array is by destroying it and creating a new one in its place with the old data. So we have to take into account I'm going to call this current size we have to take into account that that array pointer will actually change which means it does have to be passed in by reference and we're going to take in new size. So we're going to create this function which takes this array the current size and the new size and the purpose of this function is to grow the array into a new size and we can talk about how to shrink the array we can do that as well but that's not the core of what we're trying to do here. So I'm not going to focus on that so when we look at this the first thing we should recognize is that we're going to have to create a new array. So we're going to need a temporary pointer and that temporary pointer should point to an array of size new size so we're going to create on the heap a new array that is pointed to by temp so temp is going to point to an array of new size and then we've got to go through and copy over all the elements of the array into the new array. So one by one we're going to have to go through the old array and copy the elements and we can do that just by saying temp at I is equal to ARR at I. Okay so we've got now two arrays the original array ARR and new array pointed to by 10 and they effectively hold the same data and the only difference is that ARR is completely full and temp now has a little bit of extra space left. So temp has some free space which we can recognize as the new size of the array and then we're going to just take care of deleting the old array so now we've done a new operation we know that if we don't do a corresponding delete operation we're going to have a memory leak or garbage on the heap. So what we can do to take care of that is delete the array ARR which cleans up the memory space pointed to by arr now obviously that means everything in there is deleted but that's ok because we've made a copy of all that into temp. And then here's where it's really important that we pass this array in by reference we're going to make ARR equal to temp that's effectively resizing the whole array so right now those couple of lines of code there what we've done is created a new array temporary array of the new size that we want we copy over each of the individual elements and then we delete the original array and make the original array point to the temporary array. The effect of that is that we've resized the array now obviously this is a very involved operation and it's going to have to go through each element of the array in order to make the copy. So we don't just want to go up by a small amount what we'd like to do is say that the new size should be something like double the size so down here in main. I'm going to say that the new size should be the size times 2 and then I can say size equals new size because we've now updated the size of the array and we can put our new element into the additional position. So ARR at size plus plus is equal to one more and that has the effect of taking our original array which was only ten elements and now storing the same information in a new array that has 20 elements and then storing the position 10 which of course would be the eleventh element the element that we're trying to store. So we have the capability now not just to create a static sized array we have the capability to create an array that can grow and we can do that on the heap.",
                    ""
                ]
            },
            {
                "title": "Someone\u2019s Done This Already 1.16",
                "data": [
                    "Now if you're thinking that what you saw was a lot of work. It was and somebody has already done it all for you. Somebody took all the operations that need to be done to expand and an array and keep it up to date and bundle that into what's known as a vector and the vector is a component of the standard template library which has a lot of components which we'll talk about throughout the semester but the vector inside the STL is an array that dynamically grows whenever we needed to. So the key functions inside the vector are the size function obviously which tells us how many elements we've put into the vector. So that's nice because we don't need to take care of how recording how many elements are inside the vector and we use the push back function to add elements into the vector. That's automatically resizing so we don't have to worry about how many things we can put into the vector.",
                    "We just keep putting things into the vector. Now of course we're going to have to pound include vector and we'll have to create it when we created the notation for creating it is a little bit odd we have to tell it what data type we're going to be storing inside the vector so in this case we're restoring int. Now that doesn't mean that we could store other data types so if we get to the point where we want to store floats or doubles or chars or any other data type we can literally store anything in the vector whatever data type you want. So we create this vector called V and we're going to push back one hundred elements each of which is the value of the element position times one hundred just for the sake of showing something. And then we're going to go through that vector and print out each of the individual elements. So you can see that we're still using the square brackets operator the same way that we were using on the arrays we can still use the square brackets operator on the vector class and now we have something that takes care of the size. So the vector class is one that we're going to really want to get familiar with because we're going to be using a lot throughout the semester. We're also should understand how it works internally and we will in a homework assignment or in a laboratory assignment. But the the vector is a really useful data storage class inside C++ and this is what we'd like to use going forward for storage of a lot of data we just use it whenever we don't know exactly how much data we're going to store but there's a lot of it.",
                    ""
                ]
            },
            {
                "title": "But Wait\u2026 There\u2019s More! 1.17",
                "data": [
                    "Ok so we've seen the vectors and we understand how vectors work. There's one more thing that was added in C++ in 2011. Obviously C++ is dynamic it's still being updated there's a lot of work being done to it but this was added and it was actually taken more from Java and some other languages because C++ realized that they were missing this capability and it's called ranged for loop. The ranged for loop has a really strange syntax but that's because it was borrowed from other languages and what you see is I have the same code from as from the previous section and you can see that I've changed that last for loop now and this is what's called a ranged for loop because we're progressing over the entire range of the vector. So this is a very common occurrence we want to do something for each element inside the vector. So there's going to be some number of elements inside the vector in this case we have one hundred elements which we loaded up in a previous line and now we want to do a task for each element inside the vector. And in fact other languages call this a For Each loop but C++ is called the for loop so we specify what data type we're going to get out of the vector in this case it's an integer we specify a variable name for the individual element as we go through the loop in this case I am using I. And then we specify a colon and the name of the vector that we'd like to progress over. And from that point inside the for loop we have access to this variable I which is a different element of the vector each time we go through the loop. So in the first iteration through this loop I is equivalent to v square brackets zero and in the second iteration loop I is equivalent to V square bracket one and so on and so forth until we get to the end. But it saves us having to worry about which element we're specifically working on and it also saves us from having to check to see that we haven't gone off the end of the vector. So the format for this for loop while being a little bit strange actually can save us time and protect us from making a lot of mistakes.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 12,
        "module_name": "Recursions and Mathematical Induction",
        "file_name": "Module 12 Recursions and Mathematical Induction.docx",
        "transcript": [
            {
                "title": "Conclusion 1.18",
                "data": [
                    "[PART 1]"
                ]
            },
            {
                "title": "Mathematical Induction Overview 1.2",
                "data": [
                    "Hi there, hope you're having a great day. Today we're going to talk about recursions; recursions are basically a very powerful problem solving technique that we're going to explore and see how it works. But before we do that, let's talk a bit about mathematical induction. I know the two are very related to one another; there is a very close relationship between mathematical induction and recursion. I\u2019m not sure exactly how much you're familiar with mathematical induction so let me do a short overview of mathematical induction.",
                    "",
                    "First, so methodical Induction is a technique to prove statements; it's a mathematical technique to prove statements. Basically it's used to prove universal statements that are of the form P of N is true for any natural number. So, for all of the natural numbers P of N is true. This is the form of statements we use mathematical induction proving technique. When we prove using mathematical inductions or there are basically two steps in our proof: there is the base case and there is the inductive step. In the base case, we typically prove that P of one is true, that our statement is true for the smallest natural number for one P of one is true, that's the thing we do in our base case. ",
                    "",
                    "In the inductive step, we show that P of N minus one implies P of N. So, if we know P of N minus one is true, we can also show that P of N is true too. And we show this statement P of N minus one implies P of N for any natural number greater than or equal to two. When we use these two steps to prove a universal statement as we said, basically we're not showing in a straightforward manner that P of N is true for all natural numbers but we say that after proving the base case and the inductive step we can conclude that P of N is true for all natural numbers. And let's get convinced that these two steps basically imply that P of N is true for any natural number.",
                    "",
                    "So, let's see what we've proved and what conclusions can we make out of it. So, obviously when we prove that P of one is true we can say that P of one is true. But then we also prove that P of one implies P of two, now since we already know that one is true, this implication would lead that P of two is also true.",
                    "And we've proved this P of N. minus one implies P of N, not only for one and two but for any natural number so we also prove that P of two implies P of three given that we already have concluded P of two is true. These two together would lead us to say that P of three is true and once again P of three implies P or four and P of three is true, so P of four is also true and this keeps on going. And that way we can just say that P of one, P of two, P of three, P of four, or basically any P. of N for all natural numbers are true. That's like the bases of mathematical induction.",
                    ""
                ]
            },
            {
                "title": "Mathematical Induction Example 1.3",
                "data": [
                    "Okay. So, let's use this technique in order to prove the following claim. We\u2019ll show that for any natural number N: two N square plus five N minus six is greater or equal to zero. Let's go ahead and prove it.",
                    ""
                ]
            },
            {
                "title": "Mathematical Induction Implementation 1.4",
                "data": [
                    "Okay. So we're going to prove that for any natural number N, two N square plus five N minus six is greater than equal to zero. Before we start just know that this statement here is of the form for any natural number and P of N is true, where this here are P of N: two N square plus five N minus six greater equal to zero, that's a P of N. Okay. ",
                    "",
                    "Let's start to disprove here; know there are a lot of ways to show this statement here. But we\u2019ll use induction, you know, in order to prove it. So, it's first state that we will prove using induction on N. And as we said there are two steps: there is the base and there is the inductive step. ",
                    "\nSo, let's start with the base. The base: we are supposed to show that P of one is true; basically, that this thing here is true for any cause one. And that's quite easy because for any cause one, we get that two times one square plus five times one minus six basically equals to one which is greater or equal to zero just as required. Now, let's try to show; let's try to do the inductive step.",
                    "",
                    "So for the inductive step, we said that we're going to assume that P of N minus one is true, and under that assumption we'll show that P of N is also true. So we assume that P of N minus one is true, that means we should just take N minus one instead of N in our statement; two N minus one square plus five N minus one minus six is greater or equal to zero. So, that's our assumption. And we\u2019ll show that under that assumption two N square plus five N minus six is also greater or equal to zero. So, this here is our assumption and this here is what we need to show. And we\u2019ll use our assumption for, to show that thing. ",
                    "",
                    "So, okay. So let's see; two N square plus five N minus six. Let's see how we can make it using our assumption. So hopefully it would just be equal to two N minus one square plus five N minus one minus six. If these two expressions were equal, using our assumption we can just say that the right hand side is greater or equal to zero and therefore two N square plus five N minus six is greater or equal to zero.",
                    "Unfortunately, they're not equal because of this thing here is, and I'll simplify it, it goes, gets to two N square plus N minus nine. ",
                    "",
                    "So, we can fix this expression for it to be equal and for that we would need, okay, so two N square, we have on both sides. On the left hand we have five N, on the right hand we have, and so we need to add four N. On the left hand we have negative six on the right hand we have a negative nine so let's just add three more, and now these two things are equal to one another. So, now that's very easy because, it's very useful actually, because we know that this thing here is greater or equal to zero; that's our inductive assumption. We also know that four N plus three, also, is greater or equal to zero because N is greater or equal to two, is a positive integer. So, if we multiplied by four and add three we would definitely get a positive number.",
                    "Therefore, these two add-ins, basically, are greater equal to zero plus zero, which is their zero. All together, we get that two N square five N minus six is greater or equal to zero. So, the inductive step showed that given our assumption: that's two N minus one squared plus five N minus one minus six is greater or equal to zero. Given that assumption, we can then say that two N square plus five N minus six, itself, is greater or equal to zero.",
                    ""
                ]
            },
            {
                "title": "Strong Induction Overview 1.5",
                "data": [
                    "Let's talk about a variation, a more general form of induction, called strong induction. Strong induction is also a mathematical technique to prove universal statements of the same form, P of N is true for any natural number N, but then the technique is a bit different. This technique also has two steps: the base case and the inductive step.",
                    "",
                    "Base case, same as the regular mathematical induction, we prove that P of one is true.",
                    "But the inductive step would be a bit different: in regular induction we just prove that a P of N minus one implies P of N, in the strong induction we prove that if P is true for all values more than N, that implies that P of N is true. ",
                    "",
                    "",
                    "So, our assumption here is stronger instead of just assuming P of N minus one and using that a single assumption to prove that P of N is true, here we assume that P is true for all values smaller than N. Basically that P of N minus one is true, and P of N minus two is true, and P of N minus three is true and all values of P are true up to N; all of that together should imply that P of N is true. We have a much stronger assumption here to show that P of N is true; that's why it's named strong induction. ",
                    "",
                    "Let's see why this technique basically can be used in order to show that P of N is true for all natural numbers N. So, once again we prove that P of one is true, in our base case, that means that we can conclude that P of one and is true. And then we show that P of one implies P of two all values smaller than two are true, basically P of one, means P of two is true by showing this statement using the fact that P of one is already true, we can conclude that P. of two is true. Then we also show that if P of one and P of two are true, then P three is true. Our assumption, the stronger assumption for P of three was that all values smaller than three are true; P of one and two are true. That implies P of three is true and, in this point of time, we already know that P of one is true and that P of two is also true. And therefore, we can say that P of three is true. Moving on, we showed that P of one, P of two, and P of three, all of them together are true that implies P of four. Again, we already know that P of one and P of two and P of three are all true. Therefore, we can say safely that P of four is true. I believe you see that these things can go on, and that way we basically show that all P of N\u2019s are true for all natural number N\u2019s.",
                    ""
                ]
            },
            {
                "title": "Strong Induction Example 1.6",
                "data": [
                    "Let's use the strong induction technique to prove some claim. So let's show that every natural number N can be written in the form of two to the power of I times J, where I is a non-negative integer and J is odd. Basically, we\u2019ll try to show that N can be written as a power of two times an odd number. For example, forty: we can write forty as two to the power of three, which is eight, times five. So, it's a power of two with the non-negative integer exponent times an odd number. That is true, we should show it not only for forty but for all natural number N. For example, for six: six can be written as two to the power of one times three, and seven, for example, can be written as two to the power of zero times seven. And we\u2019ll show that any number, any natural number, can be written in this form: some power of two with a non-negative exponent times an odd number. Let's go ahead and show it.",
                    ""
                ]
            },
            {
                "title": "Strong Induction Example 1.7",
                "data": [
                    "Let\u2019s show that every natural number can be written as a power of two times an odd number. Once again you can see that this statement is of the form for any natural number some P of N is true; this is our P of N and know that N can be written in some form.",
                    "",
                    "Let's go ahead and prove it. Actually before we start proving using strong induction, let's see that just the regular mathematical induction can't really help us here. So, in regular induction we would probably say that we assume that P of N minus one is true or basically N minus one can be written in the form of two to the power of some I times an odd number J, so let's assume we have that. And given that, we should show that N is also some power of two times an odd number. The only straightforward way I'm thinking of combining N with N minus one is just adding one to it, but then if N minus one is two to the I times J, if I'll add one to it, I won't get something of the form some power of I times J. But then if we know that this thing is true, not only for N minus one but for all other values of N. For example, I know that N is two times N by two. So, if I know that N by two, for example, is two times two to the power of I times J, then N would be two times two to the power of I times J, which would be some power of two times an odd number. So, using the assumption on a smaller value than N minus one, in this case N by two, seems to be helpful more than using the assumption of N minus one. That's why a strong induction here is better. ",
                    "",
                    "Let's use a strong induction to show exactly that. Okay. So, a proof will prove by a strong induction on N. And once again, we have two steps: the base and inductive step. So for the base, we should show that P of one is true. So, for any cause one, we should show how we can represent one as some power of two times an odd number. So, that's very easy cause one is two to the zero times one. So, if we take our power of two to be zero, the exponent to be zero, and our odd number to be one we get that one is basically two to the zero times one, just as requested. And now, let's do the inductive step. In this case, we should assume that P of N is true for any value smaller than N. And using that assumption, we should show that P of N is true, so let's have this assumption ready. So we assume that every K smaller than N can be written in the form of K equals two to some power of I times J, where obviously I is a non-negative integer and J is odd. So, this is our assumption; it's cement. And now we show that N itself can also be written in this form. ",
                    "",
                    "Now we show that N can be written as two to the power of I times J, obviously other I's and J\u2019s here, with I and J as described. So, this is what we need to show. So, we\u2019ll use this assumption here in order to show that N can be represented this way and this assumption is for every value of K; for K equals N minus one, N minus two, N minus three, and so on. Also for K equals N by two, which would be the value of K that we would like to use this assumption on. Yeah, so let's do that. The only thing is that we need to take care, or in other words, not every N has an integer N by two; odd numbers you can't really divide by two and get another integer value after this division. So, let's separate to the two cases when our N is odd and when it's even and do all of that. So, we have two cases to take care of: the first is that N is odd; the second is that N is even. So, for the second case that's the main part of this proof is when we're going to use the inductive assumption on N by two. ",
                    "",
                    "The first case is quite easy when N is odd, obviously you can represent it as a power of two times an odd number, just take the power of two to be zero, so it would be one times N itself. So, in this case, if we take I to be zero and J to be N, we get that N equals two to the power of zero times N, which is obviously N, and I and J are as requested; so, we're fine. ",
                    "",
                    "In the case where N is even, now we're going to do what we basically said we'll do here in this case. Since N by two is less than N, by the inductive hypothesis, if we take K to be N by two, we get that N by two is then equal to two to the power of two to the power of some, let's say, I prime times J prime for our non-negative I prime and all J prime. And now we can, given that, we can present N in other requested form. So, since N is basically two times N by two, in the case where N is even, that is exactly the case. We get that N is two times two to the power of I prime times J prime, which is basically two to the power of I prime plus one times J prime. Therefore, if we take our I to be I prime plus one, the value that the inductive assumption gave us for the power of two for N by two plus one and J prime to be or our J to be the same as J prime. Then, we get a N written in the requested form. That concludes our proof here using strong induction.",
                    "",
                    "[PART 2]"
                ]
            },
            {
                "title": "What is Recursion? 1.2",
                "data": [
                    "Recursion is basically a term used in several disciplines. In computer science, recursion is a problem solving technique where we solve a combination of problems. It is a very closely related to the mathematical induction that we just spoke about. The two are similar in some aspects; one of the main aspects is the fact that both kind of combine smaller instances for a larger one. So if we look at how we proved by induction. We used an assumption of P on smaller values in order to show that P is true on a larger value. Same thing kind of we're going to do here with recursions. So when we want to design a recursive algorithm, we're going to give you a very high level description of how we're going to design recursive algorithms; later on we'll see if you will demonstrate it using a few examples and you get the hang of it. But high level speaking, when we develop a recursive algorithm, just as induction, it has two steps: there is the base case/base step and inductive or recursive step. ",
                    "",
                    "Now when we make recursive algorithm when we use recursion, we use it as we said to solve problems. Not like induction where we use it to prove statements, in recursion with all problems. because in computer science we solve conventional problems were in mathematics we prove statements. So it would be very similar but then if in induction in the base case we proved the statement for the smallest possible value, in recursion we're going to solve the problem for the smallest possible input. So that's what we're going to do in the basis we're going to solve our problem for the smallest possible input. In the recursive step, we're going to assume that when we call the function on smaller inputs it does what it has to do it does its job. And using that assumption will try to figure out a way how to combine calls for smaller instances, to solve the problem on a given input. Let's see how we do that.",
                    ""
                ]
            },
            {
                "title": "Print Ascending Problem 1.3",
                "data": [
                    "Okay. Let's try to use this technique in order to solve the following problem. Let's try to write a recursive implementation for the function of print ascending. Print ascending is supposed to get two arguments, a start and an end, both are integers and as the name kind of says, it should print the numbers from start to and you know ascending order. So we will assume that storage is less or equal to N, so there is like a range of numbers, a valid range of numbers to print and print ascending should then print the numbers from start to end. ",
                    "",
                    "For example if we'll call print ascending one and four, it would then print the values one, two, three, four in an ascending order. Once again, we would need to make a recursive implementation for this function, obviously an iterative one is very straightforward, but let's try to practice the recursion technique on this example here to see how we can then define recursive functions. So, as we said there are two steps in solving or creating/developing a recursive algorithm; first one is the base case where we are supposed to solve the problem for the smallest possible value/smallest possible input. So first we need to define what's the smallest possible input, or maybe before that we should even try to figure out how do we measure the size of the input. So we know that the inputs are start and end. These are the input themselves but the size of the input is a big difference so what should be the size of print ascending start and end.",
                    "",
                    "For example, if we use print ascending three and six; what's the size of this instance? I would say that the size of print ascending three and six would be three, four, five, six, and equals four, the number of elements in the range from three to six. Print ascending seven nine, would be seven, eight, nine, the size would be three. Again, the number of elements in the range from seven to nine. Print ascending five and five, the size would be one. There is only one number in the range from five to five. So, let's define N as the input size to be the number of integers in the range from start to end. Saying that, it would be easier for us now to solve the problem for the smallest input; we know how to measure the size of the inputs. So, the smaller input would basically be the smallest valid range of numbers, basically a range with a single number in it. So if we want to solve the problem, solve print ascending, with the smallest values/the smallest size for start and end, if we try to identify this case it would be when start equals N. When start equals N, basically, we are in the case where there is a single element in the range from start to end.",
                    "So our solution for the smallest possible input would start, if this is the case of the smallest possible input. If start equals to N, then let's see what we should do in this case. So, when we call print ascending with a single element or a single integer range, the print ascending should just print the single element in that range, basically cout sort that is, right? So, this few lines of code basically solves the problem for the smallest input; quite straightforward, quite easy. By the way just the same as mathematical induction, the base case is a very easy step in the proof. It's very easy to show that P of one is true, most of the times. Same here, solving the problem for the smallest input is typically very easy; we just have to identify the smallest input and the solution is quite straightforward then. The recursive step that's a bit more tricky.",
                    "",
                    "The recursive step, as we said, we first need to define the inductive assumption and then we would need to use it in order to solve the original of the given problem. So we said that, generally, the inductive assumption is something of the form if we call the function on a smaller input it would do its job. for print ascending, if we try to make this assumption more specific here, I would say something like if we call printer sending on a smaller range it would print the numbers in that range in an ascending order. Instead of the word function, if we call the function I would say if we call print ascending instead of saying on a smaller input I would say on a smaller range. So if we call print ascending on a smaller range and instead of just saying it would do its job, I would say it would as print ascending should do print the numbers in the range in ascending order. So, this is a very powerful thing to assume; we're assuming basically that calling the function on a smaller input would do something, actually it would do what it has to do it would print the numbers in ascending order. Having this powerful assumption, let's see how we can use it in order to solve our original problem of printing the numbers the entire set of numbers from start to end. So let's try to use this assumption let's try to call print ascending on the smaller range. ",
                    "",
                    "So, our original range is, let\u2019s say, from start to end. Let's try to call this function on a smaller range, for example, not from start to end but from start to end minus one, that's a smaller range. What would happen if we call print ascending start, end minus one? Actually I don't know, but our assumption says that if we call print ascending on a smaller range it would print the numbers in an ascending order. So, since and end minus one is a smaller range, this call here should print the numbers from start to end minus one in ascending order. Surprisingly probably a lot this call would print the numbers from start to end minus one. That\u2019s most of the job we need to do here; we need to print the numbers from start to end in ascending order, and this single line of code already printed the line from start to end minus one in ascending order. The only thing left for us to do is just after that is print the value of end. These two lines combined together then would print the entire range from start to end. I know it is surprising; I know it is very not intuitive the next we are will try to show why it works. I think maybe before the second video just try it on your computer and make sure it does work and get surprised a bit.",
                    ""
                ]
            },
            {
                "title": "Tracing printAsc with Runtime Stack 1.4",
                "data": [
                    "Okay. So logically I believe or I hope I convinced you that print ascending one and four works. Let's see it in a more formal way. Let's use our run time stack model, that basically follows the actual way that programs are executed inside our computer and let's see that using this model print ascending one and four, also prints one, two, three, four. So, at the beginning we have a single frame in our stack with a start equals one and end equals four and we are at the beginning of the function starting to execute it. So we are checking if start equals to end, in this case it does not, so we go to the else clause and we call print ascending with the start and end minus one. that would create a new frame in our stack with start equals one and end equals three, and after we create this frame we jump to the beginning of the function to store the new functions execution.",
                    "",
                    "So once again we check if starts equals end, it does not so, we go to the else clause and once again we have another function call here. So we create a new frame in our stack, in this case start is one and end is two, after creating this frame we jump back to the beginning of the function to start the execution of this new call. So we ask whether the start equals end, it does not so we go to the else clause and unfortunately, there is another function call. That creates a new frame in our stack, in this case start and end are both equal to one; once again, we jump to the beginning of this function to execute this call. This time start is equal to end, so we see out start, we print one that's our output so far one. And this call basically ends, so this this frame is popped out and the one two frame turns active. We go back to where we came from we called, this call the just ended right here in the else clause. So we go one statement after that to see out end when we see out end in this active frame and is two, so two is printed. This call is also ended. So we pop out this frame go back to where we came from with one and three as the active frame; we see out three this call also ends, this frame pops out and we go back to where we came from with one and four as the active frame. We see end and in this case would print four and finally this last frame also pops out and this initial call of print ascending, one in four, basically ends. As you can see this call here just printed one, two, three, and four. I know that this probably convinced you that this implementation works for this function, but then when we designed this algorithm we didn't think of the runtime stack evaluation that is going to occur. We thought of it kind of an inductive manner; how to combine a smaller instance of the problem to be a solution for original input.",
                    ""
                ]
            },
            {
                "title": "Tracing printAsc 1.4",
                "data": [
                    "Let\u2019s try to convince ourselves that this magical implementation here really works. So, I\u2019ll do it in two separate ways. first you've executed it and probably saw that it basically works, but let me try to convince you a first more logically some kind of a bottom up argument, just like I show you that recursion basically prove that P of N is always true, so that would be one way for me to convince you that this implementation basically is fine. The second is a more formal tracing of using, I don\u2019t know, runtime stack and stuff like that. but let's start with a more logical argument here to show that print ascending, for example, one four basically prints the values one, two, three, four. To do that, as I said, I'm going to make a bottom up argument. Let\u2019s first convince ourselves that print ascending one and one basically prints the values in that range, basically prints one correctly. So if we\u2019ll execute print ascending one one we\u2019ll first evaluate the condition if start equals to end, it would be true. We would print start, in this case one and our function with the end after that. So, print ascending one one prints one which is exactly what it should do; remember that for the future, so print one one works fine.",
                    "",
                    "Now let's try to trace the sending one and two. So when we look at the Boolean condition start equals end for one and two that would be false. We\u2019ll go to the else clause in this case we will print the sending start and in minus one which would be one in one. We know that from the sending one one prints one we don't need to re execute it we already know that when we call print the sending one while we get one as an output. So this line would print one after that we would print the end which is two, both of them together would print one and two. So print ascending one two also works as we expected to; it would print one and two, so keep that in mind to. ",
                    "",
                    "Now let's see how/what print ascending one and three does. Once again start equals two and is false here, so we go to the else clause. We call print ascending with start and end minus one, in this case it's a one and two. We\u2019ve already seen that print ascending one two prints one and two, so this line would print one and two. After that we are printing end for all so together we are printing one, two, three, four. Print ascending one and three prints the numbers one, two, three, just as required; keep that in mind so print ascending one and three prints one, two, three.",
                    "",
                    "Let's finally see how print ascending one and four prints one, two, three, four. Once again, the Boolean condition is false; we go to the else clause. We call print ascending for one and three; we know that this call would print one, two, and three. After that we print end, which is four, all together it prints one, two, three, four. That shows that print ascending one to four prints the entire range of one two three four. I hope that this argument here convinced you not only for one and four that print ascending works properly, it would convince you that any range of size one works. Therefore, any range of size to works properly and equal for range of size three works properly. Therefore, any range of size four would work properly and so on. So, print ascending basically this implementation is basically fine. next video will talk about the runtime stack of this execution.",
                    "",
                    ""
                ]
            },
            {
                "title": "printAsc Alternative Implementations 1.6",
                "data": [
                    "Okay. So we are now convinced that our implementation really works. As we said we use the smaller instance of this problem, print ascending start and minus one. That would print in entire elements in the range from start to end minus one and followed by the end would basically print the entire range from start to end. We can think of some other ways to use this assumption of calling print ascending on a smaller range and combining it in order to print from start to end. So given the range start and end taking the range start to end minus one is obviously a smaller range and calling this print ascending really worked as we did. But we can also, for example, call print ascending not from start to end minus one but from start plus one to end. This call is also a call for a function on an interval with a smaller range than the original start and end. By our assumption, this call should also do its job or in fact it should print the numbers from start plus one to end. that is great because all we have left to do here is just print start before we print from start plus one to end; so that's another version here. Another implementation, another recursive implementation, for the print ascending function. ",
                    "",
                    "We are first printing the value of start and then calling the recursive function to do the rest of the job: to print from start plus one to end. So if we have the range from start to end, we can reduce it by calling start and end minus one or by calling start plus one to end. Actually, we can think of another way or a lot of other ways, but in another very interesting way would say, instead of just reducing it by one maybe we can take half the size of the range. So let's print the first half of the numbers and after that we'll print the second half of the numbers, for that let's first, I don't know, calculate the middle point, which is basically the average of starts and ends. So, mid would be start plus end div to and then we'll make two recursive calls: one to print ascending from start up to the middle, and the second to print from the middle plus one, one after the middle, to end. The first call by our assumption since it's a smaller sized range; it would do the job it would print from start to the middle all of the numbers in ascending order. and the second call since it is also a call with a smaller size range, would bring the numbers from mid plus one up to end. Combining these two lines one after the other would print the entire set of elements from start to end in ascending order. That's kind of cool; you must say?",
                    ""
                ]
            },
            {
                "title": "Print Ascending and Descending 1.7",
                "data": [
                    "We\u2019ve seen that there are two steps we want to solve a computational problem using recursion. We've talked about the base case, the fact that we solve the problem for the smallest possible input and the recursive step where we combine solutions to smaller instances in order to solve the original of the given problem. Actually, we first define logically, it's not any line of code that we find our assumption; that calling the function on a smaller input does whatever it has to do. But given that in mind, we are trying to solve our problem for the given input. So we kind of make ourselves call the function on a smaller input and try to see what fixes we have to do in order to make it work for the given input. just the way we kind of proved by induction where we in the induction step when we want to prove that P of N is true we kind of make ourselves use the assumption that P of N minus one is true or all the values smaller than N are true. We kind of make yourself use this assumption in the proof that P of N is true, same thing in recursion we kind of makers of use or call the function smaller inputs and try to fix it, try to find a way to combine these calls for creating a solution for the given input.",
                    "Let's try to see some more examples. for example let's try to write a recursive implementation of the function print ascending and descending. once again this function could get two parameters, start and end; again we'll assume that start is less or equal to N. but this time the function given started and would print the numbers from start to end in ascending order, followed by the numbers in a descending order back to start. for example if we call print start and end with three and five, it would print three, four, five, four, three; it would printed in an ascending and descending order. ",
                    "",
                    "Let's try to create a recursive implementation of this problem here. Starting with the base case, starting to solve this problem on the smallest possible input. Once again, this input for this problem is measured by the number of elements in the range from start to end. Therefore, the smallest input possible is the range of size one; we can identify it by asking if start equals end, if it is equal to end, the solution for this very small instance, if we want to print ascending and descending a single element range, it is basically just printing that number; so cout start. Quite easy, as we said the base case are typically very straightforward. ",
                    "",
                    "But then let's see how we can solve all the other cases. First let's define the induction assumption. If we call in this case our function print ascending and descending on a smaller range, it would do the job. Basically it would print the numbers in the range in ascending and descending order so our assumption is if we call print ascending and descending on a smaller range, it would print the numbers as requested in an ascending and descending order. Let's try to use it in order to solve our problem for the range of start to end. So let's try to take a smaller range than start to end. For example, let's take start plus one to end; let's think, what the call print ascending and descending start plus one to end would do? by our assumption, we said that if we call this function on a smaller range, which obviously is a smaller range start plus one to end, it would print all the numbers in that range in ascending and descending order.",
                    "So it would print from start plus one up to end then back to start plus one, that's basically most of what we need to do in order to print the entire range from start to end in an ascending and descending order. We just need to print start before this and print start after that so if we'll add two cout statements before this call and after this call, we would get a print of start and then a print descending the range start plus one to end then back to start plus one. Followed by a last print of start, which would all combine together, would print from start to end and back to start.",
                    "",
                    "[PART 3]"
                ]
            },
            {
                "title": "Factorial 1.2",
                "data": [
                    "Let\u2019s write another function using recursion. This time let\u2019s write a recursive implementation for the function: factorial. This function, we already had an iterative version of it, this time we\u2019ll write a recursive implementation. Factorial, as you probably remember, should get a positive integer as a parameter and then it should return the factorial of that value. For example, if we\u2019ll call factorial of four, we\u2019re expected to twenty four back. That\u2019s because one times two times three times four is twenty four; four factorial is twenty four. Let\u2019s take a look at factorial of N; so factorial of N is N times N minus one, times N minus two, and so on up to one. We can definitely have an iterative implementation for the factorial, as we already have, but when we\u2019re thinking or trying to implement some sort of recursive implementation. We should try to see how we can combine or how we can define the factorial of N, with smaller instances of the factorial problem. In this case, we will probably see that one times two times up to N minus one, is basically N minus one factorial. So, factorial of N is basically N times the factorial of N minus one. So given that observation in mind, we can create a very easy recursive implementation. Let\u2019s go ahead to the computer and do that. ",
                    "",
                    ""
                ]
            },
            {
                "title": "Factorial Implementation 1.3",
                "data": [
                    "Let\u2019s implement the factorial function. So before that I could be in factorial that the parameter here is n. and we should start with the base case; we should solve the problem for the smallest possible input. In the case of factorial the smallest possible input, we're assuming that N is a positive integer, so the smallest possible input is when N equals 1. Let\u2019s identify this case when N equals 1, that the case we are trying to solve here. In this case the factorial of 1 is 1. So let's just return 1 as our output. Otherwise here we need to implement the recursive step. So our assumption would be that calling factorial on smaller value than N would return the factorial. in this case ,we already observe that pictorial of N is basically N times the factorial of N minus 1, so maybe we should just call factorial for N minus 1 and multiply that by N. so converting a single line of code but let me split it into a few, let's create res local variable. Let\u2019s store in res, the result of calling factorial with N minus 1. that basically says that res now has factorial of N minus 1 and we should then just multiply res by N, that is the factorial of N. now res would hold factorial of N we could just return res for that matter. That is totally all the implementation for factorial.",
                    "",
                    "Let's write a simple program that uses it. So I'll just declare factorial up here, and in my name let's cout factorial of four, basically prints 24. Let\u2019s execute it; yep, prints 24 which is exactly what we expected it to do. and when we're looking back at this implementation, we see that we used our assumption, we called our function on a smaller input, and it did what we asked it to do basically, to return the factorial of N minus 1 and then we update it we fixed this value in order to have the value of the factorial of N basically by each multiply by N. that was the whole idea here.",
                    ""
                ]
            },
            {
                "title": "Are All of the Digits Even? 1.4",
                "data": [
                    "Let's write another recursive implementation. This time let's implement the function are all even; this function gets in the range of integers as a parameter and its logical size N. and it should return true if all of the elements in this array even, if not all of them are even it would just return false. For example, if we\u2019ll call this function with an array, let's say four, six, zero, and two and the logical size four, obviously it would return true because all of the elements here are even. If we'll call this function with the array four, seven, zero, and two with logical size four, it would return false because not of all of the elements here are even. For example, seven in this case is odd. Let's try to have a recursive implementation using a computer; let's do that now.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 13,
        "module_name": "Searching",
        "file_name": "Module 13 Searching.docx",
        "transcript": [
            {
                "title": "The Searching Problem 1.2",
                "data": [
                    "Let's talk about two fundamental searching algorithms. The first one is the most general searching algorithm; it will implement a function search that is given an array, basically the starting address when of an array and its logical size, and in addition it would also get value, an integer value, val. And this function should return an index inside this array where val first appears. Or if val doesn't appear as one of the elements in A R R this function would just return a negative one. So, for example, if array contains five, eight, twelve, seven, eight, and ten; for example, if we call the search function with this array of size six searching for the value eight, we're expecting to get the index of the first time where eight appears. That's the second position so the index would be one; this function in this case should return one. If we call the search function on the same array of size six searching for four, which as you can see doesn't show in the array, the function should then return a negative one. So let's implement the search function.",
                    ""
                ]
            },
            {
                "title": "Searching Code Sample 1.3",
                "data": [
                    "Very intuitively we should basically iterate over the elements of the array. And check each one whether its value is valid or not. Since we're not sure how many iterations we're going to have, cause may be not all we won't need to go over the entire array, we\u2019ll see val somewhere in the middle, we can just break out. I think it's better to use a while loop, in this case. Let's use a while loop and have indexes I, in this case, iterating over the array, let's initialize I to zero. Each iteration will increment I by one, I plus plus, and we\u2019ll go while I is still inside the logical portion of the array the while I is less then there A R R\u2019s size. And then for each element let's check if there A R R I is value is val, in this case let's break the function and return this index, let's return I. if will go over the entire array in none of the elements match value, in this case outside of the a while loop we'll return negative one. Basically meaning we've gone over the entire range and none of the elements are val, so valid doesn't appear negative one should be the return value. Very basic implementation so let's try to analyze the running time. So, the instructions, we have the expressions we have before and after the while loop or constants, so they each take a Taito of one. The while loop cost; let's try to figure out. So, the body of the while loop is a constant, an if statement and the I plus plus are one, two, three statements each iteration, so let's say Taito of a one. Then the entire while loop, since we're either iterating, so let's figure out how many iterations we have for the while loop. So, it would cost us as the order of the number of iterations. So, if T of N, is as we said Taito of number of iterations, we notice that the worst case scenario is when the number of iterations is basically the size of the array, basically N. So, in this case, T of N for worst case would be paid over. So, this search as you can see is a linear search; we're going over the entire set of elements of the array and it is in a linear time, so T of N is Taito of N.",
                    ""
                ]
            },
            {
                "title": "The Sorted-Search Problem 1.4",
                "data": [
                    "Okay. So, let's take a look at another search algorithm; basically it's a variation of the general searching problems algorithms, named the sorted search problem. in this case we\u2019ll implement the function, a sorted search, where we\u2019re given a sorted array, sorted A R R, that's the name of our array. Which means that the elements are ordered in this array in an increasing order, so we get the sorted array and its logical size and again a val to search for. And same thing, so if val appears in the sorted array, let's just return one of the indexes where it appears. We\u2019re not really interested in finding the first appearance of value in a sorted array, so let's just return an index where val appears. And once again, if val is not one of A R R elements, we\u2019ll just return a negative one. For example, if our sorted array is five, seven, eight, eight, ten, and twelve, again notice that the elements are ordered in increasing order, then when we call this sorted search function for this array of size six searching for eight, we'll just return an index where eight appears; in this case eight appears in index two and three, we could arbitrarily return three.",
                    ""
                ]
            },
            {
                "title": "The Sorted-Search Problem 1.5",
                "data": [
                    "So let's try to think how we can solve this problem. First, most basic idea would say do the same thing as we've done before, basically searching the array element by element. So let's see what the first value of the array is, in this case it is one. So, we're searching, for example val equals seventy nine, so it's not one. Let's go over to the next one three; that's not seventy nine. It's go over for the next one, four that's not seventy nine. Once again eight; that's not seventy nine and so on. We can continue on iterating over the elements with an array. Yeah, basically could work, once again the running time of this approach would be linear because we'll worst case have to iterate over the entire range of elements, basically Taito of N. ",
                    "",
                    "But this approach here doesn't take advantage of the fact that our array is given to us, basically, in an increasing order and we can take advantage of that if we do something a bit differently. So, for example if we\u2019re again searching for seventy nine. Let's take a look at the entire range and look only at the element that appears in the middle of the array. Let's test that element for its value. for example, this element could be ninety six; since we know we're searching for seventy nine and at the middle there is ninety six, we don't really have to go over the entire left side of the array right because after all on the left side of ninety six would appear only values that are greater than ninety six. So, just by looking at ninety six, we can rule out all the greater part there or the left part of ninety six and keeping us with only the smaller values smaller than ninety six to look for seventy nine. So, with one check we basically ruled out half of our range. And then once again for the half of the numbers we still have to search for; again we can look at only the middle point of that range, testing the value of the element that is there. For example, twenty eight and then once again since we're looking for seventy nine the numbers are in an increasing order, if we know that in some position there is twenty eight we can rule out all the elements there are smaller than twenty eight; we can basically rule out all of these elements and keeping us with only the range of elements between these twenty eight and ninety six. Once again, we can check what's in the middle of that range; for example, eighty four and we can rule out all the elements that are greater than eighty four.",
                    "We can, we\u2019re then left with the inner range over there looking at the middle point and luckily seventy nine; so we've just found our seventy one. So, basically we are starting with the regional range, and each time we're ruling out, we're taking off half of the elements, basically closing on the element we are searching for. Let's try to implement this approach on the computer.",
                    "",
                    ""
                ]
            },
            {
                "title": "Sorted Search Implementation 1.6",
                "data": [
                    "Let's implement the sorted search function. So I have here a main program that I created, a sorted array. Notice that the elements in this array are increasing order, one, three, five, seven, nine, and so on. And I've made it call to the sorted search function, that is given the array, sorted array and its logical size of ten. and for example we're searching for thirteen here, and thirteen does appear over here so we're expecting to get the index of thirteen, which is zero, one, two, three, four, five, six; so, we're expecting to get a six out in this print statement. And let's just implement the sorted search function so this program can execute. I have this prototype up here; let\u2019s copy it and do our implementation, so something like that. And okay. So, let's get started. ",
                    "",
                    "So, we have our sorted array, something like that, and each time we are limiting half of the range. So, let's have two indexes to indicate the valid range we're searching initially; let's name the low and high. So, initially they are set to the entire range of the array, so let's create two variables low and high and set them initially low to zero and high to the last index of the array, which is the sorted array size minus one. And then, we should start iterating each time eliminating half of the valid range. For that we would use a while loop, right. We don't know exactly how many iterations we're going to have so let's use a while loop here. And I'll keep the Boolean condition here empty for now, till we figure out exactly what's the body of the loop it would be easier for us to state that really in condition.",
                    "",
                    "Okay. Another thing I think it's good to set from the beginning would be a Boolean flag that indicates if we found the value val we are searching for or still haven't done that. So, let's have Boolean variable name found, that would be set initially found to false and when we find our value we'll update it to true, but initially it would be false. And then let's start iterating, so each iteration we are searching the range from low to high; and we said we're not searching it linearly, we are taking the middle point and comparing the value we have in here, to the value we're searching for. So, for example if our value were searching is as we had before seventy nine, let's take the middle point and compare it to seventy nine. So let's have a mid index; so we have mid. And let's set mid to the middle between a low and high. So mid would be\u2026 how can we figure out what mid should be? ",
                    "",
                    "So mid, in order to be the middle between low and high is basically the average between low and high. So low plus high over two; that's the value of mid, so let's set mid to low plus high over two. Notice that since low and high are both integers, obviously, low plus high is also an integer and since two is an integer literal then these two operands of the dividing slash here are integers basically saying that C++ would interpret this slash as a div not as a real division. Which is very good for us because we don't really want mid to be the real average between low and high; it should be a valid index, an integer value.",
                    "So, div basically takes it floored down which is an estimation of the point; it could be offsetted one index to the left. So, we have the middle position and now we should compare val to the array in the middle position. So let's see if, and in here there could be a few values, so for example if we're lucky the value in sorted array at the middle position is val, exactly the value we're looking for, in this case we should raise the flag basically say that found is true. Right? So if in here we have, for example seventy nine, then basically we're done. We should just say that val, that found is true, and break out of the function return the index we\u2019re at. Actually, I don't want to break the while loop by a return statement so I'll just save the position that val is found so I'll create another integer named ind where I\u2019ll store this index and I'll set ind to be the mid point basically where value is right that is at the mid index. So if we found val, we are storing the index and setting found to true. Hopefully that would help us breakout of the while loop; we\u2019ll take care of it in a few minutes when we state the Boolean condition. ",
                    "",
                    "But most likely we won't be so lucky and we won't get seventy nine right here in the beginning.",
                    "So, let's see, else if we\u2019re as we said not so lucky and value isn't exactly as sorted array mid, we should compare it and see how what relational order they have. So, for example if we have say ninety six, or in other words if our val is less then sorted A R R at the middle position, right; like we have here where seventy nine is less than ninety six. In this case, as we said, we can eliminate all of that range right. In order to do that, we should then just said high over here right; so, we can basically create that as our valid range. To do that let's say that high in this case should equal to mid minus one right. We ruled out all the elements there are to the left, to the right, actually of ninety six. Okay, so that takes care of that. ",
                    "",
                    "But another case could be if we have here a value instead of ninety six that is a value that is less than  seventy nine. So if we have instead of ninety six, let's say fifty one; so we're in an else right. So sorted A R R mid is not seventy nine, sorted A R R mid is not greater than seventy nine, not greater than val. In this case, that mid \u2026 maybe comment and say that we are in the case where val is greater, seventy nine is greater, than the sorted array in the middle position. ",
                    "",
                    "In this case, we can eliminate all of these elements and to do that we just set low to here, right. So we should just say low equals mid plus one, right; setting the valid range to this half of our searching range. Let's take a look at the body we have for the while loop so each iteration we\u2019re figuring out where the middle index is and then we\u2019re checking whether the middle element is exactly the value we're searching for, in this case where saving this position and hopefully raising the flag that would lead us to break out of the while. Otherwise if we are not so lucky we are comparing val to the element that is in the middle position; if val is less than we are eliminating all the right values, the values that are greater than the middle position basically setting high to mid minus one. Otherwise, we're eliminating all the values to the left of the middle position, basically saying that low is mid plus one and we do that over and over. While, so obviously, while found is still false right. We initially set found to false, so we want to keep on iterating while found is still false. But there are cases where found would never become true, right. So, if the value val appears in sorted A R, eventually we'll close on it and find its position, but in case value doesn't appear in sorted A R R at all found would never become true, and we want to be able to break out of this while loop in that case as well. ",
                    "",
                    "So, in order to figure out what we need to do here. Let's take a closer look; I\u2019ll maybe try to demonstrate it on a small numerical example. So let's take a new array of three elements, let\u2019s have, I don't know, two, five, seven; these are our elements. There are indexes: zero, one, two. So, initially we have low and high over here, right. And let's try to execute this code and see what happens if we are searching for a value that is not in this array, let's search for four, right. Obviously, we should figure out that it doesn't appear there. Let's try to execute the code. So, initially mid would be low plus high div two, basically zero plus two. That's two div two, two div two that\u2019s one. So mid would be here, right. And then we\u2019re trying to compare if in index one element equals val, equals four, that's not true right. Our value here, five, does not equal four. In this case, we should then continue on. So if val four is less then what we have in the middle position which is five, so four is less than five which is true, then we should set high to be mid",
                    "minus one, which makes a lot of sense right. If four would appear here, it wouldn't be in the right half, it would be in the left half. ",
                    "",
                    "So, let's set high to being mid minus one; so high would then be right here. So high would be zero as well. Okay. So we have both low and high at zero position; another iteration mid would be low plus high div two, zero plus zero div two that would be zero as well. So, our mid would also be here. Once again we\u2019re checking whether the element we\u2019re searching in, which is basically the only element in our valid range, the one and only element in our range. So, two is it equal to val, to four? It is not, so or we should check whether val four is less than the element we have in the middle; is four less than two, that's not true as well. So it means that the val is greater than our middle element, which is true four is greater than two, in this case we said that we should make low mid plus one.",
                    "",
                    "So our low would go over here. Okay. So as you can see, basically, low crossed over high; always we have low to the left of high, right. Initially low is to the left of high then we change high to that half and again, low is to the left of high and low would go over here and it would be to the left of high and high would go over here and a low would be to the left high. But if we have one single element here and it's not that element, low would go over high and that basically means that our element is not found.",
                    "So, in addition to looking whether the flag is turned out to true or false. We should always check whether our range is valid, basically if low is less or equal to high. So, if both we still haven't found our element, found is still false, and our range is a still valid range, low is still to the left of high, low is less or equal to high, we want to keep on iterating. ",
                    "",
                    "But if found becomes true, we want to break out. If low crosses over high and low is not less or equal to high, that means our val doesn't appear, we want to break out. Okay. So after this while loop, we should basically check what's the reason we broke out; if we found this element? So if found equals true that means that val appears in sorted array so we should just return the index we saved earlier, this index here. But if found is false, basically that means that we broke out of the loop not because we found the element but because our range is invalid, low is not less or equal to high, in this case we should just return negative one.",
                    "",
                    "Okay, yeah. That seems to be the implementation for this function. Let's try to execute it and see if we get a six if we\u2019re searching for thirteen in this sorted array. Okay. We've got a six here. Let's try to call it one more time with an element that does not appear. For example, it's fourteen and see that we get a negative one here. Yeah. For fourteen we got the negative one which seems to be right. Yeah. So, just you know maybe it would be a good idea if you try on your own, to take an array with values and to trace it on your own, but the idea here is just as we've talked earlier. Let's try to analyze the running time of this algorithm, right now.",
                    ""
                ]
            },
            {
                "title": "Implementation 1.7",
                "data": [
                    "Let's try to analyze the running time of the sorted search function. Hopefully, it would give us a better result than linear running time; we\u2019ve worked quite a lot in order to get this coded. It's much more complicated; I hope it's worth it. Let's take a closer look. So, once again if we look at the code we have a few statements before the while loop but they are constants. So, let's consider them as Taito of one. There is the if statement after the while loop, also constant, Taito one. And I think the dominant part here is basically the while loop, so let's try to analyze the running time of this iterative process. ",
                    "",
                    "Once again if we look closer at the body of this a while loop, there are a few statements here but we can count them; they are not depending on the size of the input, no matter what the size of the range the number of operations we're doing there remains the same. that's why we're staying saying it's constant, it's also Taito of one. Which basically implies that the number of operations, the total number of operations for the while loop, is once again the number of iterations is the order of the number of iterations. But in this case it's a bit more tricky to figure out what's the number of iterations that this algorithm can do.",
                    ""
                ]
            },
            {
                "title": "The Sorted-Search Problem 1.8",
                "data": [
                    "Let's try to make a table that would help us estimate how many iterations this algorithm does. So let's try to find the relationship between the iteration number and the size of the searching range. We have each time, right. We are basically cutting half of the range each time. So, let's try to find the relationship between these two sizes. So first iteration, the size of the searching range is the original size basically N right. Second iteration, we cut the searching range by two, by half. So, that gives us, leaves us a searching range of size N by two. Next iteration the range is again cut by half, so we'll have a range of N by four and next iteration the range would be N by eight and so on. ",
                    "",
                    "In a general iteration number for example K, let's try and figure out or formulate the size of this searching range. So if we take a closer look at the first four numbers will see that N by eight is also can be written as N by two to the power of three and N by four can be written as N over two square and N by two can be written as N over two the power of one. They're all N over two to some power, even the first N can be thought as N over one or in other words N over two the power of zero, which is one. And we can see that each iteration we have N over two to the power of one less than the iteration number, right; two to the three is two to the power of four minus one. And two to the two is to to the power of three minus one or in general form we can say that in a ration number K, the size of the searching range is N over two to the power of K minus one, right. It matches the pattern we've found before. So the biggest question is: what's the number of the last iteration, right? That's basically would tell us the worst case scenario right. Obviously we can stop at the first iteration and find the value we're searching for, but the worst case is we'll keep on reducing the sorting range more and more and more till we have only one element in that certain range and then low and the high would cross over one another. ",
                    "",
                    "So we should figure out what the iteration number where our searching range basically equals one. This question mark: what's the number of this iteration where the searching range is one? If we figure that out that would be basically our running time; that would be the number of iterations.",
                    "So let's do the math; let's ask ourselves for what value of K, N over two to the K minus one equals one right. If the general form of the size of the searching range is N over two to the power of K minus one for which K this size equals the size of the last searching range, basically one.",
                    "So let's do some math tricks here; let\u2019s multiply by two to the K minus one that says that N equals two to the power of K minus one. Let's apply log base to both sides of the equation, basically saying that Log two of N equals log two of two to the power of K minus one. I hope you are familiar with the log rules here; where you can say that log two of two to the power of K minus one basically equals to K. minus one times log two of two. So that means that look two of N. is then equal to K. minus one times Log two of two. Again, Log two of two is one so that means that Log two of N basically equals to K minus one. Let's add one to both ends; that means that K, which is what we're looking for, equals to one plus Log two of N. In order of growth means that is K. is theta of Log two of N, saying that the number of iterations is theta of log two of N. And that concludes the worst case running time of the sorted search here. A lot of people also name it \u2018Binary Search;\u2019 the running time of this binary search algorithm is T of N equals theta of Log two of N. The next video we\u2019ll talk about how great improvement log two of N versus a linear algorithm is.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 14,
        "module_name": "Sorting",
        "file_name": "Module 14 Sorting.docx",
        "transcript": [
            {
                "title": "Linear vs. Logarithmic 1.9",
                "data": [
                    ""
                ]
            },
            {
                "title": "The Sorting Problem 1.2",
                "data": [
                    "In this Module, we're going to talk about sorting algorithms. Let's start first by stating the problem, kind of obvious by the name you can probably guess what this problem is trying to solve, but let's be formal here. So given an array, AR of numbers, let's change their order. So at the end they would show in an increasing order. So for example if our original array contains some values: five, eight, twelve, seven, and so on. After sorting, the array elements would change their order. So, they are now there would be: five, seven, eight, eight, ten, twelve, again in increasing order.",
                    ""
                ]
            },
            {
                "title": "Sorting Algorithms 1.3",
                "data": [
                    "There are a lot of sorting techniques; one is selection-sort, insertion-sort, bubble-sort, merge-sort, quick-sort, heap-sort, and many, many more. In this model we're going to focus on only two of them: the selection sort and merge sort. Selection-sort is a very basic naive sorting algorithm. Merge-sort is a more sophisticated one but a very nice one as well. Let's go ahead and start with a selection-sort.",
                    ""
                ]
            },
            {
                "title": "Selection Sort 2.1",
                "data": [
                    "Let's take a look at selection sort. As I said earlier it's very basic our sorting algorithm, maybe the most intuitive or na\u00efve algorithm we can think of; sort of how you would explain your kid how to sort using this approach. So, let me demonstrate the idea here. So, assuming we have a collection of elements, obviously not sorted: five, fourteen, ten, eight, thirteen and so on. It would be an iterative process each time placing the minimum element at the next position; so it's going to work something like that. So let's take a look at the entire range we\u2019re sorting and first iteration, we are looking to place at the zero position, in the first position, the minimum element. So we\u2019ll search for the minimum element, we\u2019ll get this one, index-five, and then we'll just swap them. So one would be at the beginning and five would be somewhere at the rest of the elements. So our range would now be only the elements from the second on. This iteration, second iteration, will place the second minimum at the second position; we\u2019ll search for the minimum in that varied range: that would be two and we'll swap the two in fourteen placing two at that position. The range is shorter now; we'll search for the minimum that would be three and we'll swap them, three and a ten, so three would be in the right position. So now we have one, two, and three already ordered, and next we find the minimum in the valid range. That would be three in the index-eleven and we'll swap them. So, three would be at the right position and so on and so on. Yeah, let\u2019s try to implement this idea.",
                    ""
                ]
            },
            {
                "title": "Selection Sort Implementation 2.2",
                "data": [
                    "Let's implement a selection-sort algorithm. So we'll write a function, selection-sort, that is given an array, basically the starting address int ARR brackets and logical size ARR size, and this function should reorder the elements so they would be in an increasing order. No return value here, so it would, it's a void function basically. ",
                    "",
                    "As we said it's an iterative algorithm, so we need to iterate over the elements one after the other, over the indexes basically or the positions, one after the other each iteration placing the correct element, i-th order element, in the i-th position. So we\u2019ll use a for loop to iterate over the indexes, and we'll use an index i ranging from one up to ARR size minus one; so i goes from zero less than there ARR size plus plus. Each iteration will search for the index where the minimum element is so we'll have min index equals to the location where the minimum element is. For that we would define a function: find index of min, right. So each iteration min index would get whatever find index of min function returns and after we do that we need to swap them. But let's first define the prototype of find index of min. So find index of min would get basically the array but it also should get the range where we are looking for the minimum index. So initially it would probably be the entire array, then it would be with one item less, and another item less and so on and so on through. So let's assume that find index of min would get the low index and the high index to represent the range where we\u2019re searching for the minimum index.",
                    "So find index of min is given the position where our array starts, low and high indexes to indicate the range we\u2019re searching the minimum in, and it should return an int, basically the index of the minimum element in the positions between low and high. So, assuming we have this function, in a minute or two we\u2019ll implement that as well, but assuming we have this function each iteration we need to find the index of min in the current valid range. So first iteration it should be from zero to the end, then it should be from one to the end, then from two to the end, and then from three to the end, and so on. So basically we are searching for the index of min on the array ARR starting at index i, right; first iteration starting at index zero, second iteration the starting index is one, the next iteration the starting in the two, and so on. And every iteration we stop at the end of the array; so first edition it goes zero to the end, then one to the end, then two to the end, and three to the end. Each time we're searching for the minimum index in that range. After we get that min ind for each iteration, we're just swapping the current element ARR i with the element at the min index, ARR min Ind. That's basically it; that\u2019s the selection-sort algorithm.",
                    ""
                ]
            },
            {
                "title": "Selection Sort Implementation 2.3",
                "data": [
                    "Let's implement the find index of min so we would have that as well. So once again find index of min is given the starting index, the starting address of the entire array, but in order to indicate the range we are searching for the minimum index we have low and high, two indexes that basically indicate that range. We would need to accumulate the index of the minimum element; actually, let's accumulate two things, not only the index of the minimum value, but also the minimum value itself. So that have two variables min and min ind. And let's accumulate these two values over the path of the elements between low and high. So initially, let's set min to be the first element: ARR at the low position, the min ind would be low obviously. And then, we\u2019ll iterate over the rest of the elements using a for loop basically iterating from low plus one to high, accumulating the minimum and the minimum index. Eventually after doing all of that, we would just return the index of the minimum: the min ind. So let's just iterate and accumulate that value. So we'll use an index i ranging from low plus one, low is already taken care of before the loop, so we\u2019ll start at low plus one, up too high so for i goes from low plus one less or equal to high incrementing by one, i plus plus. Each iteration let's compare ARR i, the current element, to the minimum we've seen so far, right. So if ARR i is less than the minimum we've seen so far that is the new minimum. Basically we\u2019ll assign min to be ARR i and also save min index as our current location as i. So if we first set min and min index to be the first element, then we\u2019re starting to iterate from the second and on up to i, each time checking whether we need to update the minimum index.",
                    "We are accumulating both min index and min itself, eventually we can just return the min index. So all together, we have a function that finds the index of min in a range from low to high and by calling it over and over on range that changes each time and the swap, we are basically sorting the entire array.",
                    ""
                ]
            },
            {
                "title": "Runtime Analysis 2.4",
                "data": [
                    "Let's analyze the running time of selection-sort and see how efficient this algorithm is. Before we analyze selection sort, let's start by analyzing find index of min, this helper function, so we would know what's the cost of each call there and then we'll just use that when we are analyzing a selection sort algorithm. So let's start with a find index of min; let's find the running time and the running time of that function. Like all algorithms that we are analyzing, we need to give the running time as a function of the size of the input. So, in this case the size of the input N would be the number of elements in the range were searching for the index of min; so let\u2019s have N as the number of elements between low and high, arithmetically we\u2019ll just have high minus low plus one, that the number of elements between low and i so this is our N. And let's analyze the running time as a function of N. So once again, before we start the for loop, we are doing a constant amount of operations and after the for loop we are having a constant amount of operations. So min equals ARR low, minInd equals low, that\u2019s theta of one, and return min Ind that\u2019s also theta of one. And we\u2019re mostly interested at the cost of this for loop. Once again the body here, each iteration is constant; it's only two or three primitive operations so that\u2019s theta of one. Which means that the number of iterations, that\u2019s basically the cost of this for loop. Here the number of iterations is N, so the entire cost is theta of N.",
                    ""
                ]
            },
            {
                "title": "Runtime Analysis 2.5",
                "data": [
                    "Let's go to the selection-sort algorithm now that we know that find index of min is theta of N, in other words is linear, let's use that when we're analyzing the selection-sort algorithm. Once again let's define N, as the size of the input ARR size in this case, and let's try to figure out what T of N is. So, we have a for loop that we need to figure out what's the cost of this for loop. But not like the other algorithm we had so far, each iteration the body costs differently. And let's try to see what the cost of each iteration. So basically the most expensive, I would say operation here, is the call of the function find index of min. And then since the range changes from iteration to iteration, basically the linear cost or the number of elements that we are going over, changes from iteration to iteration. I would say that the cost of the body could be described as theta of N minus i, right. So when i equals zero, when we're going over the entire range, we are basically paying N, right. When i equals one, when we're going over one item less we\u2019re basically doing N minus one. And when i equals two, we\u2019re doing N minus two and so on. So basically when we are summing up all the cost of calling find index of min, we'll pay N for the first call, and we'll pay N minus one for the second call, and N minus two for the third call and so on and so on, till the last call would be two and one. So that's basically the cheapest score. So the entire set of calls to find the index of min would be the sum of that, of these numbers: N plus N minus one plus N minus two and so on. You know that this here, actually we've seen it in one of our previous models, that we can think of it as a kind of a triangle inside a square. And we know that it is theta of N squared. You can look at the exact formula and figure out where it's theta of N squared but we've already seen that this kind of a sum is a quadratic running time. Yeah, so basically meaning that selection-sort is a quadratic running time algorithm for sorting.",
                    ""
                ]
            },
            {
                "title": "Merge Sort 3.1",
                "data": [
                    "The second sorting algorithm we're going to talk about is called merge-sort; it is a recursive algorithm, a recursive sorting algorithm. We can think of, again a few approaches to recursively sort a sequence of numbers, but let me show you how merge-sort works. So for example we have this array of numbers: fourteen, five, eight, ten, thirteen, one, eighteen, and three, and we want to sort it. ",
                    "",
                    "One way of doing it is to have a three step process. First step, would be, would say let's store, let's sort, recursively the first half of the numbers: fourteen, five, eight, and ten. Again it\u2019s recursively, so our assumption is that when we are trying to sort it, it would just do the job basically sort the elements so it would reorder the elements in a sorted order. So, first step would reorder the first half of the numbers to be sorted; kind of a magic, but okay, that's one part of the job. Step two, you can probably guess would say sorts recursively the second half of the numbers, in this case: thirteen, one, eighteen, and three, should be sorted in an increasing order. That would reorder them to be: one, three, thirteen, eighteen. Now that we have two halves that each one is sorted on its own, we would need to merge these two halves into one sorted sequence. So step three would say you merge these two halves together into one sorted sequence. So after doing step three, we would have the numbers all sorted together: one, three, five, eight, ten, thirteen, fourteen, eighteen. ",
                    "",
                    "Let's do it one more time. Step one: sort the first half of the numbers. Step two: sort the second half of the numbers. And step three: combine these two sorted halves into one sorted sequence. Cool; not very intuitive. But yeah, actually that that's a recursive algorithm. We\u2019ll implement it first and after that we'll try to trace the execution and see how the recursion here unfolds into sorting the entire array. Let's go ahead and implement.",
                    ""
                ]
            },
            {
                "title": "Implementation 3.2",
                "data": [
                    "Let's implement this sorting algorithm, and it's a recursive algorithm, so let's do it correctly. So we\u2019ll write a function merge-sort, it\u2019s supposed to rearrange the elements so is doesn\u2019t need to return any value since it's a void function. And the parameters are obviously the array and basically something to tell the range of elements. So we can do it in two ways, either pass the starting address of our current part of the array we're trying to sort and the logical size, or we can always pass the starting location of the entire collection. And in order to indicate the range we're trying to sort, we'll have two indexes, low and high, that kind of indicate the range. Something similar to what we've done in the find index of min at selection-sort, where we had the starting position sticked and the low and high indicated where we we\u2019re searching the index of min, same thing here. We want to sort only in the range from low to high but it's located in a more bigger sequence starting at address ARR, so merge sort would get the starting position of the entire sequence and low and high to indicate the current range we're trying to sort.",
                    "",
                    "Okay, so that the prototype of the merge-sort algorithm. And then since it's a recursive algorithm, we need to start with the base case. So if we're trying to sort the smallest sequence possible, basically the sequence of a single element or in other words if low and high are equal to one another. In this case, it is already sorted so we don\u2019t need to do anything, just return an empty return statement with no value because it's void we don\u2019t need to return any value, just break out of the function. Otherwise, if it is not a single element, a collection, we have a real range of elements we want to sort. Let's do it with the three step algorithm we've just described. ",
                    "",
                    "So we have our array, right. We have low and the high, indicating the range of elements we want to we want to sort. And then first step is basically for the first half of the elements. So we would need to figure out where this first half starts, yeah we know what starts at low, but where it ends. So we would need some kind of an index mid; let's define this mid. And just as we had in the binary search algorithm in our previous model, mid would be the average between the low and the high, basically low plus high div two. Now that we have mid, let\u2019s sort the first half, right. Let's make it in increasing order, so let's call merge-sort; it's a recursive algorithm so merge-sort would do the job, basically sort the first half of the numbers if we call it correctly. So in order to indicate the range of the first half, the starting address would be the same array, but the low and the high, in this case would be low and mid. We want to sort the elements starting at low ending at mid. After we've done that, so this line here sorts the entire first half of the elements. Then we need to sort the second half of the elements. Once again, let's call merge-sort in order to recursively sort. And then, let's pass the second half as a parameter. ",
                    "",
                    "",
                    "So that, in order to create that half we need to know where it starts; so it starts one after where the first act ended at mid plus one and where it ends, obviously at high. So let\u2019s call merge sort with the same starting address ARR but the range of elements we want to sort now is starting at mid plus one ending at high. So after these two calls, we have the first half of the number sorted and the second half of the number sorted, we now just need to merge them together into one whole sequence. ",
                    "",
                    "For that we better create unique function, a designated function, to do that job. We'll call it merge; and merge should take these two halves and combine them together. So the parameters for merge would be the starting ARR, and some values to indicate where the first half starts, where the first have ends, where the second half starts and where the second half ends. So we would have the low left and the high left, to indicate where the first or the low left index is and where the index of the high element of the left side. And instead of having low right and high right to indicate where the right side starts and ends, we don't need both of them because the right side starts right after the left side ends, right. It should start one after high left. So, we don't need to pass that as a parameter; let\u2019s only have the high right. So we have where the left side starts where the left side ends, and where the right side ends, obviously we know where the right start starts. ",
                    "",
                    "So the prototype would be: ARR low left, high left, high right. And it's a void function, it rearranges the elements, it doesn\u2019t need to return any value. So assuming we have that, we\u2019ll make and implement this function in a few minutes, assuming we have the merge after calling the two recursive calls; after having the first to have sorted, we just need to merge them by calling the merge function that just passed the right values for the parameters of merge. So, let's call merge with our ARR low and mid for the bounds of the left side, low is the low left, mid is the high left, and mid plus one would be the low right, but obviously high that\u2019s the high right. So, this call basically merges these two halves together and after sorting the first half, sorting the second half, merging them, we have one long sequence from low to high that is sorted. That's basically the merge-sort algorithm.",
                    ""
                ]
            },
            {
                "title": "Merge Sort 3.3",
                "data": [
                    "What we need to do now is basically implement the merge algorithm. Let me demonstrate how we should merge two sorted sequence into one big sorted sequence, and after we do that let's go to the computer and just implement it all. But let me first show you the idea so it would be easier for us.",
                    "So, assuming we have two sorted sequences: one, three, six, ten, and four, seven, eight, thirteen, fifteen, twenty, so two sorted sequences. We want to merge them together into one big sequence, to a new big sequence. We can do it in a lot of ways but in order to take advantage of the fact that these two sequences are already ordered in an increasing order, already sorted, we can do something kind of cool. ",
                    "",
                    "If you think about it, obviously the first element can't be, I don't know six or ten or thirteen or twenty one, it could either be the first element in the first one or the first element in the second one. So basically we should choose one of these two to come first in our resulted emerged sequence. So we have two candidates for the first element, in this case when we are comparing one and four we would obviously want take one to be the first element and then we can pick another candidate to be the next element. It would either be three or four.  Now that we have taken care of the one, we have to pick between three and four to be the next element. In this case it would obviously be three. Now we have two candidates for the next element, either six or four, right. Obviously it can\u2019t be ones that are greater than six and ones that are greater than four before we put the six and four in their sorted merged output.",
                    "So then we\u2019ll obviously pick four and then we'll have to pick between six and seven; we\u2019ll choose six. And then we'd have to pick between seven and ten, and we pick seven. And then we'll have to pick between eight and ten, and we\u2019ll pick eight. And then we need to pick between thirteen and ten, obviously we'll take ten. And then basically we're done, adding all the elements from the first array but then the second array still have a tail of elements. So let's just take all this tail and copy it to the remaining positions in our result merged array. And that basically completes the merging of these two sorted sequences into one big sorted sequence. So let's take the merge-sort algorithm, the merge idea, implement them all on our computer and make sure it works. After that, we\u2019ll also trace the execution of these runs.",
                    ""
                ]
            },
            {
                "title": "Merge Sort Implementation 3.4",
                "data": [
                    "So let's execute and implement the merge-sort algorithm. So I have a min here that we have an array of eight elements, obviously not sorted: fourteen, five, eight, and so on. I\u2019ve declared ARR size to be eight. I did all hardcoded but you can see how we can make it in a general form. And then we're calling merge-sort, in order to sort this array. Initially, it will pass ARR in the entire range of elements as our low and high; basically low is zero and high would be the last index ARR minus one and then we're calling print array in order to print the array after merge-sort basically sorted the elements. Merge-sort is a very simple function that just iterates over the elements, prints them with a space and a new line at the end; so just prints the array one by one, element by element. And merge sort as we have seen before is a recursive algorithm it knows i don't equal anything otherwise we set the min index to be low plus high div two, we call him merge-sort with the first half from low to min index. And then we call merge-sort just for the second half from an index plus one to high and then we merge these two halves together, basically having the first half from low to mid-ind and the second half ending at high. We now only have to implement the merge algorithm. We've described before but let's implement it in C++. ",
                    "",
                    "So we have the prototype; let\u2019s take it over here and implement it. Okay. So first thing we have the low left, high left, and only right, so let\u2019s declare low right variable, just so we can set it to be high left plus one, right. That's where the right side starts: one element after the left side ends, right. So now we have low left, high left, low right, and high right. Another thing, let's also figure out how many elements we're trying to merge together so let's have a size variable. Let's set size to be the high right minus low left; that's basically the entire range we're sorting, we're merging plus one, so that's the size here. And then we need to create an array for the result of the merge sequence; so let's have pointer int star merged array. And let\u2019s allocate memory for that array: merge ARR equals new int of size elements. Again, we cannot do the merge in place because we'd be stepping on values that we\u2019re not necessarily done working on. You can try thinking why we can't implement this merge in place, basically on the same array we are reading, we have the sequence on, so we're using an additional array. And again, since we don't know how many elements we're going to have in this array; it would kind of be a static array, has to be in a dynamic array, that's why we're using the memory allocation here to allocate a new array of size elements. ",
                    "",
                    "Now that we have this array and we have the input arrays, the two halves, we can start merging them into the merged array. For that as we've seen we would need three indexes: one for the current element we're looking in the first half, then for the first element we're looking in the second half, and then the index where we are currently writing our result. So, let's declare these three indexes here: we'll have the index of the right side, the index of the left side, and let's also have the index of the results. So I\u2019ll have iRight, iLeft and iRes. Let's start with iLeft; let\u2019s set iLeft to be the low left, right; that's where the left side starts. iRight would be where the right side, starts basically high left plus one or we have low right that we created. So we have iLeft and iRight set to the beginning of the left and right side. iRes would be set to zero, right, because that's where we want to start writing the result into the merged array. And then we should over and over repeatedly choose the element, the smaller element, between the one at the iLeft position and the one in the iRight position; so we\u2019ll use a while loop here, again, let's keep the Boolean conditions for a later stage. And then each iteration, let\u2019s pick one from iLeft and iRight, so let's ask: if the element at the iLeft position is less than the element at the iRight position that means that this element should be taken, right, there the iLeft one is smaller so it should become first. So, let's put at merged array at the current position, at iRes, let\u2019s put there the element from iLeft, this element here, right. So we are writing to the result array the element that is currently smaller between iLeft and iRight, the left one in this case. ",
                    "",
                    "Then we need to increment iLeft and also obviously increment iRes, since we\u2019ve written to that position. Otherwise, if basically ARR iLeft is not less than the iRight one, we should take one from iRight. So let's put it at the merged array at the iRes position, in this case the array element from iRight, and now we should increment iRight in addition to incrementing iRes, right. So each iteration we are choosing one from iLeft or iRight to write to the merged array, depending on which one is smaller; either rewriting iLeft incrementing iLeft by one, or writing the iRight elements to the array and incrementing iRight. We do that over and over and over, as long as we have two candidates to choose from. Eventually one of them would go over the entire range of the array and we would have to take the tail of the other array.; so we can keep on going while they are still validly inside the range of their halves. So basically let's keep on going while the current index in the left is still less or equal to the high left, right; that means that the letter iLeft is inside the left half. And we also need the same for the right side, so the iRight is less or equal to the higher end of the right side, high right, right. ",
                    "",
                    "So while they are both inside their arrays, we want to choose one of them. Eventually one of the halves, we would end our work on it and then we would need to take the tail from the other parts. So we could either use an IF statement here to see which one ended and copy the rest, or we can just use a while, actually two while loops here that\u2026 So we'll have a while here and a while loop just after that. This one would take care of the tail we have in the first array, and this one would take care of the tail we have in the second array, we are certain that after the first while loop, after this loop, there would be only one tail so currently only one of these two whiles would be executed. If there is a tail here it would be copied and then this while loop won\u2019t even execute once or if there is no tail here, so this while won't be executed and this while would copy the data from the second half. So, let's take care of the first array first half. So while, again, iLeft it is less or equal to high left; that's basically saying there is a tail in the left side that should be copied. In this case, we should do what we've done here: basically taking two of the merged array at the iRes position, the element at the iLeft position, right, incrementing iLeft and iRes. So, that takes care of the tail left in the first half in the left half of our array. And if there is no left half then this while won't be executed at all right. If iLeft is not less or equal to high left, this body won't be executed. And same thing for the right side, so if iRight is less or equal to high right then again the merged array at our current position would just be the element from iRight and then let\u2019s increment this value and iRes. So now that's basically it; now we've picked the right candidates between iLeft and iRight over and over and over, when one of them ended we just copied the remaining tail to the merger array. After these three whiles, we are definite that all the elements are at the merged array.",
                    "",
                    "Since we said that the merge function originally should reorder of the elements in the array from lower left to high right, it's not good enough for us to have the merge sequence in a totally new array, right. So we would need to copy the elements from the merged array back to our array. So for that we would have a for statement and let\u2019s do something like, we would need to copy from the merged array to the original array and the merged array basically ranges from zero to the size, right size minus one, where the array we want to put the elements not from the zero index but from the low left index. So we would need to set two set of indexes here; we would need i for the merged array and let\u2019s do ARR that would be low left for the second array. Okay, something like that. Let's just declare i and iARR, so we have these two indexes. And in the second you\u2019ll help me figure out what we want to do here, but basically the body would be copy to ARR at iARR position, the element from the merged array at i index.",
                    "So we'll have both indexes increasing or getting bigger simultaneously, so we need i plus plus and i ARR plus plus. But they start at different positions, so we want i to be an index in the merged array starting at zero and want iARR to be an index if they are are a starting at low left. So we just set them initially with different values, each time we\u2019ll increment them, we just need to do it the right amount of times; we can either use our iARR to control it or i to control it. Let\u2019s just do less than size; size, if you recall, that\u2019s number of elements we have all together that we want to merge or in other words the size of the merged array. So we can go from zero to size, each time incrementing by one copying this element to the corresponding position in ARR, basically the iARR position. After we've done doing that copy, basically we\u2019re done, right. ",
                    "",
                    "So we have the merged sequence in our ARR array, what we need to do now is just delete the memory, the location we had for the merged array. So for that let\u2019s use the deleted operator, which is an erase so I\u2019m using empty brackets, that seems to be this function here. ",
                    "",
                    "And now that we have merge-sort and everything, let's just try to execute it; see that it really sorts.",
                    "We have the sequence sorted increasing order: one, three, five, eight, ten, thirteen, fourteen, and eighteen, which is exactly the sorted sequence in the array. So it seems to be working fine. ",
                    "",
                    "So merged is a fun function; we had a few details when we worked on it. It's a cool function but I think more interesting, more surprising here, is the merge-sort algorithm, where we surprisingly called merge-sort two times assuming it sorts the first two halves, and apparently it does. All we have left to do is just merge them together and we've implemented the merge function, so that it is kind of surprising, you can think. But let's go ahead and trace the execution and convince yourself how this basically works.",
                    ""
                ]
            },
            {
                "title": "Tracing Merge Sort Execution 3.5",
                "data": [
                    "Let's trace the execution of the merge-sort algorithm. So at first I want to make sure that all of you get inductive idea here, where we have three steps first sorting the first and then sort of the second half and then merging them together and logically understand why these three steps, in that order, basically soars the entire sequence. But I think it would be even more convincing if in addition to that we would also trace one execution from start to end. My suggestion is don't do it too many times because understanding the inductive idea for recursive algorithm, that\u2019s the important thing. But let's do it for merge-sort. ",
                    "",
                    "So, we have eight elements sequence: fourteen, five, eight, ten, thirteen, and so on, which we want to sort. So we are basically sorting the first half and the second half and then, since it's a recursive call each one of them is going to sort its first half and second half. So we have calls for two element array and each one of them also would call two sorting halves of one element and that would be the base condition. But that's not the order that these calls are called; let's try tracing the merge-sort algorithm now in order. Okay. ",
                    "",
                    "So we're first calling the original eight element array. It, as step one, calls to sort the first half of the elements; the first four elements, right. And then when this function executes, it's also a recursive call. That's not the base condition so we also called to sort the first half of the elements; basically sorting fourteen and five, and fourteen and five is also not the base case so it also calls to sort the first half of the elements, which is fourteen. Fourteen: that's a single element array that is already sorted, so nothing to do here just return. And then fourteen five calls to sort the second half, which is five, so now fourteen and five after having two halves that are sorted it should merge them together to a sorted sequence. Basically reordering them to be five fourteen; that's the merge. And then the four elements: fourteen, five, eight, ten, now need to call to sort the second half, basically eight and ten. Eight and ten, that's not the base, so it calls to sort the first half of eight which is sorted. and then it calls to store the second half of ten, which is sorted and then it merges them together. In this case no reordering is needed the assorted sequence is eight and ten. ",
                    "",
                    "Now the four element array has two halves that are sorted by fourteen and a ten. We need to merge them together in order to get a four element sorted sequence: five, eight, ten, fourteen. So that's basically the end of the first step in the original call; merge sort of the first half it would recursively merge-sort that half. And eventually would have the elements, five, eight, ten, fourteen, in the sorted order. ",
                    "",
                    "And now would come the second step, basically sorting the second half of: thirteen, one, eighteen, and three. That's not the base case so we would start by sorting the first half of that, which is thirteen and one. That's not the base case so it starts by sorting the first half of that, thirteen which is sorted, and then sorting the second half of that, one which is also sorted. And then we need to merge them into: one, thirteen. And now we need to sort the second half of the four element, which is eighteen and three. Not base case, so let's recursively sort the first half: eighteen which is sorted. Let\u2019s recursively sort the second half: three which is sorted. Let's merge them together to three, eighteen. Now we have two sorted halves of the four element array. ",
                    "",
                    "We need to merge them together into one sequence of four elements that are sorted: one, three, thirteen, eighteen. That\u2019s the end of the second step of the original sort of eight elements. We've sorted the first half recursively; we've sorted the second half recursively. And now comes step three, where we have the two halves sorted that are needed to be merged together. And that\u2019s step three of the merge function, basically merging them into one sorted sequence: one, three, five, eight, ten and so on. It's a bit crazy way to trace this execution, but that's how basically it was. I think it's easier and more understandable to think of it inductively. Basically, thinking step one would do the job basically sort the first half, step two would do the job basically sort the second half, and then we just need to merge them together to get the entire job done, basically sort the entire sequence.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 15,
        "module_name": "Object Oriented Programming Concepts Script",
        "file_name": "Module 15 Object Oriented Programming Concepts Script.docx",
        "transcript": [
            {
                "title": "In this Module 1.2",
                "data": [
                    "So now that we can talk about object oriented programming, let's get into what we can do in this module. We'll give a definition for an object and a class, we\u2019ll talk about the concept of encapsulation, we're talk about how to create a class, enforcing protections and access, mechanisms for class accessors and mutators, and we'll talk about the constant modifier. We're also going to talk about how to guarantee that an object is created properly. We'll talk about using operators on classes and we'll talk about classes that contain dynamic memory. We're also going to get into concepts of inheritance and polymorphism. And we'll talk about dynamic binding as well as pure virtual functions.",
                    ""
                ]
            },
            {
                "title": "Object Orientation 1.3",
                "data": [
                    "The idea of object orientation came out in the early 1980\u2019s because we had an enormous number of functions and we had data not associated with the functions. So programmers would have to pass a large amount of data to a function to perform a specific task on that data and if the data got disconnected from each other, from the different parts of the data, we would have a loss of information. So what the concept came about was to put all of this data into one capsule, encapsulation, and have functions that can work on that data as a single atomic entity. So we can create in code something that exists in real life which contains enormous amount of information, enormous number of pieces of data, and we can combine them and have functions that operate on that data at the same time. We use classes in C++ to protect the information so that it can\u2019t be changed from the outside world. So from anything outside the class it wouldn\u2019t have unrestricted access to the data inside the class; we only have access from the functions inside the class. It allows us to do multiple atomic operations inside a single function that has to change lots of data. ",
                    "",
                    "So it\u2019s not just asking a user or a programmer to make one change then a second change, because we know that the programmer invariably will do the first change but forget to do the second change. Now we can create a function that solves the program to make both changes. It enforces the idea that the designer of the class knows best about what to do, rather than having a set of documentation which tells a programmer outside what to do. So the model that I like to employ, the model that I like to use to visual this, is that of a card shuffler inside a Vegas casino. So if you\u2019ve ever been to Las Vegas you\u2019ve seen these card shufflers and what they do is they continuously shuffle these cards, the deck of cards. And when new cards come in, they get added to the shuffle so that nobody can count the cards and know what\u2019s gonna come out next. But the point is that they\u2019re a single contained item, and what happens inside is sor5t of a black box. We don\u2019t know what happens inside. But as the dealer puts more cards in and takes more cards out, what is happening inside is controlled by the designers of the card shufflers. ",
                    ""
                ]
            },
            {
                "title": "Class vs. Object 1.4",
                "data": [
                    "So first it helps to define what the term\u2019s class and object are. and for our class we consider that a framework which is used to build multiple objects which are similar. So for example you might think of a blueprint for a house as a class. The idea is it's the framework that we're going to build the object around; it tells us everything about what's going to be inside the class but it doesn't tell us what the values of the data are for example. The object is just an instance of a class; it's a class instantiated. So when we create one of these variables, we have an object and for that. I'll use the example of a house or a cookie, so going back to our previous examples of the blueprint for the house with a cookie cutter for baking. We can talk about the object being the House or the cookie and of course we don't try and live in the blueprint, we want to live in the house and if we don't want to eat the cookie cutter, because it doesn't taste very good, we'd rather eat the cookie.",
                    ""
                ]
            },
            {
                "title": "Encapsulation 1.5",
                "data": [
                    "The idea that we want to employ here is one of encapsulation: we don't want anybody outside the class, outside the class creation, to be able to access data inside the class. So the idea is that code written as part of the class has a greater access than code which is not part of the class. For example main, main shouldn't have access to some of the private information inside the class. And the idea of encapsulation allows us to set levels of protection so that programmers that aren't involved in the creation of the class are not going to have access to certain items. We can give them controlled access to items that we want them to be allowed access to, and then we can strip their access so they can't have access to the items that they shouldn't have access to. So, for example going back to our house and blueprint model. The architect should be able to change anything on the blueprints of the house, the designer of the class should be able to change anything inside the class, but the plumber who's installing the toilet shouldn't be able to change where the location of the toilet is or else the bathroom door might not close, for example. So we want to create a model where we can protect certain parts of the class and allow public access to other parts of the class.",
                    ""
                ]
            },
            {
                "title": "Creating a Class 1.6",
                "data": [
                    "Let's get down to the syntax of how we can create a class. First off the class is an abstract data type, so we're creating a new data type for C++ to know about. We can create this class as a data type which is a more complex data type made up of simpler data types. And we're going to get into this a lot because we can make them very, very complex. Of course we\u2019ll use the C++ keyword class, which means incidentally that you can never use the word class as an identifier for anything inside your programs. You can't create a variable named class. Here's a quick view of what a date class might look at, and we know that a date is actually just three integers; it's a day, a month, and a year. So here we\u2019re creating a new data type for the storage of three integers, atomically, so that they go together. The year and the month and the day are all associated, so that for example, your birthday is actually three different numbers associated with them one another: the day, the month, and the year. And we need to have all three contain together; hence the idea of encapsulation.",
                    ""
                ]
            },
            {
                "title": "Enforcing Protection 1.7",
                "data": [
                    "The previous code we looked at was a really simplistic view of a class. And the problem with the simplistic view of a class is that it's storing three integers, and integers, the domain of an integer is negative two point one billion to positive two point one billion. Now I don't know what it means if the day is negative two point one billion or the day is positive three hundred thousand, it really doesn't make sense because in our model in our world, the day can't be a number outside the range of one to thirty one. So rather than allowing uncontrolled or what we call public access previously, we should really protect that data and we should mark the data as being private. Now one thing to note is that if we leave off the notation of private or public, then by default C++ assumes that what we want is private.",
                    "So we can modify the class now to change that public keyword to be a private keyword. And now nobody outside of the date class, can access day, month, or year. And that means change it print it do anything with it; nobody has access to those three variables.",
                    ""
                ]
            },
            {
                "title": "Accessors and Mutators 1.8",
                "data": [
                    "Well now unfortunately since nobody has access to day, month, and year; nobody can store anything into day, month, and year. Nobody can get out the day, month, and year. So what we've done is create a fairly useless class, because day, month, and year are completely inaccessible by anybody. What we really need here are something called accessors and mutators, less formally known as getters and setters. So the accessors are used to get information out of the class and the mutator are used to put information into the class, and that doesn't mean uncontrolled access, what it means is that we can restrict the access so that the values stored in day, month, and year are going to be reasonable.",
                    "",
                    "So we create three functions here that will act as the mutators; so we've got the set day, set month, and set year function, and what I've done is I've created set year as a function inside the class defined inside the class, and set day and set months defined outside of the class. I'm only doing that for demonstration purposes. There's really no difference in the code for creating it inside the class versus creating it outside the class; they both work exactly the same. But what I'm careful to do is not create a lot of code, not to write a lot of code, inside the class because it tends to make things a little bit less easy to read.",
                    "So in this case, all three functions are members of the class which means that they have access to all of the private information. So this is the idea of encapsulation that members of the class will have access to the private information inside the class. And here what I can see from the set day function, if you take a close look at it is, it restricts the setting of the day variable to make sure that the day does not go outside the range of one to thirty one. Now you'll notice that I have not passed in any other variable besides the new value that I'm trying to set and I have access directly to the day variable inside the class. As a member of the class, I have access to all the data members inside the class and all of functions inside the class and I can do that directly without any extra definitions. So the set day and the set month are just definitions that I've put outside the class and to do so, I need the scope resolution operator. But you can see that the code keeps track of making sure that the day and the month are reasonable values, so that we can't set something that's completely outside what we normally expect.",
                    ""
                ]
            },
            {
                "title": "Accessors and Mutators 1.9",
                "data": [
                    "Now that we've got the mutators, we can change the data but that doesn't give us any access to actually view what's in there; so we can store it but we can't change it. If we are going to create a function that is going to access information inside the class but not make any changes to the to the data inside the class, then we really need to mark that function as being a const function. what a const function means is that the object from the start of the function to the end of the function will not change. and it's a protection mechanism to make sure that we don't accidentally make any mistakes by changing variables. So the same as we would use const during our previous programming samples, we now can use const to protect our objects from accidental damage by ourselves, the programmers. Now there is one thing to take into account when we have const functions. If an object is const-ified, so if for example we were to pass an object to a function and we will pass that object by const reference which is very common to do, then the only functions we have access to inside that function is a const function inside the object. So here now if the object is const-ified, the only functions that we can call are const-ified functions inside the object. It's a protection mechanism from C++ to make sure that we can't do more than what the function can do.",
                    ""
                ]
            },
            {
                "title": "Other Useful Functions 1.10",
                "data": [
                    "So now that we've got data that we can store inside the function inside the object and we can get data out of the object, then perhaps we should have some other useful functions. It's really common here in the U.S. that we might print a date in the term, in the form of day slash month slash year. So we could add a function inside our object inside our class where we can display the date in the form of day slash month slash year. So you can take a look at the code for that; it would be a very simple function to write. Again it's const-ified because it's not changing any of the data. We might also write a function to validate, to check to see if the date is valid, and it will return true or false. For example, if somebody puts in February thirty first of two thousand and sixteen, that shouldn't be valid because there really is no February thirty first; so that might return false if we were to have a validate function on that. We could also check to make sure that the leap year is valid and if we need a leap year calculation; we need to check which years are leap years and which years are not leap years that may require another function. And in that case the calculation of a leap year may not be a function that we want to have allowed access to from the outside, which would make that function a private member function. Really what I'm getting at here is the possibilities are endless. What you decide to do with a date class might be very different from what somebody else decides to do with a date class, but you can do literally anything you want with it inside C++.",
                    ""
                ]
            },
            {
                "title": "Creating and Working with an Object 1.11",
                "data": [
                    "So we can create objects just like any other data type. it's now a new data type that we have access to and we can create and we can use. Working with an object requires the use of the dot operator, and you may have seen this before in your work with structs but here it's much more common to use it.",
                    "Remember that we're not going to have any access to the private information, but for example if we want to set the day, the month, and the year, we can have this code to show you how to set a date to for example the sixth, set the month to August, set the year to one nine hundred ninety one and then print out the fact that this is a very important date. And if you're interested, google that and see what date it is.",
                    ""
                ]
            },
            {
                "title": "Constructors 1.12",
                "data": [
                    "One of the problems that we're going to run into is that if we just construct an object like we did before and it's a Date object; what are the values of day month in year immediately after the construction of the Date object, they\u2019re garbage, they\u2019re garbage values. We don't know what their values are and they may not even be valid so it's very possible that we may have a large number for the day, like negative eight hundred thousand and it's now here are stored inside, what we would classify as an acceptable date object, because we created it. So what C++ allows us to do is make sure that we always know that valid values, even if we just created that object, that there are valid values in it. So C++ creates thing called a constructor, and we can create the constructor where it runs code that we want so that we can set the values appropriately for whatever the default date is. So the default constructor is a function which is given a name which is exactly the same and it is case sensitive, it\u2019s exactly the same as the name of the class. There\u2019s no return type, so it\u2019s not void or anything like that, it\u2019s just literally no return type, we don't list anything and it doesn't have any parameters. So we can see from the code that what we've done here is create a constructor where we're setting the values of day, month, and year, to the appropriate to what we classify as an appropriate value, without any interaction by the user. All the user has to do is create one of these Date objects, by default, and it will set those values to what we consider the default values.",
                    ""
                ]
            },
            {
                "title": "Constructor Parts 1.13",
                "data": [
                    "Here I'd like to demonstrate that we can actually do this two different ways. In the previous slide, you saw the method that we used was creating a value for the day, month, and year, and what I'd like to show you here is that we can use what's called the member initialization list. So the constructor, if we put a colon at the end of it and then we have a list of the data members, and we don't have to have all of them we can have some inside the member initialization list and some initial-ised inside the code. This allows us to construct the member variables using whatever values we'd like. But I'd also like you to take a look at the other method, which is creating the data inside the code for the class, without the use of a member initialization list. At this point it's really up to you which you like to do. I just want you to see both. Later on there will be a situation where we can only use the member initialization list, but for now it's entirely up to you which you'd like to use.",
                    ""
                ]
            },
            {
                "title": "More Constructors 1.14",
                "data": [
                    "So we can also have constructors that take, for example three integers. We might have one that takes a day month in year, sets those values. We can also have a constructor that takes just one of the values and constructs it based on the Unix epoch, in case you're interested in constructing it based on that. But what we can do now is inside main, we can now create a date with those three variables so that our code is a little bit easier. So if you remember back couple slides ago, the previous code, we set the date to August sixth one thousand nine hundred one. We had to create the date first and then set each of the three individual values. Now we can take those four lines of code and compact them down into one/ where we create the date and set the values all at the same time. You can create as many constructors as you like; each has to have a unique set of parameters. And you have to remember that even though we have a default constructor, only one constructor will ever be called. So whichever constructor is chosen to be called, that's the only one that will ever be called; it's not like one is going to call the default constructor and then go further on to do some extra work. Construction only happens once and we only call one constructor.",
                    ""
                ]
            },
            {
                "title": "An Important Pointer 1.15",
                "data": [
                    "Now that we've created an object, for example the date object, we have to take a look into the object itself a little bit more. And C++ has a really interesting structure that it creates inside every object and it's called this: this is actually a pointer which points to the object itself. It's sort of like a pointer back into itself. And this pointer points to what we call the calling object, so when you make a function call like a dot set year, the this pointer will be pointing to a inside the set your function. So it's really useful in a lot of cases, for example, if you created a person class and you wanted to have a recognition that the person could be married for example. Then each person would have a spouse pointer, a pointer to another person so speak, and when you get married, of course one person marries the other person but then that second person also marries the first person at the exact same time. So we would have a function call like a dot marrying B, in which case A's spouse pointer should point to B, and B's spouse pointer should point back to A. And the only way that we can get a pointer to a is with this, so the this pointer is needed in a lot of situations where we need to recognize that we have some pointer back to the object that we're working in right now.",
                    ""
                ]
            },
            {
                "title": "Operator Overloading 1.16",
                "data": [
                    "C++ allows us to do something that most of the languages have eliminated; they don't like this anymore. Java has gotten rid of it, Python never had it to begin with, actually Java never had to begin with, to be honest. What we can do with C++ is what we call operator overloading, and operator overloading allows us to take two objects, and for example add them together. We can operate on two objects or on an object, using what we would consider a normal operator.",
                    "",
                    "So there's three different types of operators: we have unary operators which only work on one object one operand. A unary operator would be like plus plus, we would say A plus plus; there is no other object that we're working with. A binary operator works on to operate operands, so two objects, A plus B, for example. And the majority of the operators that we're going to work with are in fact binary operators. So we have all the regular math operators, plus, minus, multiplication, division, module, zero. We have the compound operators, which are things like the plus equals, and the minus equals, and so on and so forth. We have the comparison operators, which are the less than, the greater than, the output, the input operators, and then we also have things like the square brackets operator or what we call the array index of operator.",
                    "",
                    "There's also a category of operators called the ternary operators, and in fact C++ only has one of these. It's called the conditional operator and if you haven't come across it, it's horribly confusing and we don't often like to use it; but it is there and some people like it. So you can have the turnaround operator. Really when boils down to it operators are nothing more than functions, and they're just a strange way to use functions. What C++ actually does is it takes that A plus B and it says, I don't know how to do A plus B, but I know how to make function calls. And it converts the function; it converts the operation, into a function call. What it converts the function call to depends on how we as the programmer decide to overload that operator. If we overload the operator as a nonmember, we\u2019ll talk about this, the operator the function call would look like operator plus A B. So the A plus B function call would be operator plus A B; it would take two parameters. It's a binary operator, it's overloaded as a nonmember, it takes two parameters. However if it's overloaded as a member, then the this object becomes one of the operands and so we only need one parameter. So, the equivalent function call if we're a member, is A dot operator plus B. Here we only have one parameter because the this object, the calling object, is the second, is actually the first operand. Operator functions can be either member or nonmember and we're going to talk of a lot about how to choose which to use. And some operators change the data inside the objects; some return a new object, so it's important to look at what the operator is supposed to do. A compound operator like the plus equals operator should actually change the calling object. Whereas the plus operator A plus B, doesn't change A or B, it in fact return something entirely different. So you have to take into account what the operator is supposed to do, and then you've got to figure out how to write it.",
                    ""
                ]
            },
            {
                "title": "Operator Restrictions 1.17",
                "data": [
                    "C++ has some restrictions; it's not open. There are some operators that can't be overloaded. For example the dot operator; you can't change what it does, it's known as the struct member union select operator and you simply cannot change anything about it. The scope resolution operator, the one that goes between the name of the class and the name of the function when we're defining it outside the class, you can't change that there's nothing you can do with that. There's a couple more that you would probably never try to change; the Star dot which is the de-reference and member select. The size of which is very rarely changed; very rarely even considered, very rarely used. And the ternary operator, you can't change what the ternary operator does. So those are some that are just completely off limits. You can't change anything about them; they are what they are. ",
                    "",
                    "There are some that are restricted that you can overload them but you have to be a member in as an to overload this this operator and that's because the operator has so much involved in what it's going to do inside the class, that C++ says you have to be a member of this class in order to overload it. So these are functions that can only be overloaded operators that can only be overloaded as member functions. So the assignment operator and the array index of operator can only be overloaded as members of the class; you have no choice in that. There's also some that can almost, not be overloaded as a member: the input and the output operators, or technically they're known as the bit shift the left and bit shift right operators, but we all call them the output in the input operators. These operators can be overloaded as a member of the class because if you look at the equivalent function call, if we said something like C out arrow arrow A then the problem is that this would be C out dot operator arrow arrow A. So you wouldn't be making this a member of your class, you would be making this a member of the class which has the object C out, and that's called an O stream. And you're not going to overload override the entire Ostream class; you're going to recreate the entire Ostream class, and create a new C out object just to overload this member. ",
                    "",
                    "So while C++ doesn't technically say that you can't overload this members, overload this operator is a member, it's almost universally accepted that you're going to make this a non-member of the class. So those are the only restrictions; any other operator that you can think of, you can't create your own you can't change the precedence of any operator, but any other operator which C++ has you can overload.",
                    ""
                ]
            },
            {
                "title": "Choosing Member vs. Non-Member 1.18",
                "data": [
                    "A little while ago I mentioned that you could either have an operator overloading as a member or nonmember. So what are the benefits of having one versus the other; why do we have the option of doing both. Well first off, the key thing to remember of course is that members have access to private.",
                    "So if we're a member of the class, if we overload this operators a member of the class, then we can directly access the private information. And in a lot of cases we have to overload it as a member because of those restrictions, because we're not going to create functions to allow everybody to access the private information; we might not have an accessor function so we want to have a limited access to the private data, yet we want to have these operators overloaded. In that case the only choice is to make it a member. ",
                    "",
                    "We also have the option, and this is a little bit strange, but we also have the option of listing a non-member function, as a friend, if inside the class we write the signature, which is the prototype of the function, we write the function signature and before we write friend. Then we're not saying that that function is a member of the class in fact it's by definition a nonmember but it has access to private information. So friend functions have direct access to the private information, even though they\u2019re non-members. And this is a little way of sidestepping the private restrictions inside C++. It is used in some cases and, is in fact used often in the input and output operator, but apart from those operators it's not used very often because we consider it side stepping around the rules, so we don't want to use it. ",
                    "",
                    "Otherwise there's really one fundamental difference between choosing member versus nonmember, apart from the fact that it has access to private. If we're doing for example A plus B; A plus B is going to work in either case whether we choose to overload the plus operator as a member or nonmember function, A plus B will still work. But if we have a constructor which can construct an object out of constant like an integer, then A plus five or A plus an object plus A constant is going to work if there's a constructor, which we can take five and construct one of these objects. So that would working again in either case if we have that constructor; it'll work if it's a member or nonmember. ",
                    "",
                    "But here's where life really gets interesting. If we turn that around and we say five plus A, well of course assuming that we have that constructor that can construct one of these objects out of a constant, five plus A will only work if the plus operator is overloaded as a non-member. C++ has this restriction that it says that because we now are going to require that the left hand side is an object, we have to know that it's not a constant object. And the plus operator overloaded, if it's a member is going to have to have full access to the objects of the object will have to exist before hand It will not be able to be constructed automatically out of a constant. ",
                    "",
                    "So here, the only difference between choosing member a nonmember is that a plus five will work in either case but five plus A will only work if we have a nonmember. Generally speaking this is a small price to pay for over for having access directly to the private information. So personally I usually make things members, but there are a lot of purists in C++ who prefer that you create only those items that are members that have to have access to private. And then create nonmember functions that are based on the use of the member functions. So for example, the plus operator you might be able to overload by using the plus equal operator. So you\u2019d overload the plus equal operator as a member but the plus operator as a nonmember, and then use the plus equal to solve the problem. What you choose is entirely up to you; you\u2019re the programmer and you decide.",
                    ""
                ]
            },
            {
                "title": "What to Return? 1.19",
                "data": [
                    "Now that we understand how we can overload these operators. we have to take a look into what the operator returns, so what's it going to do when it's done the value that's returned really depends on what the operator is going to do. So the plus operator for example is going to return back the sum of the two operands; it doesn't change either operand we said that before, we're not going to change A or B. If we do A plus B, but instead we're going to return back a new value which is the sum of A and B, whatever that means because we haven't defined what data types A and B are. Here we have a little bit of a rule in that we the return data type, really depends on what's being returned of course.",
                    "",
                    "If the item that's being returned was created inside the function, then the return has to be by value.",
                    "So in the plus operator we add together two operands A and B. In order to do that we have to in the plus function, in the operator plus function, we're going to have to create a temporary object, store the value that's in A and add the value that's in B, and then return that temporary object. So we've created an item; we've created something inside the function and now we want to return it. The only choice we have is to return by value. But if we're returning something that was passed in as a parameter. So for example, we have the plus equal operator; the plus equal operator instead of returning the just the sum of the two items, now we're expecting that we're returning the left hand side item. So A plus equal B would return A, and that's something that existed prior to the function call. We didn't create it; it existed prior to the function call. So the return line is probably going to look something like return star this, there\u2019s another use for this. ",
                    "",
                    "And in doing so we recognize that we could create another copy of this object, but the time it takes to create the other copy and the duplications of the data uses more memory is a waste of time and memory. So what we'd rather do is return by reference, because now we don't have to do the copy and do use up the extra memory instead we can just return by reference. Returning by reference is really the preferred mechanism. But in order to do that the object that we are returning has to have existed prior otherwise or returning a reference to something that will no longer exist at the end of the function because it was created inside the function. So if we created it, returned by value; if it was passed into us or existed before, return by reference. And that's what to return.",
                    ""
                ]
            },
            {
                "title": "An Odd Case 1.20",
                "data": [
                    "One interesting thing about C++ is it has both the pre and the post increment operator; of course we've used these before. But we need a special case to differentiate between the pre and the post increment operators because they're both unary operators. So since, if they\u2019re members it's not going to have any parameters; how do we differentiate between the pre increment and the post increment operators. Well C++ resolves this by passing an int to the post increment operator; the pre increment operator is just going to change the value and then it's going to return a reference to the existing operator, it\u2019s going to return star this. ",
                    "",
                    "So the format for that looks exactly like you see on the screen: it's just date ampersand operator plus plus with nothing in the parameter list, returned by reference because we are returning star this. But the posting current operators going to create a copy of the object, it's creating something inside the function, and then is going to change the value of the object, not of the copy, and then it's going to return a copy. So we're returning the original value, or a copy of the original value, and we're changing the value at the same time. Since we're returning by value, we have to make sure that that return is by value. So we\u2019re returning a copy; we\u2019re returning something that we created inside our function so we have to return by value. So you're going to see it as date with no ampersand operator plus plus, and because this is the posting current operator, we need to pass in that integer; so it's date operator plus plus integer. The integer really has no bearing on what happens inside. It's really just a way to differentiate between the pre-increment in the post increment operator and that's the only way that C++ can solve it. So don\u2019t look at the value of the integer; the value of the integers completely irrelevant. Just the fact there is an integer there tells us that it's suppose increment operator.",
                    ""
                ]
            },
            {
                "title": "Classes that contain Dynamic Memory 1.21",
                "data": [
                    "We've got an interesting problem that we've got to take into account here, and that is that if the class contains any dynamic memory then there is an issue that comes up with what we call shallow copy. So all classes have a leftover of C's struct era, where they'll have an assignment operator and what we call a copy constructor, a constructor that can copy an existing object. That's really a left over C++ because with structs we were always able to say A equals B and all the data members would be copied over. Unfortunately, because we're in C++ and we are using pointers, where pointers are involved those built in operators are going to copy the pointers instead of copying where the pointers of pointing to. And that's really problematic and this is known as what's called a shallow copy. ",
                    "",
                    "So I'm going to show you some code here. What we've got is we've got a class called thing, it's very boring, but it contains a pointer to a value. And the thing object constructor is going to create a new integer on the heap which is going to store that value. So here in main I'm creating thing one and thing two, and yes that is a Dr. Seuss reference in case you get it. And what I want you to see is that thing one has a value that points to the value one, and thing two has a value pointer that points integer two on the heap. So on the heap we've got these two integers, one and two, and we've got on the stack we've got two variables, variable one and variable two, each have a value pointer. And the value pointers pointing appropriately, so on the surface this all works perfectly fine and everything's okay. But when we set one equal to two, when we say one equals two, that has the effect of copying the pointers.",
                    "And now that we've copied the pointers, we have the value pointers both pointing at the same object on the heap, instead of the same object holding the same value. So if we looked at the value in object one we would see two and if we look at the value in object two we would see two but the problem is that those two pointers are actually pointing to the same thing. And what happened to the variable, the integer that object one was pointing at, it's a memory leak. So here we've created a memory leak; just by copying an object. And we can\u2019t allow that to happen; so we've got to find a solution to this.",
                    ""
                ]
            },
            {
                "title": "Three Problem, Big 3 Solution 1.22",
                "data": [
                    "Now that we\u2019ve seen the problem, let's take a look at the solution. Well we're going to need to overload; we're going to need to change copy operations. The copy operations need to copy the data and not the pointers, and that's what is really the solution to this problem. Since we're creating memory in the constructor, we're going to need to also destroy that memory at some point. Now we haven't talked about this yet but if you look back at the code when main ends, the data that was created on the heap isn't ever released; we never call delete for that data. So really what we've got are a couple of problems and the solution to that is what we call the Big Three.",
                    "",
                    "And the Big Three is a set of functions that if we need any one of them were going to need them all, and that's almost with one hundred percent certainty. There are very, very rare situations where we might need one copy constructor, for example, we wanted one copy constructor and not need the other two but it's very, very rare. So we call them the big three because if you're going to, if you realize you need any one of them you better create all three of them. The destructor is just like a constructor but the exact opposite; the destructor is called automatically when the object falls out of scope. So at the end of a function call when the object is no longer going to be used, the destructor is called automatically. In the same way that the constructor was called when you created this thing, the destructor is going to be called when it's no longer in use. We also have the copy constructor, which is useful if we want to create an object based on another object; so we might have a copy constructor for the date class that takes in an existing date. And the assignment operators, the third one, it copies one object to another copy and it does that sort of deep copy.",
                    "",
                    "So here we have to be very careful because in a lot of situations that we're going to run across, what we have to do first is clear out the left hand side object. So we might have to empty everything out the left hand side object. And there is one situation that the assignment operator has to take into account and that is self-assignment. If the user is doing something very, very silly and ends up writing something like X equals X, then clearing out the left hand side object will accidentally clear out the right hand side that we're about to make a copy of. And so that we've lost that information that we had intended to make a copy of. To protect against that the code that we like to use is just a simple if statement it\u2019s: if this equal equal the address of RHS. And in doing that what we're doing is testing to see if the this pointer is the same as the thing that we're about to copy, the pointer to the thing that we're about to copy, and if those two are the same pointer then really the pointing at the same object which means we really don't need to do any work we can just return star this. So the big three is the solution to the problem of dynamic memory, and I'll give you the code here to take a look at it. You can see the big three: the destructor, the copy constructor and the assignment operator, are really just a couple of pieces of simple code to take care of what has to be done in terms of a deep copy and in terms of destruction of the object.",
                    ""
                ]
            },
            {
                "title": "Inheritance 1.23",
                "data": [
                    "Now we have a good basis for how to create classes and when to create classes, there's something else that we can do in C++ which is really great; and that's called inheritance. Inheritance allows us to create really, really complex classes out of simpler ones; not just with composition, not just with adding elements into a class, but what we can really do here is create what we call the is a situation or the is a solution.",
                    "",
                    "I'll give you a couple of very simple examples: a car is a vehicle, or a circle is an object or, for example, a student is a person. What we're saying is that the car class should contain everything that the vehicle class can contain. So anything you can do with a vehicle, you could do with a car. The person is the same sort of thing, any information we'd like to store about a person, we'd also like to store about a student. So for storing things like name, and height and age and that sort of stuff for a person, we'd like to store that same information as a student but we also like to store additional information in a student that's not in a person.",
                    "",
                    "So what we're creating here is a larger more complex derived class from an existing base class. So using the base class as just a form so that we can add things and not have to reimplement all the work that was done to create the base class. Now the great part about inheritance is that all the items all the functions, all the data, everything that was in the base class is automatically going to be in the derived class. Unfortunately, we don't copy over the constructors, so the constructors for the base class don't come over to the derived class; we have to recreate any constructors that we might want. But we also have the ability to call those base class constructors so it's not as difficult as it might seem. The derived class, of course, can add any new material that it wants to add; anything that we want to put into the derived class that doesn't exist in the base class can obviously be put in. ",
                    "",
                    "The derived class can create new versions of existing functions and then it's called overriding. We can override a function that exists in the base class by creating a new function in the derived class which does similar things which has the same parameter list which has the same name. One thing that we're going to come into play with, that's going to come into play here, is that we are not changing any of the accessors; so public is still public and private and is still private. And if you remember I said that inside the only member functions of a class can access private data; does that mean that member functions of a derived class can access private data? The answer is no: member functions of a derived class cannot access private data of a base class. Obviously, member functions of a derived class can access its own private data, but not that of the base class. So we have to take that into account when we look at accessing private information. The derived classes member functions can\u2019t access the base class\u2019 private data.",
                    ""
                ]
            },
            {
                "title": "Pets and Cats 1.24",
                "data": [
                    "Here we've got quite a bit of code and I just want to go over it quite simply. We've got a base class called Pet. and the base class pet, what we're doing is we're creating a cat and a pet combination here. So the base class pet has functions like get name and set name and it asked the pet to speak and figure out what the pet speaking does later. But we also have a cat class, which is based on the pet class. If take a look that colon public pet indicates that a cat is a pet, and that's the derivation; that's inheritance right there. The cat doesn't redefine name and pet ID that were created inside the pet class, but it does add a double, which is the whisker length of the cat. The constructor for the cat class tells the pet class to set its pet ID to ten thousand. So here we have an explicit call to the base constructor and if you look, that form of it looks very much like the member initialization list that we saw so many slides ago. Now we call it the base initialization list and it can only be done this way; you can't put anything inside those curly braces in order to initialize the pet. Now keep in mind if you take a look at pet, the cat will not have access to the pet ID because it's a private data member inside the base class. So the only way to set the pet ID is to use that base initialization list. We also have a function speak, which just prints out meow and set length and get length, which are accessors and mutators for the whisker length. And we have another function called set name which coincidentally looks very much like the set name function inside pet. We\u2019re going to look at that in just a minute.",
                    ""
                ]
            },
            {
                "title": "What if we need to override? 1.25",
                "data": [
                    "So we want to look at the set name function because the set name function inside cat is going to do something different then just set the name. We have a rule that says if we change the name of our cats we have to cut off their whiskers; I don't know this is a crazy rule, but that's what it is. And so we have to set that whisker length back to zero any time somebody changes the name of a pet. Now how do we do that? Whisker length is a member of the cat class; only cat can change the whisker length. But name is a member of the pet class and only pet can change the name; so how do we do it? Well, we override the set name function so that if anybody has a cat and sets its name the first thing we're going to do is change the whisker line but we also have to go back and actually set the name because at this point we've overridden the function. This is the new version of the function sequence; C++ will not automatically go and call pets set name if what we're working on is a cat object. So what we do is use the scope resolution operator and indicate the name of the base class function, in this case pet, to indicate that what we're calling is the set name function inside the pet class. If we didn't have this we'd have recursion. If we just said set name; we would be the function calling itself would be calling cats set name function which would be a horrible loop that we'd never get out of. So what we do is we indicate that we're calling the pets set name function and then we pass in the name. We effectively have the solution of solving both problems, setting the whisker length and changing the name, in one easy function.",
                    ""
                ]
            },
            {
                "title": "What if derived SHOULD access base\u2019s stuff? 1.26",
                "data": [
                    "From time to time, there's going to be situations, of course, where we have to have allowance for the derived class to access the base classes\u2019 private member variables. And we can't make them private in that case; so there's no friend possibility here. But what sequels post does create is something called protected.",
                    "",
                    "The protected modifier is added, now we now have public, private, and protected. Protected is specifically for this purpose: it allows a derived member function to access a base class\u2019s protected information. But it doesn't allow any access outside of those two classes: outside of the base class or the derived class. So if we're inside main protected in private or fact of the same thing but if we're inside a derived class we would not have access to the private information, whereas we would have access to the protected information. It doesn't change anything Internal to the class; so it's only in the case of inheritance that protected becomes useful because that's when the derived class would exist. And it would have access to those items marked as protected inside the base class.",
                    ""
                ]
            },
            {
                "title": "Polymorphism 1.27",
                "data": [
                    "This is a topic I actually really like; it's polymorphism. And what we're saying is that since every cat is a pet, we should be able to copy data between cats and pets. Now if we take a look at main memory, what we're going to see is that inside every cat there is a pet object. Not in reality that there is a pet inside, But what we're going to see is that all the stuff that is in pet is also in cat. So if we have a function which expects to take a pet, it could take a cat because the cat class will have all the items that exist inside the pet class. It's going to have more in this case is going to have whisker length but it's going to have everything that was inside the pet. All the functions, all the data, everything that we should be able to do with a pet we should also be able to do with a cat. So since every cat is a pet every cat is going to contain all of the functions inside pet, though not necessarily the same versions and that's where one of the problems really comes along.",
                    "",
                    "Polymorphism in C++ allows us to copy the data over from a cat into a pet object. And that's what I want to want to show you here with a little bit of code. What I've done is create a pet object and a pet pointer and as well as a cat object and a cat pointer and we're going to look at a couple of examples here. If we take a cat and try and store it in a pet, this always works, this is always allowed; it's a solution known as slicing. So taking a cat object and storing it into a pet in this case, P equals C that will always work. But what happens is those objects, those items, of the pet that are in class are copied over. What happens to the whisker like this; it's just completely lost.",
                    "",
                    "So in our example we'll be copying over the name of the cat will be copying over the pet ID of the cat and we\u2019ll store that in the pet object but we can't fit the whisker length, so we just don't copy it. It's very much like when you took an integer and you stored a double in that integer; it's just chopped off the decimal point and threw away all the stuff after the decimal point, didn't do any rounding or anything like that. We can also copy information from a pet and sort of upgrade it to that of a cat, but C++ doesn't do this automatically. C++ won\u2019t allow this to be done automatically; what we will have to do is overload the assignment operator. So inside the cat\u2019s class inside the cat class, if we overload the assignment operator to take a pet object then we'll get this upgrade possibility because the cat class will know what to do, will know what value to set and whisker length. It will copy over or it should copy over those items that are in the pet manually and then you can have whatever value for whisker length you deem appropriate, maybe at zero maybe not, I don't know. But the point is that if we overload the assignment operator for a pet inside the cat class, we're allowed to do cat equals pet. ",
                    "",
                    "The pet pointer being assigned to a cat will always be allowed; and this is the real core of polymorphism this where the magic really happens. Because we have a base class pointer, we can make it point to a derived class object because the derived class object will contain all the functions and all the data that are in the base class. So the pointer can point to a derived class object and we haven't lost anything. It's really important to recognize the difference here between slicing P equal C and polymorphous P pointer equals the address of C. In one case we're taking the cat information in the slicing case we're taking the cat information and storing it into an object that actually is a pet; and the other case we're taking a pet pointer and making it point to a cat. So polymorphism is not working with pets, it's working with pet pointers that are pointing to cats. And that's a huge difference because we have to recognize that the pet pointer is not pointing at a pet; it's pointing at a cat or maybe it's pointing at a dog or a fish or a turtle or whatever kind of pet you want. But the point here is that the pointer can point to any data type as long as it's derived data type of the pet class. All the pet functions, all the pet data is still going to be in those derived class objects so it's allowed. Again we've got to keep in mind that the versions of the functions might be different so that's going to become an issue in the next slide. But for right now we can recognize that everything that's in the base class object will also be in the derived class object. ",
                    "",
                    "If we do have that pointer, the only things that we can use are those things that exist in the base class. So we can't change the whisker length via the P pointer, we can't change the cat's whisker length, even though it is a cat object; P pointer is a base pointer and the only things were allowed to access are those things that exist in the base. So we can set the name and we can set the pet ID number but we can't set the whiskered length using in the pet pointer, we need a cat pointer to do that. And one thing that's very important to recognize is that we can never take a cat pointer and make it point to a pet object. We're never allowed to do assignment between the data type pet pointer and the data type cat pointer. Even though they're both just pointers and even if the pet pointer is actually pointing at a cat, C++ will not allow us ever to copy the pet pointer into a cat pointer. So what we're doing here with CPTR equals to the address of P. There's no way to make that work in C++.",
                    ""
                ]
            },
            {
                "title": "Virtual Functions 1.28",
                "data": [
                    "If we go back to our previous example of having a base class pointer point to a derived class object; C++ makes some interesting assumptions. First off, by default C++ assumes that a base class pointer points to a base class object. And that is not at all true because polymorphism tells us that we can make a base class pointer point to a derived class object. But C++, for good reason, tries to optimize things and say that if we have a base class pointer we can assume that it's pointing to a base class object. Unfortunately, that means that if we call any functions via that base class pointer, the function version that we're going to be calling is that of the base class even if the pointer actually points to the derived class object. So if we, for example in our previous example if you want to take a look at the code going back, if you take a look at the code for the pet object there is a function called speak. And if we call that function on a base pointer, on a pet pointer, we will get no output even if the pointer is pointing in a clear cat object. So by default, we've got this issue where the base class pointer might be pointing to a derived class object and we call the wrong version of the function; that could be catastrophic. Imagine if the destructor for the base class was called but what we wanted to destroy was the derived class. That would be absolutely catastrophic; we'd have a memory leak. ",
                    "",
                    "The solution to this is to mark the function as virtual in the base class. If we put virtual in the base class then C++ waits until run time to make the decision on what your version of the function to call. So if we have a base class pointer and were accessing a function which is marked as virtual C++ stops and says: I don't know which version to call, I'll wait until I can actually see what object I'm working on and then I'll call the version appropriate for that object. So rather than make the static binding decision, rather than decide which version of the function we're going to call at compile time, we do what's called late or dynamic binding at run time when we actually see the object that's when C++ will make the decision of which version of the function to call. The version of the function call depends on the type of object not the type of pointer. And that's the way that we can get around the problem of accessing derived classes functions via the base class pointer.",
                    ""
                ]
            },
            {
                "title": "Pure Virtual 1.29",
                "data": [
                    "There are some situations as you can see where we might not know what the base class should do, but we know that the function should exist. What kind of noise does a pet make? It's a question we really can't answer; until we know what type of pet this is we can ask the pet to speak although all pets can speak. Alright fish make some sort of \u201cblub blub\u201d noise; I don't know. But the point is that the pet class should have a speak member function we don't know how to make a pet speak. All the pets that we're going to create cats and dogs and fish and turtles, and I don't know what else, are going to have a function called speak. And we want to be able to use the base class pointer to access that speak function. So if the base class should contain a function but it doesn't know what the function should actually do, the function can be marked as pure virtual with the equals zero. So here's the code that we're going to look at.",
                    "",
                    "The idea here is that we have the best pet class and we have the speak function. And speak function is going to be virtual, of course, but we don't know what the speak function should do. So we'll make it pure virtual; we'll set it equal zero. And in doing so, it allows us to use the pet class pointer to call the speak function, in fact inside the pet class itself we could even call the speak function. But we don't know how to actually perform the code for the speak function until it's defined later inside the cat class. So if you see the cat will override the speak function and print out meow in this case, and what we've got is the ability to have a pet pointer pointing to a cat object and call the speak function of that cat object.",
                    "Now there are some restrictions though: because the speak function is pure virtual it causes the pet class to become what we call an abstract class. ",
                    "",
                    "Abstract classes have a lot of restrictions on them whereas we can create pointers to an abstract class; we can never instantiate an abstract class. We can't create an object of type pet, because if we were allowed to create an object of type pet then we could possibly call the speak function and then we don't know how to do that. So C++ says if you have even one pure virtual function you can't create an object of this class. So what can we do? We can create pointers and we can derive from this class. So that pet becomes useful as a base class but we can never create an object of it; we can create pointers to lots of pets and they can point to objects which override the speaker function and provide the code. But if any of the derived classes contain pure virtual functions, because the either have it overridden or they've created new pure virtual functions, then they too are abstract classes. In this case the cat class is perfectly fine; we can create an object of the cat class and have a pet pointer point to it.",
                    ""
                ]
            },
            {
                "title": "The Same Function Call results in Different Output 1.30",
                "data": [
                    "Here what I've done is create a dog class. And the dog class is defined as being having a class as the base class and it also creates a speak function. Dogs have ears but that's not important; we don\u2019t care about the ear size. The speak function here will woof instead of meow. And now what I can do in main is create an array of three pet pointers. Now it may be a little bit strange to see the pet star star there, but what I'm saying is that the array is in array of three pet pointers. If I created the array as three pet objects; I would not get any polymorphism. Because polymorphism requires that we're using a pointer to access the object and it requires that the function we're calling is virtual. So here I need three pointers to pets not three pets: I set the first pointer equal to a cat, I set the second pointer equal to a dog, and I set the third pointer equal to a cat. And then I can use a for loop to go through and make all those pets speak. And what I think is great is that if you look at that last line of code, we're making a function call to the speak function via a pet pointer. And we have C++ deciding at run time, which version of the function is going to be called. So it doesn't know which version of the function to call until we actually get to the point in the code, the very nanosecond that C++ is running, that the computer is running that code and it says: oh I'm looking at a cat object so I will call them in the cat's function for speak and I will meow, or I'm looking at a dog object and I'm going to print out woof. So I think that really kind of almost magical, the way that C++ can take the virtual function and a pointer to the base class and decide which object it's actually looking at and call the appropriate function.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 16,
        "module_name": "File Processing Script",
        "file_name": "Module 16 File Processing Script.docx",
        "transcript": [
            {
                "title": "Introduction 1.2",
                "data": [
                    "In this model we're going to start working with files on the file system as input into our programs. We're going to use these files to bring in data into our program and then we're also going to use files to store information after we're done with it. So this whole module is about bringing in information and sending it out to the file system for permanent storage.",
                    ""
                ]
            },
            {
                "title": "File Processing 1.3",
                "data": [
                    "The first thing we have to understand is that there's so much data that has to be entered into most programs that it's really unreasonable to ask the user to type it in at the keyboard each time the program starts. And of course we understand this from our past experiences with Word and Excel and PowerPoint and all the other regular programs that we use generally store that information in a file. And a file is a nice compact way of keeping track of all the data that's needed for a particular run of a program. There needs to be a way to access data on the file system and in fact, we need to talk about both the input and the output. Input means bringing data from the file system into our programs and using it for processing or whatever we want to do with it. Output means taking the data from our program from main memory or from wherever we have it stored in variables or vectors or arrays or wherever, and putting it out on the file system. And this is actually a two part process; there's actually two steps here. So that when we're working on a file we would have read that file initially, we would work on it in main memory, and then we would output that file back to the file system.",
                    ""
                ]
            },
            {
                "title": "Files and Locks 1.4",
                "data": [
                    "Files all contain a name and we know that. But what you might not realize is that files also contain what's called an extension. And the extension tells the operating system whether it's Windows or MacOS or Linux or you, what might be inside that file and what program might be useful to look at the information in that file. So I'm sure your experience with Doc, DOC, which is often opened in Microsoft Word, or DocX which certainly is open in Microsoft Word. You might also be familiar with TXT which is a plain text file that contains no formatting, it contains no extra information; it's simply text. There might be an XLS file. And all these file extensions serve the purpose of telling us what program we want to use to open that file. ",
                    "",
                    "So to open the file, you have to know its complete name. So we're not going to have a nice graphical user interface for our programs, at least not initially, that allows us to select the file. Instead we're going to ask you or the user at the console, to specify the file name; and the filename includes the extension.",
                    "So whereas in Windows you might look at the files and see file without an extension, in reality there's probably an extension that Windows is hiding for you. So the real filename would be something like file dot TXT. ",
                    "",
                    "In addition we know that the file system is organized in directories and you might know these as folders on your operating system. In reality it's simply a list of files that are contained inside that directory. And for simple purposes at least for now until we get into complex operating systems concerns, the file exists in just one directory. And a directory can exist inside of another directory and so on and so forth; we call this a hierarchical database, which we'll talk about later. If you don't specify a directory when you're opening a file, then the operating system assumes that you mean the current directory. ",
                    "Now the question then becomes what is the current directory. And if we're working in Visual Studio and we try and open up a file then the current directory is wherever we have stored our dot CPP file. If we're running an executable program then the current directory is most likely where that executable program is. So this brings up a little bit of diversity. If we run the program inside Visual Studio, it's going to look in the current directory for any files you want to open and that current directory is the dot CPP file. But if we then produce the EXE the executable program and run it manually through windows explorer,",
                    "it's going to be looking in a different directory because the EXE and the CPP files are not in the same place. So you kind of have to keep track of where you expect your programs to look for these files and you have to put them in the right place. Now of course if you don't want to do that, you can specify another directory at least on Windows you can specify using a backslash. So you can start at the top of your C drive and do backslash users backslash your name backslash so on and so forth and so on and so forth and you can specify the entire full path name for that file and then it will open appropriately.",
                    "Most of the time we're not going to be worried about doing that so we're going to be looking at the current directory. And this holds true for Windows it holds true for MacOS, Linux whatever you're using. ",
                    "",
                    "Now the other problem about this is that files can be locked. So if a file is in use by one program, it can't be used by another program at the same time; and this is to protect the consistency of the data inside that file. Now the only exception to that rule is that if both programs want to read the file as input, both programs can have access to that file. So we have what's called a locking system: there's a lock to prohibit reading of a file that's being written to, and that's really what it amounts to.",
                    ""
                ]
            },
            {
                "title": "Objects 1.5",
                "data": [
                    "So C++ has an internal representation object for the files. So there's an object inside C++ which represents our connection to the file. The data type of that is going to differ based on what we're going to do with it; whether it's an input file or an output file. And if we want to have an input file, then the object type that we have to create is called an ifstream. And if we want to have an output file, then the object type we have to create is an ofstream. So it's fairly easy to remember these as fstream objects, and the i is an input file stream and the o is an output file stream so ifstream or ofstream. You have to create the appropriate type and you can't change what you're going to do with that file once it's created. But of course you could always close it and open it is a different type.",
                    ""
                ]
            },
            {
                "title": "Steps to Creating 1.6",
                "data": [
                    "To get this to work, first you have to include fstream. So we put at the top of any program that we're going to use files we're going to put pounding include fstream as you have seen it here. Once we've done that, we have the ability to create those objects of type ifstream and ofstream which is appropriate to the action that we're going to perform. And then we have to open the connection to the file in the file system. So it's not enough to just create the object, we have to actually connect that object to the file in the file system. Now be very cautious, because opening a file could fail. There's a lot of situations which could cause either an output or an input file to fail so it's really important that we check that with an if statement after we've made the attempt to open the file. Once the files open the object can be used for import now put in really the same way that we've been using the cin and the cout variables. We can use the output operator and the input operator appropriately for whatever the data type is we're working with, whether we're doing input or output. And in reality it works very much the same way.",
                    ""
                ]
            },
            {
                "title": "Passing to a Function 1.7",
                "data": [
                    "When we're going to pass an ifstream or an ofstream an object to a function, or return it from a function as we'll see later on. Those have to be passed or returned by reference. So the ampersand has to be there. The reason for that is that the act of writing to or reading from a file, actually changes the object.",
                    "Specifically, we have inside the file object, inside the representation, inside C++, we have a pointer to how far into the file we've been working with. So that when we read in, we're always reading in from where we left off on the last read in operation. Now obviously any read in or write out operation is going to change that file pointer, and that's an act of changing the object which means it has to be passed in by reference or returned by reference from a function.",
                    ""
                ]
            },
            {
                "title": "Cin and Cout 1.8",
                "data": [
                    "Cin and Cout are also objects. We've been working with them over the past few weeks and we didn't realize that they are actually also objects and their data type is an ifstream and an ofstream. Now since we understand inheritance, we know that an ifstream object is actually an istream object as it's base class. Every ifstream can be treated as if it were an istream, and every ofstream can be treated as if it were an ostream object or said another way, if we design a function to accept istream and ostream by reference of course objects then we can also accept ifstream and ofstream objects. Which is a really nice convenient way of saying that we can create a function that can accept either a file or it can accept the screen or keyboard for input and output. And that's very helpful for debugging purposes if we create a function to do all the processing, to do all the output or all the input, and then allow for debugging to send either Cout or Cin to that function.",
                    ""
                ]
            },
            {
                "title": "Output 1.9",
                "data": [
                    "So C++ makes output relatively easy and it's not really very difficult. All we have to do is create one of these ofstream objects; that's the first step. So we create the ofstream object and of course we give it a variable name like out file is descriptive enough to tell us what that file is going to be, and then we have to open it. And when we open it we tell it the full filename that we want to open as you see here we've got filename dot TXT. Or you could also if you wanted to skip the step of opening it, you could also just use the constructor for the ofstream class and pass the filename to the constructor. Only in very rare instances will opening an output file fail; but it still could. For example, you might not have permissions on the drive to create a file, or the drive might simply be out of space. If any of those things happen, we're going to see how to detect that opening a file has failed in just a few minutes. But you should check for that every. If it's important that the file is going to open, you should check to make sure that the file does open. Now once the files open, you can write to the output file exactly as you would write to the screen. So it's a simple output operation you can say outfile arrow arrow, or output operator. And then print out your string or print out your variable or any capability that you would have to write it to the screen output to see out, you have the ability to write it to this output file. And it works again very much like printing to the screen.",
                    ""
                ]
            },
            {
                "title": "More on Output 1.10",
                "data": [
                    "So what happens when we actually open up the file? Well the first thing is that the program, your program, will ask the operating system to open the file. And what the operating system does, for output files, is it checks to see if the file exists. If the file does exist, it's going to be erased and a new file created in its place; so you'll lose the contents of that file. If the file doesn't exist then it's created and either way your program is going to receive back the connection to that file, so that then you can start to output to it. It's really important that when you're finished working with a file, that you close the file. And there's a function inside the ofstream object called close. So you would just simply say out file dot close, for example, and that would close the connection. Because when you're writing to a file what you have to realize is that, you\u2019re actually sending information into a buffer; it's into a memory space. And the operating system is then told a periodic interval to take the memory out of that memory space and physically put it on the on the hard drive. ",
                    "",
                    "What happens if your program crashes or the system shuts down when your program has already written to that memory and hasn't yet flushed that buffer out to the hard drive; you lose all that information, it's all completely lost. So it's best to make sure that you close the file. The other problem of course is that if we have an output file we're doing writing to this and, we remember on the discussion on locks just a minute ago, that if we're out putting information to a file we've locked that file for reading. So if you still have a connection to the file if you still have the file object open, nobody can read in and that includes you if your program continues and tries to read in the data that it just wrote out. It won't be able to because of that locking mechanism. So it's really important you close that file connection when you're done with it. If the of stream object is destroyed because it falls out of scope or if your program ends, of course the file connection is closed in the buffers flush so everything's written at that point. So you don't have to worry about it if your program ends. But if you're going to read in later on it's really important that you close that file and reopen it.",
                    ""
                ]
            },
            {
                "title": "Example: Creating a File and Outputting Information 1.11",
                "data": [
                    "I just wanted to start by talking a little bit about what we can do with outputting a file. So we'll go through the quick procedures of how to create an output file and where that information is stored and then we'll talk about putting some information in the file. So of course as we discussed the first thing that's important is that we pound include fstream. Okay. And from that point we then have options of creating an ofstream object and I'll just call my object, out file here. When I do that I get an object of type a left stream which I can write to and it's not at this point connected to anything. So we haven't specified a file name or anything like that at this point, so we don't really have a file. In order to create or rather connect this object to a physical file, we can do an open operation and we can just give it like file name or let's call it out file dot TXT. And when that happens the file actually gets created. Now if the file already exists it's going to be overwritten, we know that, and if the file doesn't exist it's going to be created. So I'm just going to compile this and give it a run, just that we see. ",
                    "\nNow the question becomes where are these files created because we haven't specified where the file is supposed to be created. So what it's going to do is create it in what's called the current directory, and for visual studio the current directory actually is where the CPP file is. So I'm going to go ahead and open that here; you can see I hope the folder view here and I've got this CPP file and in the CPP file I've got out file dot TXT. So I've got the output file TXT; of course, it's zero bytes because we have to put any information into it. So it was created here and you can see the creation times 12:37 pm, if I run it again I get a different creation time. So now if I go back I'm going to see is 12:38 because now a minute later. So we are creating those files successfully. We also have the option of doing it in one shot of just simply creating the output file and calling the open using the constructor. So I could have done this; although I'll comment it out. I could have done that because you'll see the combination. That would make it not necessary to call the open operation on that file; it'll be opened. How do we output thankfully we can use the outfile object very much like we've been using Cout since the beginning of the semester, and here I have just a \u201chello world\u201d and I'm going to run that program. Now you're going to see that there is some information here in this output file a file dot txt and if we open it up you're see \u201chello world.\u201d",
                    ""
                ]
            },
            {
                "title": "Input 1.12",
                "data": [
                    "Just like the ofstream object, we have an ifstream object which we can use to read information. And we can use the dot open or we can use the constructor for the object to open the file. Unfortunately, input files are a lot more likely to fail when they're opening and the reason for that is more than likely somebody inputted a bad filename. Remember that we're not selecting the file graphically for through a graphical user interface, we're writing in the filename. So if you miss typed the file name, what should the program do. Well when you try and open the file using the ifstream dot open or using the constructor, if the file doesn't exist opening that file will fail. And we won't get a message back from C++; we have to check actively on whether that succeeded or did not succeed. ",
                    "",
                    "So there's really no response from the dot open function, we just make the attempt to open the file and then it's our responsibility to use an IF statement to check to see if opening that file succeeded. Now we can use a simple bool member function inside the ifstream class which allows us to simply say if in file and that's enough to check to see if the file is open and connected properly or if it rather if the ifstream object is open and connected to the file properly; but if we attempt to open it once and it fails then who's to say that the second time we try to open it will succeed. So the problem of course with an if statement is that we're only going to check it once, it's actually better to use a while statement and put all the code for opening the file connection inside the while statement because if the while statement doesn't succeed we won't continue on. So doing while in file, as you'll see in just a minute, is actually a better solution because it allows us to prohibit continue continuing the program until the file actually is opened successfully; whereas, if will only try once and then whether it succeeds or doesn't succeed it's going to continue on. If you are going to try opening the file again as in a while statement or an if statement and not just fail out and say we've got a problem, then you have to understand how the flags operate inside the if stream object. ",
                    "",
                    "When we try to open that file and the file openings fails then a flag is set up inside the stream object to tell us that something has failed. Now unfortunately with the ifstream object even if we have a successful opening later, the flag to tell us that a failure has occurred will still be set. So the solution to this is to do a clear inside the stream object; there's a function called Dot clear and when we before we attempt the opening again we're going to call dot clear. So to just walk you through the process, we'll try to attempt to open the file once. If it fails we'll enter into the While statement to say that it has failed. Then we can check to see that the file has failed to open and then we can call clear to clear the flags and open to open it again. That way if the file succeeds in opening the second time than the flags will now be clear appropriately so. So we have this multistep process towards checking how to open a file, and then once that process is complete we know for sure that the file is actually open.",
                    ""
                ]
            },
            {
                "title": "Example: Opening a File Stream Object 1.13",
                "data": [
                    "Now I'd like to show you how to do some input from a file. And so what I've done is create a couple of sample files here; I'll just show you the first input file. And again it's very important these are text-based files, so if you're creating these files on a Mac you have to make sure that these are created as regular text. If you're creating them on a PC you might just use notepad; notepad is really easy to do to create these files and you can just go and right-click inside the folder and choose new text documents. Don't choose word or anything like that because if you look at a word file it actually contains quite a bit more formatting information and that sort of stuff. We're not going to really care about any formatting; we're just working with text.",
                    "",
                    "So here I've created a text document with just a number of integers and what I'd like to do is create a program, which can read in and maybe find the average of these integers. So we'll keep it very, very simple to start off with. So the first thing is of course that account include my fstream again. I'm going to give you the open input file function and open input file is going to have to take into account the fact that the user might type in the wrong file name. So it's going to pass in the ifstream object which an ifstream object we talked about is a file coming in and when we pass streams to functions, of course we always pass them by reference. So I'm going to pass in my in file, by reference, to my open input file function and I'll just create a string for the file name here and ask the user the file name and I'll read in its file name. And then I'll go ahead and open that file so I'll assume that the in file objects not opened at this point which makes sense since that's what we're asking this function to do. And we'll give it the file name I haven't included string here so I can see that these strings. ",
                    "We hope that the file name opens but hope is not enough we're gonna have to verify that the file actually does open. So I'm going to write a while loop to see if the file is open and you ask why it's a while loop, well because what happens if we try again and it still fails to open a second time and a third time and a fourth time. In fact, you could ask the question how many times is it necessary to try to open a file before successful and the answer is we don't know; we want to continue in this loop till the file actually opens. While in file is not open so while a file is not successful while we have a failure on any file it can say file fails to open and then we can ask the user to do the same process over again: read in in that file name, and then we can try and open it again, but before we open it we have to call clear because what we're testing for is a flag that shows failure. And even if we have a success the previous fail will cause that flag to remain, unless we\u2019ve called clear so before we open it again before we attempt to open this file again we're going to have to clear it that failure flag and then you can try and open it on the file.",
                    "So it's not exactly the same code as previous because you have to clear the flags but we do want to try opening it again and in order to test it right after that. We're going to have to make sure that it's not in a failed state to begin with and then we can try opening it and see if it's a failed state. This function almost becomes wrote because you can use it over and over and over again for different projects that you have to work on, so I\u2019d commit this to memory or at the very least get used to knowing how this function works. And that way when we create an ifstring object called in file we can use the open input file function to open that in file and trust that when it's done that that input file is open. So now we know that in order to get out of that function the input file must be open and ready to read. From that point we can read in maybe into an array or a vector the elements of the file, so I know that these are integers. ",
                    "I'm going to go ahead and read these into a vector of integers, so of course I'll pound vector and then I can have a temporary one so I want to make sure that I can't read in into V there's no simple way to do that. But what I can do is create a temporary object and read in in a while loop from in file into the temporary object; now, what this does is reads in one integer into temp and then we can process it. Now if the input operation if the read in operation is a temp doesn't succeed then we're not going to continue in the while loop, that would be a failure and we'd fall out of the while loop. So like when the file is done when we've read in all the information from the file then the read in operation will fail and we can go on and do something else, but as long as we're able to read in an integer then we can store it in temp and then do something with that. And for this we're going to be real simple, we'll just push that back the push temp back out to V and then if we want let's say the average we can take the sum using our for loop, V sum plus equals the average of the integers we can do some divided by dot size, so that tells us a quick average of the items in the file. And I've called this input one dot txt very important in Windows that you show the file extensions, so if you don't show the file extensions this is only going to show up as input one but it's very important that you recognize the file extensions do actually exist. So if all you're seeing is input one and it says text document then in reality this is input one dot txt. I have it showing all the file extensions for all my files because that's the way I like it.",
                    "I'm going to compile this program I didn't initialize it. Yeah, that's right okay. So I've got to initialize that and then when I run the program txt. The average of the integers in the file is 55 so that's an easy way to read in a whole slew of integers there and you can work on those integers using a vector or a dynamic array of some sort. I'll show you the code just one more time main and finish the open it"
                ]
            },
            {
                "title": "Reading in Data 1.14",
                "data": [
                    "So how do we, now that we have the file connection open, how do we now read in from that I asked stream object. We've been doing it the same way with as with the keyboard.  The only difference here is that in the keyboard instance, we had to wait for the user to input the information so that we could read it in into our program; now everything is open and everything is available inside the file, so we don't have to actually wait for anything to happen. All the read in operations can happen before anything proceeds we can do the entire read in of the file before we proceed. Now one of the questions is how do we detect when we've run out of stuff in the file or even better question is how do we detect if the file never contained anything to begin with; maybe it's just simply empty. And you'll see a lot of books and a lot of it's information on the internet about using dot E.O.F. but I want to caution you against using that because a lot of a lot of these instances don't understand what E.O.F. does: E.O.F. tells us that the end of file has been reached. But unfortunately recognizing that the end of file has been reached requires that we read in that end of file marker. So if we open for example a completely empty file then E.O.F. will not have been reached because we haven't read in the end of file marker, and if we start to then read in and process information will be processing information that doesn't really exist; we will have garbage information. ",
                    "",
                    "So I caution you against using E.O.F. and the better solution to that is to do a while infile arrow arrow O. temp, and what that does is read in the file read in a piece of information into that temporary variable, whatever the data type is not really important. The first piece of information in the file will be read in into that temporary variable and then because we've contain this inside of a while loop the while loop will test in a infile in very much the same way that we just tested it to make sure that it was open successfully. So it's a two-step process, which does both the read in operation and the test to see that it was successful. So we've got a nice compact way of both reading in and testing that it was successful at the same time through this simple while loop format; which works out a lot better than ever then E.O.F. ever actually did. The input operator, we have to understand how that really works, and we might have gone over it in the past but it really bears repeating here. The input operator in C++ is going to skip over all leading whitespace characters; whitespace characters are r your space your tab your return characters anything which does formatting inside the file but we don't actually see.",
                    "",
                    "So, if you were open this in Notepad it shows up white but it's really there. It skips over all the leading whitespace characters and then reads in any valid characters and we\u2019ll talk about what valid characters are in just a minute and then it stops when it reaches any trailing whitespace or any invalid characters.",
                    "So that's the three step process that the input operator is going to doL its going can skip leading whitespace read invalid characters and stop when it reaches trailing whitespace. And if we put this in a loop and constantly read in, then what we're going to be doing is skipping any leading whitespace characters reading in the valid characters and then stopping at the trailing whitespace character but then the next read in operation will skip over those whitespace characters. And if we've done this correctly we can read in the entire file and when we get to the end, we will have completed everything and everything will be working exactly as we expected. So, that's what we're going to aim to do in reading in data.",
                    ""
                ]
            },
            {
                "title": "What\u2019s Valid 1.15",
                "data": [
                    "So we understand that now we're going to do this three step process of skipping leading whitespace characters, reading invalid characters, and stopping when we reach trailing whitespace or an invalid character and the input operator is going to do that for you automatically; We just need to know and understand what's happening internally inside that operator. So what constitutes a valid character really depends on the data type that we're trying to read into. Remember the operation is something like in file our temp. So what's the data type of temp and that really defines what valid characters are. Now temp is a string then anything goes any characters a valid character and we're going to read in a word at a time if you will, so effectively if we had a somebodies name on a line it would read in something like Daniel and then stop at the space between the first in the last name. And then the second read in operation would read the last name like Katz then that's my name. So we need two read in operations to read that in; that's for strings. Now what about integers; we can't read in letters into an integer so a letter would be constituting an invalid character for an integer. Integers can only accept whole numbers. There's no period a period is an invalid character if we're expecting to read in is an integer. But for a double, a period is a valid character but if we have two periods that second period constitutes an invalid character. And for a character, a char the first character is valid but a second character is invalid.",
                    "So whatever the data type is C++, will process the data type appropriately, will process the input appropriate for that data type and its skips over only the invalid characters rather it's stops when it reads the invalid characters.",
                    ""
                ]
            },
            {
                "title": "Getline 1.16",
                "data": [
                    "So, we know how to read in one individual item in the individual string, an individual int or an individual double, but what happens if you want to get a whole line of text and process that whole line of text accordingly. So, in my previous example of reading in a name, what happens if the person\u2026 Some people have a name like Daniel Katz and some people have a name like John J. Jones that would be two items to read in for one person and three items to read in for another person.",
                    "",
                    "We can use the getLine function to get everything up to the end of the line and that's a really useful function. It doesn't skip leading whitespace so this doesn't work the way that input operator does; it doesn't skip any leading whitespace. It captures everything in the line so all the characters in the line. And so we reach the return character at the end of that line and then it stops, and it does not return the return character. I know that's a little bit of a strange way of saying it but the return character at the end of the line is removed from the stream and we don't return that into the string. So the way to call this is just getLine and we have inFile and myString and what will happen is from the in files current pointers, wherever the current position inside in file is, will read in everything up to and in the up to and not including the return character at the end but the return character at the end would be removed. So if we put this in a loop, for example, we could get an array of strings and each of the strings would not contain a return character and it would not really skip the leading whitespace it would include everything on the line. ",
                    "",
                    "So, we have a really nice way of reading entire lines at a time and then processing them in memory. The only thing to take note of is that if the file pointer is pointing to a return character then getLine is going to return nothing because its gonna read a no characters and then throw away the return character also. But this is a great way to get a large chunk of data out of the file when you really don't know how many whitespace characters there are, you can just get the whole line or the rest of the line and then you're set up to read in on the very next line.",
                    ""
                ]
            },
            {
                "title": "Ignore 1.17",
                "data": [
                    "So we can always use the ignore function if we want to skip over a set of characters. Now what the ignore function does is it allows us to tell it how many characters we want to skip or what character we want to stop at skipping. so you'll commonly see something like in File dot ignore, and then nine nine nine nine and then backslash n in a character in a single character scene and single quotes there. which means we're going to skip over nine thousand nine hundred ninety nine characters or the first return character that we see. so it basically skips to the end of the line as long as the line is not more than one thousand nine hundred ninety nine characters. and what this is useful for is if we want to read in part of the line, not the whole line, and then we want to skip to the end of the line. we can use the read in operation like the input operator and then we can once we've got the information that we want we just ignore the rest of the line and then go back to using the input operator to read in the next lines piece of information that we want.",
                    "",
                    "So the ignore function is actually a very useful function. Sometimes we might be at the end of the line, because we just did an input operator and we stop the trailing whitespace, the return characters trailing whitespace, and the next line we want to read in the whole line. if all we did was a get line we would get nothing because we were really looking at the return character, but we can first do it an ignore and skip over that return character and then read in using the Get line then the whole next line.",
                    "So get line and ignore kind of go together and ignore is the solution to having to do get line after we've done an input operation using the input operator. we put in ignore in the middle to make sure that we skip over that return character and the next line we're going to get the whole line As opposed to a blank which is nothing at the end of the line.",
                    ""
                ]
            },
            {
                "title": "Example: Opening a File Stream Object 1.18",
                "data": [
                    "Alright, now I\u2019d like to show what we can do with something a little bit more complex; maybe for an input file. We can take a look at this input file and it has some student IDs and test scores and names associated with each other and tab characters in there. And, let\u2019s imagine that we were given this file and you can imagine it\u2019s a much bigger file than it is, but we have guaranteed in column one student IDs and in column two we have student test scores and in column three we have the students name. And what we\u2019d like to do maybe is order these by test score or we like to find out all the students who had a test score higher than 90. So, lets load this data into a vector inside C++, and then we can start to work on it. So just keep that format in mind: student ID on the first column, test score on the second column, and student name on the third column. And what I\u2019m gonna do is create an object; so, now we have the ability to create an object and I\u2019ll do this easily, I\u2019ll just do it as a struct. And here we go: Student and we\u2019ll say that the integer; we have an ID that\u2019s an integer we have test score and we have a string that\u2019s the students name. ",
                    "So that\u2019s the format of the file and I can create a vector of students, call that VS. S now I've got a vector of students and I\u2019m gonna create one student, again, so this is going to be my temporary student. Now when I go to read in, I\u2019ve already opened the input file using my open input file function that you saw previously. And I\u2019ll go ahead and read in into the temp dot ID, and I know that I\u2019m going to have to do that inside of a while loop because this is going to happen over and over and over again. Now once I\u2019ve done that, I\u2019ve guaranteed that I\u2019ve got a student. And I know that some of you might ask the question of what happens if the file is corrupt; my answer is that if the file is corrupt you really can\u2019t do anything. So we\u2019re assuming that the file is complete and that on every line we have a student ID, a test score, and a student\u2019s name. So, if we read in into the ID, we then still have the additional problem of reading in the student\u2019s test score and then we have the problem of the name. ",
                    "So, now I\u2019ve filled in the temp ID and I\u2019ve filled in the temp test score, but the problem is going to be that this name contains multiple breaks. Now we remember that in a read-in operation, we read in valid characters and we stop when we reach trailing white space, like tab character, or we reach an invalid character and we\u2019re gonna say that we\u2019re not gonna reach any invalid characters anywhere on this file. But how do we go about reading in an entire name? So we\u2019ve read in, let\u2019s say, the first student ID, and we\u2019ve read in the first test score. And the file pointer is now sitting at this point because it stopped when it reached trailing white space, and what we\u2019d like to do is read in this. And that looks simple; it looks like two names. The only problem is that if you look carefully at some of these lines, it\u2019s not two names, its three or perhaps even more names. But the problem is that we\u2019re going to have to read in basically the rest of the line. So, what I want to do is go ahead and read in the whole line, starting from the point where we left of, which was the end of the test score and going to the end of that line. And I have a function for doing that; it\u2019s called getLine. And I can give it the in file and I can give it temp dot name and it will read in the rest of the line. ",
                    "Now I know that getLine seems to describe that it gets the entire line but in fact getLine begins reading from the point where you left off. And that might include that tab character there; in fact, it probably will include that tab character. So what we might do instead is before we do that we can either get the one character, which is the tab character, or what we can do is that if we know they\u2019re tab characters, we can tell it to skip the tab character by saying inFile dot ignore. And inFile dot ignore needs first a number of characters that we\u2019re gonna skip and we know this is just going to be one character, but we\u2019ll put in nine thousand nine hundred and ninety-nine, just for the sake of putting it in. And it\u2019ll stop when it reaches that first character that it should ignore. So, it\u2019ll stop either at ten thousand characters, nine thousand nine hundred ninety-nine characters, or it\u2019ll stop when it reaches the first tab character. And what that\u2019ll do is that it\u2019ll skip over this tab character and then getLine will read in the rest of the line. Of course, I\u2019ve only read into temp and now it\u2019s necessary to go in and push back everything onto the vector. ",
                    "So I\u2019m gonna have push back temp onto the vector, and when I finish with that I should be able to write a simple for loop that gets the student and we can test to see if S test score, we said greater than 90, and if it is then we can output the students name. I\u2019m gonna go ahead and put in a little description here, and let\u2019s go ahead and run that. Okay. Looks good. Input two dot TXT. And you can see that students with test scores over 90 are Daniel Katz, George Washington, and E. F. Johnson. And if we take a look, that is correct; it looks correct. We did not have John H. Jones because John H. Jones was not over 90, he was 90. But that\u2019s perfect; that\u2019s exactly what we expected. So, what we\u2019re doing is we\u2019re reading in the individual items; so, we\u2019re saying we\u2019ve seen an ID, once we\u2019ve seen the id we know there will be a test score, and we know that there will be a tab character, and we know that there will be a name. And we don\u2019t know how many items there are in the name but we\u2019ll go ahead and grab that and push it back onto the vector and then once it\u2019s in the vector we can start working with it. So that\u2019s an easy way to start working with a lot of files; files with a lot of data in them. If you don\u2019t need the getLine, you can just read in straight into temp ID, temp dot test score, and stuff like that. You can keep read in operations, you can tie the read in operations together. So, you might do something a little bit more insightful like that; that would work as well. Okay, you don\u2019t have to but you could; I like to do it just this way but whatever works for you, works. So that\u2019s the way we can read in a large amount of input information and begin processing it. "
                ]
            },
            {
                "title": "Seekg 1.19",
                "data": [
                    "There's another function inside the stream object which is useful, called seekG. And seekG allows us to move the file pointer around skipping over characters either forwards or backwards. We can also use seekG to move back to the beginning of the file. So, if we specify a positive number that moves us that number of characters ahead in the file. If we specify a negative number that takes us back that number of characters, but if we specify as zero takes us back to the beginning of the file. Now one of the things to take care of here it with seekG is that if we've reached the E.O.F. marker, if we\u2019ve reached the end of file and the file is now what we consider in a failed state because we read in the end of file marker, using seekG to send it back to zero doesn't clear those flags. So if what your point is, is get once you're at the end of the file is to move it back to the beginning of the file, make sure you call it clear because it's going to clear those flags also.",
                    ""
                ]
            },
            {
                "title": "Reading and Writing 1.20",
                "data": [
                    "So, what happens if you want to do both reading and writing, and I mentioned this before but it's bears repeating. Surprisingly, it's actually strange to want to do both reading and writing at the same time. and I know the concept that you have in mind is one of like well what if I open up a document in word and I want to edit that document. Well, what word actually does is read in the entire file; it\u2019ll read in your Word document into main memory and when you make any changes you're not making them in the file, you're making them in main memory. And anybody who's forgotten to save the file knows exactly what I'm talking about because when you go back if you hadn't saved it, it won't remember any of those changes that you made. So what's happening is we're reading in the file making the changes in memory and we're writing back over the same file. Which is useful because if we want to save the information out we can either save it out to the same file or we can save it out to a different file altogether. Now usually we'll read in that entire file, will make those changes in memory and then we'll write the file out to the disk and that's a normal process to do; that means we're not going to be doing both reading and writing at the same time. So what you would see is the opening of an input file, the complete reading in operation of the input file, and then calling close on that input file. Then we would do whatever changes are as necessary, open the output file with the same file name, write everything out and then close the output file. So that's really the process for doing both reading and writing; in other words, in place modification of the files is not a normal thing to do.",
                    ""
                ]
            },
            {
                "title": "Appending 1.21",
                "data": [
                    "One thing that we may want to do, which is actually very common to do inside programming and inside the file system, is adding data to the end of a file. So you might have seen a log file, you might have a log file which keeps records of transactions or you might just want to record a large amount of information and you don't want to have to read in the data and then add to the end of it and write it, out as we've said we would have to do. So there is an option inside C++ called appending and what appending does is leave the contents of the file alone. So in the original file, nothing will have changed and what we do is add on to the end of the file. ",
                    "",
                    "To do this we have a second parameter that's passed to the open function so when we open it, we're going to specify the file name first and then we're going to specify this IOS double colon APP. And that's the way to tell C++ any ofstream object that we don't know that we want to append to the file and not overwrite it. If the file doesn't exist, it\u2019s going to be created and then we\u2019ll add on to the end of this brand new blank file, so it's effectively like writing. If the file doesn't exist, then IOS APP really has no impact on what happens internally in the ofstream object. However if the file does exist, it sort of does seekG, if you will, to the very end of the file and then any information that you write into the file is going to be added on the end of the file, rather than overwriting what's existing there. So, if what we're doing is taking in data from the user and storing it on the file system; we might take in some data in one run of the program, write it out to a new file and then the second run of the program, we don't want to overwrite what the user already gave us, so we'll just append on to the end of the file.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 17,
        "module_name": "Linked Lists Script",
        "file_name": "Module 17 Linked Lists Script.docx",
        "transcript": [
            {
                "title": "In this Module 1.2",
                "data": [
                    "In this module, we're going to learn about linked lists and what a linked list is and why we need linked lists. And then we're going to talk about templates for classes and functions, so that we can work with any data type. And then we're going to talk about how a linked list is designed and what we use linked lists for. And we're going to use linked list extensively for the rest of the semester here. In the later modules, you're going to see a lot of implementations of linked list; so we just want you to have a background for understanding what a linked list is and how you would use it and when you would work with it.",
                    ""
                ]
            },
            {
                "title": "What is a Linked List 1.3",
                "data": [
                    "The linked list data structure is one of the fundamental data structures in computer science. It\u2019s basic idea is that it has two parts each: it's made up of nodes and each node has two parts. The first part is a data section which could be any data type, but that's the component, that's the element that we're going to store. If we were talking about an array this would be the individual item in an array but it's not so keep that in mind. So we've got this node that has a data section and it also has at least one pointer, if not multiple pointers to the next sections or to the rest of the list. And the idea is that the pointers are connected in a form of a chain, so that we have a pointer to the first node in the list and we call that the head pointer. We record the head pointer to the first node of the list and then the first node of the list\u2019s next pointer will point to the second node of the list, and the second node of the list\u2019s next pointer will point to the third node of the list, and so on and so forth and so on and so forth.",
                    "",
                    "When we get to the end, we have what's called a null pointer; so we have the null which is an indication. So when the next pointers pointing at null that's an indication, that this is the last node on the list. So these nodes are tied together through one direction at least, where each node points to the next node. Now since this is only the most basic form of the linked list, in reality what's usually stored is both the next pointer and a previous pointer; so you may end up having two pointers in the in the linked list structure and of course the previous pointer just points at the node before this node, and of course the before the first node would be a null pointer. So it's sort of like a linked list in reverse: we've got the head pointer that points of the first node, the first node points at the second node, the second node points at the third node and so on and so forth. So this is the idea of how we can store a large amount of data in a linked list and we're keeping them in individual nodes; remember these are node pointers and they're pointing at individual nodes.",
                    ""
                ]
            },
            {
                "title": "Linked List Visual 1.4",
                "data": [
                    "So what you can see here is a visual representation of the linked list. We have a head pointer that points at the first note and inside the node there is an object called a data section and the next pointer the next pointer points at the next node. So the pointers are all the same pointers; they\u2019re list node pointers. That's really what we're going to see later on in this and this module. So the objects that are being stored are whatever data type you'd like, we'll see how to do that later with templates, whatever data type you'd like. And we have this head pointer that points at the first node, the next pointer points the second node, the next pointer the second node points at the third node, and the next pointer of the third node points at null. So there's three nodes on this list. There's three data items on this list you can call them: one, two, and three, if they were integers, and you can store in them whatever integer you want. Now this list might also have a tail pointer to point at the last node and that's functionally dependent on what we decide to do with this linked list. So, we may end up needing a tail pointer and if we do then we can have that as well to point to the last node in the list. And that's just for easy access if, for example, we want to expand on the list by adding a node then we can do that easily through the tail pointer or through the head pointer.",
                    ""
                ]
            },
            {
                "title": "Why Do We Need Linked Lists 1.5",
                "data": [
                    "We've been working with the arrays up to this point, and they array data structure works great for how we'd like to store information. Unfortunately there are some limitations on arrays; if we were doing insertion into an array even if the insertion that we're doing is on to the end of the array, we have to make sure that there's space in the array available. If there isn't space available, we have to expand the array as we saw in the pointers and dynamic memory discussion. We have to expand the array and doing so takes big O of N. Regardless of that, if we want to do an insertion at the beginning of an array or if we wanted to an insertion in the middle of an array, we have to push back every element that's beyond that insertion point. So we have to go to the last element and move it back by one, we have to go to the second to last to move it to the last element and so on and so forth. That's a severe limitation on arrays in that we can't insert into the middle of them without a big O of N process.",
                    "",
                    "But we don't have that problem with a linked list with a linked list. We can do an arbitrary insertion on to the beginning the end or anywhere in the middle in constant time we can do it in big O of one. And the reason for that is because these pointers keep track of where everything is going. And as long as we're suing that it's being stored in main memory, which it usually is, if we're using main memory that's ram we can access any node at a constant time. So we don't have a problem with being able to insert into the middle of the linked list. ",
                    "",
                    "Now unfortunately, the downside of the linked list is that in order to go through to find any node in the list we're going to have to use a linear searching algorithm so it's going to be big O of N. So we can't arbitrarily go to the third node of a linked list like we could with an array. With an array, if we want the third node of an array we simply go to that memory location and access the third element. But with a linked list to get to the third node you have to go through the second note, and to go through the second node have to go through the first node and so on and so forth. So we have these differences in access and insertion times and each one serves its own purpose. If we don't need arbitrary access to the middle of a linked list or the middle of the data set then a linked list would be a perfect way to store that. But if we do need arbitrary access to the middle of the dataset then a linked list is not a great idea. So it's really just playing the averages of what we expect to be doing with these things that we can make a decision on to whether we should use a linked list or an array. Array insertion is big O of N; linked list insertion is constant time. Array access is constant time, whereas linked list access is linear so you make the decision. Now linked lists can be reorganized; they can be merged. They can be broken apart into new lists. These operations only take constant time because we're just manipulating the pointers. linked lists don't require any overhead for storage. When we were working with vectors, we had to make sure to leave a little bit of extra room in case of expansion. With the linked list, we have no need for that so all we have of those pointers and they only take up four bytes for each node so hopefully it's not too significant a factor. But the idea here is that we can make a decision consciously about whether we choose an array or a linked list to get the job done.",
                    ""
                ]
            },
            {
                "title": "Working with Templates 1.6",
                "data": [
                    "Before we get too deep into linked lists, I have to talk to you about something else that we need to go over first. C++ has this idea of templates and a lot of languages have these now; they might call them generics, they might call them templates. But the idea is that we don't really know what data type we're going to be working with.",
                    "",
                    "So in C++ we can template; we can use a template when we don't know what the data type is of one or more elements. Before each function or class that we're going to be working with, we have to put this indicator this is a template class T. And T is just my individual use, you can really use any name but I just used to see all the time so that's what I normally consider. And that's a data type that we don't know which data type it's actually going to be in the end. What C++ plus does is basically a find and replace through that function to replace T with the appropriate data type, whatever the data type is. So I've got here a function called my swap, which is going to be a templated function and it takes in two items and I don't know what data type the items are. It could be two integers. It could be two doubles. It could be two floats. It could be two strings. It could be two elephants. What it can't be is one int and one elephant, so we can't mix and match data type. T has to be T consistently whatever the data type is. ",
                    "",
                    "So here we're going to swap two items, meaning we're going to make the value that was in A into B. And we're going to take the value that was in B and store it in A, and the code for that is overly simplistic; it's just three copy operations. But the real takeaway here is that you can use any data type for this my swap function. And in C++ you would just call my swap and give it two integers or two elephants, it would be fine with that and you don't need to rewrite this function numerous times simply to change the data type. And that's a huge benefit when we get to working with larger classes, particularly with storage of items where we don't know what data type is going to be stored in really we don't care what data type is going to be store; our job is to simply store the data type. And I imagine this the way I tell my students to envision this is, imagine if you're running a storage company, like the big the big storage companies here in New York where you can rent out a room. The company doesn't care what you put into your room. I guess they might care if it's going to be dangerous but they don't care what you're going to put into that room; they simply care that you own the room. And that's exactly what we're doing here. We're saying we'd like to the ability to swap two things and don't worry about what those two things are.",
                    ""
                ]
            },
            {
                "title": "Templated Classes 1.7",
                "data": [
                    "There's a little bit of extra work to be done when we template a class. So when we template the class, the name of the class actually changes to incorporate the template. So here I've got a simple description of a class called \u201csum valve.\u201d Where we've templated it to store a simple data item and this is leading up to what a linked list node would look like and we'll get to that in a later slide. But basically the sum val class stores one data item, and it's got a function for get val and for set val, and we don't know what the T data type is and really we don't care what\u2019s the T data type. And now inside main what we do is we\u2019d create this as sum val int and that's where you've seen before, that's where a vector comes from. So you've done this before with vector and now you understand why; it's because vector is templated. So we have to tell it what data type we're working with inside the sum val object, so when we create it we say sum val int var name or sum val char var name or sum val whatever var name and you're giving it a data type to now store inside some val. Now the one thing to take notice of here, is that every function that we create has to be templated. So the set Val function, for example, has to be a templated function because first off it has a parameter of type T. But also because the class name is now changed to some value less than T greater than. So we can see that the actual name of the, of the class changes to incorporate the data type. And that's just an internal C++ representation but we do have to take note of that for any time we're going to work with a templated function inside of a templated class.",
                    ""
                ]
            },
            {
                "title": "Designing a Linked List Node 1.8",
                "data": [
                    "First thing we have to do and we're really designed to classes here. But the first class we're going to design is a link list node class and of course can be templated because it's going to have to store the data item and we don't know what the data type of the data item is. So immediately we recognize that we have a templated class. So the nodes are templated. And what we're going to be storing here is the T data item and that is really the data that's going to be stored inside each node, so this is the data storage that we've got. The next thing that we've got is a pointer to the next node and if you pay attention to that, if you look carefully, you're going to see that this is an eldest node T pointer. So you're actually got a pointer to a node when we're creating the node. So yeah. This works because we can create a pointer to the class object that we don't have yet created, so we've created the LS Node class and in the process of creating the LS node class we need an Ellis node pointer. But that does work; C++ was allowed us to do that. ",
                    "",
                    "The next thing that we have is just a simple constructor and the constructor I just made to make life easier. It's really it's not necessary, but it's yet very useful in certain circumstances that you'll see. For example in the recursive copy operation that we do later on, we use the LS node constructor so that we can pass in either a data item or we can pass in a pointer to the next item and then of course it constructs those appropriately. So you have to look back to the object oriented lesson, if you need assistance on figuring that one out.",
                    "",
                    "Then we also have a friend class, which is the LS class. The LS class should have access to everything inside the Ellis node. So we'll make the whole class a friend of this class so that all the functions inside the Ellis class do have access to all the private data here inside the illest Node class. So it basically makes this a lot more accessible and makes it a little bit easier. It's not necessary. We could do getters and setters but there's not really a great reason to do getters and setters especially because the Ellis class is going to need to manipulate those next pointers; so it can get very complex. So that's why I do the friend there. There's not much to this class; it's a very simple class. And in fact, in years past it was actually implemented as a struct with no functions because the only function that we really have is the constructor. So here we've got a very simple class to take care of the LS node and it does store data item so we're going to have to template it.",
                    ""
                ]
            },
            {
                "title": "Designing a Linked List 1.9",
                "data": [
                    "Here's the basic format for the linked list; there's not much to it. It is a little bit busy here but we'll go through it step by step. The first thing that you are going to see is that there's a head pointer and really that's the only thing that this entire class needs to store; a pointer to the first node in the list. And of course it\u2019s an LList node pointer, it looks exactly like the next pointer that we saw in the class, just a moment ago. But here we've got a pointer to the first node in the list. Now of course if that head pointer is null then that means there's nothing on the list, so that's going to be our indication for an empty list. And if you look down into the public section first line in the public section, you'll see that that's what the default constructor actually does; it just sets had pointer equal to head equal to null pointer.",
                    "",
                    "We've got a copy constructor and assignment operator and destructor; that takes care of our big three operations. So those are our handle on the next three lines, because we are working with pointers, we are working with data storage that's on the heap, and we are going to have to make sure that that they had a storage is consistent. So again look back at your pointers and dynamic memory discussion to look at how the big three plays into a factor here. ",
                    "The insertAtHead function is simply going to update the head pointer to point at the new node. So to do an insert and head all we have to do is set the head equal to a new node and make that new node\u2019s next pointer equal to the original head. So that means keeping track of what the original head pointer was maybe it was null, maybe it was a node. But if we're doing inserted head all we have to do is create a new node and insert it physically into the list.",
                    "",
                    "removeFromHead is sort of the same thing which had to advance the head pointer to heads next. isEmpty is a simple function that just checks to see if the head pointer is null. Clear means we're going to have to eliminate these nodes and the easiest way to eliminate all the nodes is just constantly call remove from head in a loop until isEmpty. So it actually turns out to be while not is empty remove from head and that's it; that's the entire clear function. insertAtEnd we're going to see a little bit later on takes a little bit more effort and then insertAtPoint is almost the same sort of thing. And then we have a size function to tell us how many nodes there are actually on the list, which we're going to see in the next slide also. ",
                    "",
                    "So here is the basic format for the linked list if you boil it down, it's really just a head pointer and that's the entire linked list. And before we had object orientation back in C, linked lists were created with a struct for the node and then you would just simply keep a pointer to the beginning of the linked list and passed that pointer around. So we could do this all without a class, it\u2019s just having the class keeps it all encapsulated which is the point of a class together and makes it a little bit easier to work with. But there's not a lot of effort here and we get a lot of benefit.",
                    ""
                ]
            },
            {
                "title": "Stopping at the End vs. Going off the End 1.10",
                "data": [
                    "One of the problems a lot of students have with linked lists is the idea of whether they should stop at the last node and do some more, or whether they need to go off into the null. And this often comes down to a lack of understanding with when to stop. So I'd like to go over that at this point. Both solutions are going to use a while loop, but the conditions inside those while loops are going to be slightly different. ",
                    "",
                    "Stopping at the end means that we want to run the while loop as long as the next pointer in the node that we're looking at is not equal to the null pointer. Now that requires that we keep a little bit of track of the current pointer. And it also requires that we're very careful that we make sure that we have at least one node, because if we start a temp pointer at null and we say temp pointer is next, then we've just de-ref null and the program crashed. So we've got to make sure that the linked list has at least one node and if we can do that then we can advance that temporary pointer until we reach the very end of the linked list, the very last node in the linked list.",
                    "",
                    "However, if what we want to do for example is count the number of nodes on the list then we want to include that last node and we want to go until the temporary pointer equals the null pointer. Here's the biggest problem: we can't go backwards. Once we've reached know all we can't take a step back and move to the last node, so we've got to decide from the beginning whether we should go and stop at the last note or whether we should go off the end of the last node because we don't even need it. Here's a here's a size function so here's the function that size and what it does a starts a simple counter at zero sets the temporary pointer equal to head and runs that while loop until the temporary pointer gets to null pointer. It really is just advancing that temporary pointer; you see that line of code that says temp equals temps next, temp arrow next that line of code is going to advance that point or node by node and we're going to update the counter. When we're done temp is going to be equal to the null pointer and we're done, we've counted all the nodes. So if you take a look at a couple of views of this, if the list is empty to begin with then temp equals no all temp equals had causes temp to be null, we don't do anything in the while loop and we will return zero which makes sense because list is empty. So that's the simple idea of how to do a size function but if we want to see how to do some work at the end like an insertion at the end assuming we don't have a tail pointer. Then the first thing to check of course is to see if the list is empty and if the list is empty, then an insert at the end is the same as the insert at the head. So let's just go ahead and use that code again so we can reuse that inserted head code. But assuming that there are some nodes on the list, we're going to have to go search for the last note on the list and that's what this\u2026 That's what this function is going to do here. The temporary pointer points at the new node that we're going to create. And now it's time to find out where to insert this new node so we have the end pointer and calling end pointer and we say while ends next is not equal to null pointer then we just advance that and pointer. ",
                    "",
                    "Of course this is a linear time problem, it's big O of N to find the last node in the list and then we can just update the next pointer of the last note on the list to point to the temp, to point to the new node. So what we've done is we've created a loop that finds us the last node and it's very different from the loop before it which counts the last node, or counts all the nodes including the last node. So we've got to keep in mind that those conditions are quite different in those while loops and to use the appropriate one at the appropriate time.",
                    ""
                ]
            },
            {
                "title": "Recursion in Lists 1.11",
                "data": [
                    "Going back to discussion on recursion that we had a few modules ago; you're going to see a lot of recursion here in linked lists. It's often used in linked lists because if we look at a sub list, if you look at a list that doesn't necessarily start at the original head, you're going to see that it looks exactly like the larger list and in fact, even the empty list at the very end will look like a regular old list. So recursion is a real big popular topic here in linked lists because everything can be done using recursion or almost everything can be done using recursion. ",
                    "",
                    "So here what I'd like to demonstrate is the recursive copy function and what I did was I just simply have a pointer. So for example the assignment operator or the copy constructor might call this to make a copy of the right hand side list and it would be passed the right hand side head node pointer. And if you look at the only thing it does is check to see if the right hand side had nodes pointer is null. If it is then we return know it's an empty list we're done. And if not we can construct a new node based on the right hand sides data, of course, so that makes the actual copy of the data and then call recursive copy for the right hand sides next. So we're using the constructor here, that we created in the list node class to our benefit, so we can construct a list node based on the data and the copy of the right hand side next. So in fact if we look at this, if you look at the recursive algorithm, this copies the null pointer first we get the null pointer back and then it copies the last node and returns the pointer to the last node. So the recursive copy function which is copying the next to last node and on and on and on and on at the end. When we finish copying we finish copying the first node in the list and we return back the pointer, which is then stored as head and we've got the list copied. So ultimately this makes what would be a very complex or very time consuming algorithm, because remember even if we didn't insert an end that would be a linear time problem just to find the end. So we dealt them at least have a big O of N squared algorithm to find and search all the nodes which would be horrible but here we can do a recursive copy in linear time and be done with it.",
                    ""
                ]
            },
            {
                "title": "What are Linked Lists Used For 1.12",
                "data": [
                    "You may ask yourself where we're going to use linked lists and it's a reasonable question, and anywhere that we need storage where we have constant time insertion with no overhead and we don't really need anything other than linear access to the individual nodes. That comes into play in a lot of situations in computer science so you'll run across this constantly. In a later module, we're going to talk about using linked lists to develop stacks and develop cues. We're also going to talk about other data structures which are either similar to linked lists or are based on linked lists. ",
                    "",
                    "So there's a lot of situations like that but one real world situation that you might actually have come across is the FAT thirty two file system. And this file system was in use in a lot of places for a long time; ever since from the basically the Windows ninety-five era on up through to, well it was still being used a little bit in Windows XP, but we stopped using it pretty much in Windows seven. You still use it today in a lot of situations where you have to transfer a file from a Mac machine to a PC and it needs to be editable on both of them. The only file system, at least right now, that's compatible for editing with both MacOS and with Windows is the FAT thirty two file system so we still use it. This file system stores a file, when we're storing a file what we do is we store a pointer to the first block on the hard drive where the file is stored and the first block on the hard drive stores a pointer to the second block on the hard drive.  It's got quite a bit of data, but it also has a pointer to the second block. And then the second block stores a large amount of data, usually about four kilobytes, and a pointer to the third block and so on and so on and so on and so forth. The reason that this works is because very standardized format, we all understand it; it's a very old format so everybody knows how to work with it. ",
                    "",
                    "But there are some downsides to it and let me explain one of the biggest downsides that you might have actually experience. If you had a large video files stored on a FAT thirty two file system, this video file would be stored as pointers and the video file might be, let's say four gigabytes. But that means that it's broken down into four million blocks; each one of which stores a small portion of the video file. Now if you watch the video straight through you won\u2019t notice any problem, you\u2019ll retrieve the first block on your video player will display that and then the second block in your video player will display that and so on and so forth and so on and so forth. But if you're like me and you stop and start movies a whole bunch of times; you might stop the movie restart your machine come back to it a week later. And you drag the slider to the middle, well unfortunately we can't jump to the middle of that set of blocks. So we can't jump to the five hundred thousandth block out of the one million to get to the middle. What we actually have to do is access all five hundred thousand blocks and then we can get to the five hundred thousand and one block. So this is a downside of the FAT thirty two file system and you may have experience that if you have a video file stored on a FAT thirty two file system, drag the slider and wait for it to load all those five hundred thousand blocks and throw them away because all they're doing is accessing the pointers sequentially.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 18,
        "module_name": "Stacks and Queues Script",
        "file_name": "Module 18 Stacks and Queues Script.docx",
        "transcript": [
            {
                "title": "Stacks and Queues \u2013 Intro 1.2",
                "data": [
                    "In this module we're going to talk about a couple of data structures the two data structures that we're going to talk about our stacks and queues. And these really popular data structures in computer science and we use them over and over and over again. So we're going to talk about first the stack we are going to explain what it is and how it works and what it's designed for and then I'm going to talk about some uses that we could apply the stack to and then we're going to talk about queues. We're going to talk about the same thing what it is and how it works and what it's designed for and then what we use queues for. So coming away from this you're going to have a good look at two very fundamental data structures in computer science and you should understand both how they are built and how they are used.",
                    ""
                ]
            },
            {
                "title": "Stack 1.3",
                "data": [
                    "So our first data structure we're going to talk about is a stack and the way that you can best understand this is if you think about going to like a buffet at a large restaurant and at the beginning of the buffet there's a stack of plates. And the plates are nice and warm they're recessed into the table so the only plate that you can grab is the very top plate in the stack. So here we have this stack of plates it's recessed into the table and all you can grab at is the very top of the stack. And that's one of the purposes of a stack you can't access arbitrary information in the stack. So once you put things into the stack you can access only the last thing that's been put into the stack and then on top of that when it comes out of the stack you get the last thing that was put in so this is what's called a last in first out or first in last out depending on who you talk to. Which is abbreviated as either a FILO or a LIFO or a buffer. So the point here is that when we put things into the stack the last item that we inserted is the first one that's removed from the stack. Now the functions that we use on a stack are push and pop and top and those are the common functions that we use. Push to put something into the stack pop to remove that top element from the stack. We have a couple of supplemental functions that do other purposes of course like clear and is empty and size. You've probably seen these before they're pretty common clear throws away everything that's on the stack so if you just want to throw the whole stack away we can call clear. is empty tells us if there's anything left on the stack so it will return true if there is something at least and size will tell us how many things there are left on the stack. So we're going to work with these stacks and we're going to design how the stack works in the next slide.",
                    ""
                ]
            },
            {
                "title": "Stack \u2013 How it Works 1.4",
                "data": [
                    "So the two fundamental functions again are push and pop and of course we'll have top and that just accesses the highest element on the stack. But the push function is going to add to the front of the stack and the pop simply removes from the front of the stack and you can do this for the back also it really doesn't matter. But the point is that the push and the pop are inserting and removing from the same place so that means that when we do the latest push the next pop is going to cause that item to be to come out immediately. So we don't have to or we are not looking at further information down in the stack now of course when we implement it will have to implement all those supplemental functions like clear and is empty and size. And we'll also have to implement the big three if we're talking about using an array. Now for this demonstration we're not going to use an array. We'll actually use the STL list class which already has the big three already built but I just did that so that it's a little bit easier for us to understand we don't have to worry about the complexity of building the big three.",
                    ""
                ]
            },
            {
                "title": "Stack Storage 1.5",
                "data": [
                    "So we're going to have to store all the information from the stack so the user is going to give us a ton of information through push operations and we're going to have to put that information somewhere and the two data structures that we've worked with so far that we can use as fundamental storage are the list and the array.",
                    "So we have to decide if we want to store the information that's given to us as a list in which case we have a linked list with each node pointing to the next node or as an array in which case we have sequential storage of a limited size where we can add on. So if we take a look at the comparison between the two methods that we could use and we look at each individual function. You'll see that the list for example the push operation is a constant time operation big O of one. Because every time you add to the list all we're going to do is create a new node we are just going to put it on the front of the list and we're going to link it to the next node in the list. So a push operation is actually very simple and it doesn't take anything more than constant time we don't care if there's ten thousand of these elements already on the stack the next one that we're going to push isn\u2019t going to take any additional amount of time. As a comparison we have the array and if we do a push on an array sometimes it's constant time if we have enough space left on the array and sometimes it's going to be a linear time of the big O of N. And that's because we may have to expand that array if we ran out of space we may have to expand that array to create more space. So sometimes it's going to be constant time and sometimes it's going to be linear time which creates a little bit of inconsistency. If we need that consistency then that would negate the possibility of using an array. ",
                    "",
                    "As far as a pop it's always going to be a big O of one it is always going to be a constant time removal from the list we just throw away the top node. From the array we could just reduce the size by one the top element accessing the top element should always be constant time and that's really fundamental that\u2019s important we want that. From the list we would just access that top element through the head pointer from the array we would just access the zero element or if we wanted to size minus one element. For clear it's always going to be constant time for the array but it's going to be linear time for the linked list and then with is empty we always have just the size parameter we can always check to see if the size zero. And for the size well the array of course is going to be a constant time. How can we make the linked list be constant time? Well since we know how many push operations there is going to be we can simply use the size parameter we can create an integer and store that size parameter so that we always have it in a constant time access so we won't have to run through the list and actually calculate how many elements are on the list. We\u2019ll just keep an integer in the in the class that records how many elements are actually in the list at any given time. So if you take a look at these we can compare easily how the list works with how the array works and we have now the tools to understand the performance benefits. So we can choose between either the list or the array.",
                    ""
                ]
            },
            {
                "title": "Stack Storage \u2013 Continued 1.6",
                "data": [
                    "So now we've seen how long each of the functions are going to take we can make a decision about whether we would use a list or an array to do the actual storage underlying stack. And it's pretty obvious at this point I think we would make the decision to do a linked list because we're seeing that we're going to do a lot of pop operations a lot more pushes a lot more pop operations than we would the clear operation. So it's relatively unlikely to do a clear as compared to a pop operation. Remember that the clear just completely dumps everything whereas the pop operation would remove one element. So to that end I think it makes considerably more sense to choose to use a linked list as the underlying storage mechanism for the stack rather than to use the array. And this is a great analysis that should be done for most of the data structures or for most of your programs that you're going to do make the decision as to whether or not you want to use A or B and play out the analysis. So that's just one of the points that I want to make is is it's not clear immediately whether we should use an array or list but once we flesh out what's actually going to happen on the on the stack you're going to see that the linked list actually plays out a lot better. So for this purposes we're going to use the STL list class we are not going to build it ourselves. So that's fine we know how to build a linked list we did that in a previous module so you don't have to worry about that. And the STL list includes all of the big three and includes all of the features of a linked list class so we can deal with just the stack portion. For our purposes the push function is going to call just simply push front on the STL list the pop function will call pop front. So we're going to push on to the front of the list and when it comes time to remove we're going to pop off the front to the list so we're popping the most recently pushed element. The other functions we're just going to map to the equivalent STL functions. So there shouldn't be very much code as far as we're concerned creating this.",
                    ""
                ]
            },
            {
                "title": "Stack Code 1.7",
                "data": [
                    "So here's the code for the stack and you can see it fits easily in a couple lines it really doesn't take up a lot of space. We have as a private data member we have this data member called data which were storing a linked list of T objects. So we've included the STL list here and you can see that we're just using that underlying storage just as a mechanism to create the stack. The push function like we said it's just going to do a push front the pop function is just going to do a pop front. So we can push items onto the front of the linked list and we can remove items from the from the linked list. Top just returns the top elements of the linked list. Remember the data dot begin is actually an iterator so we have to dereference it and that's the first element on the list. We can assume that there's something on the list if somebody asked for the top element. We can assume that there's something on the list already so we hope that this isn't going to deref null or cause any problems there. For is empty we're just telling C++ to return the size whether the size is equal to zero and for the size which return the size. When it comes time to clear we just simply call clear on the underlying data structure. So you can see that the code doesn't really have a lot of beef to it it's very relatively easy. All we're doing is inserting elements on to the front of the list and we're removing elements from the front of the list. We know that both of those functions are constant time function so insertions on to this stack are only going to take constant time. Of course accessing the top element of the stack is only and it's a constant time in fact if we take a look very carefully the only one that's not going to be constant time is that clear function which will be linear. But we aren't too concerned about that because we know that that's not going to happen terribly often. So the code is relatively easy to understand relatively easy to go through. Take your time taking a look at it and if we wanted to use this we would understand that we just have to create a stack and call push and pop as we see fit.",
                    ""
                ]
            },
            {
                "title": "Stack \u2013 What is it Used For 1.8",
                "data": [
                    "So now that we've seen how to create the stack. How do we use the stack or what do we use the stack for? Well one of the most fundamental components of computer science of course as we see is the compiler and we're using the compiler extensively for all of our C++ work and there's a lot of other programming languages that use compilers. Compilers have a lot of places where they use the stack but one of the most fundamental places is pattern matching. So that when we have a piece of source code we want to make sure that for example all the opening parentheses correspond with all the closing parentheses and all the opening curly braces correspond with all the closing curly braces. And all the opening square brackets correspond with all the closing square brackets and it's not just a matter of counting how many openings we have and matching it to the equal number of closings. It has to be precise so that when we have an opening it matches with the closing both in number and in order so that's a really important factor. If we have an opening curly brace followed by an opening parentheses and then we have a closing curly brace before we have the closing parentheses we have an invalid syntax there because we haven't closed the parentheses before we close the curly brace. And the easiest way to deal with this is to create a stack and when we encounter an opening we can push on the opening symbol on to the stack. It's going to be stored there and remember that because this is the first in last out buffer what we're looking at on the top of the stack is the last thing that we inserted. So if we have an opening parentheses and then immediately a closing parentheses. We're going to see that the opening parentheses matches the closing parentheses. And it doesn't matter what came before it or what comes after it. We have to have that matching opening with that matching closing. So what we do is when we find an opening we push onto the stack and when we find a closing we compare it to the top of the stack and if it's a match then we can pop the top of the stack we can remove that opening element and say let's go to the next element and see that the next element matches. And the next element might be an opening or might be a closing. So the stack is used to do the pushes and the pops to make sure that the last thing that we see is the first thing that we see in the in the closing. ",
                    "",
                    "There's also a couple more examples of where we would use stacks for and that is in math operations it's very difficult to deal with infix operations using using a programming language like C++. So if we have even a simple task like two plus three times four we recognize from a math perspective that the three times four portion has to be done before the two plus three portion because that's order of operations. But the computer has a little difficult time because we're going to be scanning that from left to right. So there's a different type of notation that's called postfix notation and what we can do is take the operators the plus and the multiply and we can push them on the stack and there's a number of different rules about what you can push on top of something else you can't push a lower precedence operator on top of a higher precedence operator. And then we can order this so that two plus three times four actually turns out to be two three four times plus which makes it very easy for the next thing that we use stacks for and that's for converting a postfix notation into an actual value. We take the two the three and the four and we push those on to the stack and then when it comes time to do the multiplication we pop the last two elements off of the stack so we have the three times four so that makes it very easy to do order of operations without having to worry about infix notation. We've converted it's postfix notation. Both of those examples we use stacks in the conversion we would use the stack to store operators and in the evaluation we'd use the stack to store operands would use to store numbers and when we get done we can push the number back onto the stack and when we're completely done with the evaluation the top number on the stack which should be the only number on the stack is the actual value. So we can see that in the examples in later on in the course but you'll see the infix to postfix conversion and you'll see postfix the value of valuation and it's done with stacks.",
                    ""
                ]
            },
            {
                "title": "Stack \u2013 What is it Used For 1.9",
                "data": [
                    "So here's a piece of C++ code and I just wanted to show you how this works when we use a stack. So we're going to take the code we're going to parse it character by character. And what we're doing is we're saying if we see any parentheses or curly braces or square brackets we're going to push those onto the stack of their opening. And if they're closing we're going to compare them to the top of the stack make sure it matches. So we can see of course is this actually is an acceptable piece of code of course, we're going to take first the characters int main we're just throw them away because they're not relevant to our discussion. And we're going to take the parentheses we're going to push it onto the stack and when we see the close parentheses, you're going to discover that on the top of the stack there is an open parentheses and so that matches and the open parentheses would then be removed in the stack would be empty. We then encounter the opening curly brace and we'll push that onto the stack. Some more characters that we just completely throw away. And then we have an opening square bracket which will push on top of the opening curly brace. And then we have a closing square brackets so we throw away the openings square bracket; more characters that we throw away. There's an opening curly brace, and we have two opening curly braces on the stack some characters that we throw away, and then we have a closing curly brace. So we match it with the opening curly brace that's already on the stack and throw both of those away. We throw away all the other characters leading up to the opening square bracket, and the opening square brackets pushed on top of the opening curly brace that's already on the stack and then we have an opening parentheses, which gets pushed on top of the stack. And you'll seen at that point in the compiler there's an opening curly brace and opening square bracket and an opening parentheses. And then we have the closing parentheses so the opening parentheses comes off the stack. We have the closing square brackets so the opening square bracket comes off the stack; we throw some characters away. We have the closing curly brace and we throw away the opening curly brace. We\u2019re done with the input; the stack Is empty. This code is great. If anything doesn't match up, we can throw an error and we can actually tell the user tell the programmer roughly where the error is. For example, missing square bracket at line blah blah blah, and I'm sure you've seen that in your experience, this is the exact way that your compiler has generated that error.",
                    ""
                ]
            },
            {
                "title": "Queue \u2013 What is it 1.11",
                "data": [
                    "Now that we're comfortable with the stack, let\u2019s take a look at our second data structure which is the queue. And the queue is our FIFO data structure: it's a first in, first out data structure, which means that whatever item is first in queued, is the first item that's de-queued. And this is exactly what you have at a line, what we call in the United States we call it a line, in Europe they might call it a queue which is exactly what we're demonstrating here. when you arrive at the bank if there's a line they have to get on you're going to enter into the line and you're going to wait until it's your turn. if we used a stack for something like that the first person to arrive in the morning might not be serviced until the very end of the day, but in the queue it's much more fair. The first item that comes into the queue is the first item that goes out of the queue; so they go out in order as opposed to in reverse order. the FIFO buffer or the queue is often use for storing information temporarily and then releasing it later so called a buffer. the functions that we have you might see them called N.Q. and D.Q. and top, but a lot of people refer to him just as push and pop and top the same as they do with the stack. So just be clear of what functions you're using and what data structure you're using, because it is possible you might have some overlap between the names push and pop. Of course, the supplemental functions that we're going to have are still the same as the stack; we're still going to have the clear, we're still going to have the is empty, and we're still going to have the size. and you can see that the elements that get inserted get inserted to the back of the queue and when they get removed they get removed from the front of the queue. So it's our simple FIFO data structure for just temporary storage of information. ",
                    ""
                ]
            },
            {
                "title": "Queue \u2013 How it Works 1.12",
                "data": [
                    "So if we do this, we\u2019re going to consider that we would either use a linked list again or an array and we'll talk about the performance differences between those in just a minute. But what we'll do is we'll add to one end of the data structure and we\u2019ll remove from the other end of the data structure. So, let's say enqueue is going to add to the end of the list or of the array, and dequeue is going to remove from the front of the array. Of course, we can do that backwards doesn't matter which way we choose, it's a personal preference thing. But for the purposes of demonstration we're going to say that enqueue adds the end and dequeue removes from the front. The supplemental functions, of course, we need to write those and we need to provide the big three. And again we'll use whatever STL structures are available either the vector of the list. So we don't have to write all these functions ourselves.",
                    ""
                ]
            },
            {
                "title": "Queue Storage 1.13",
                "data": [
                    "So again we have the breakdown of whether we use a linked list or whether we use an array to make the decision on what we should use of the underlying data storage structure for our queue. so if we take a look at the list and we're breaking it down by function here. we take a look at the push function and of course the push function on the list; it's still going to be constant time. Now we can say it's going to be constant time because we're going to have pointers to both the head in the tail of the list and we're going to make sure that we can insert at either the head or the tail for us, for our purposes, we\u2019ll be inserting a tail and will be popping from the front but that's overly unimportant. the array, again, for the push operation sometimes it's going to be constant time and sometimes it's going to be linear. So the problem here is again if we run out of space in the underlying data structure of the array, We're going to have to expand the array and that takes a linear amount of time. so we have that trouble still.",
                    "",
                    "Now pop on a list, we've said it's constant time and that's just removal of a node from the list, so we're just going to have a simple delete operation. but Pop on an array is really problematic, because we're going to be pushing out to the end the bottom of the array; it means we have to remove from the front from the top of the array. and if we remove the zero element from the arraign we're going to have to move all the elements upwards by one. So ultimately the pop operation is going to take a linear time and that right there is incredibly problematic. We'll come back to that in just a minute. ",
                    "",
                    "the top operation on either of them is going to take constant time, the clear operation to get on the linked list is going to take a linear time but the array is going to take constant time and is empty in size are all going to take constant time. So if we come back, circle back to thinking about that pop operation. You can tell that on a range of a very large size we can see that there's going to be a very big problem because removing that top element from the array Is going to cause us to have to shift everything up by one element inside the array. and that's a very very costly endeavor so it's going to take us a long time to do that. there are mechanisms for dealing with it, for instance we could record where the start of the array is instead of assuming that it's at the zero position but that runs the potential of wasting space in creating a more circular array. so there are mechanisms to dealing with this but ultimately what I think we're coming to terms with is that it's easier and more efficient to store this as a linked list.",
                    ""
                ]
            },
            {
                "title": "Queue \u2013 Continued 1.14",
                "data": [
                    "So we\u2019ve come to the conclusion, I think, that we are going to use a linked list again for this data structure. And the implementation here is going to be really similar to that of a stack; we're going to have just a couple of minor changes. Now, instead of inserting into and removing from the same point, either the front of the back, we're going to be removing or inserting from different points. So enqueue is going to insert on to the end of the dequeue is going to remove from the front of the queue. We really have to make sure that the underlying data structure of the linked list has both head and tail pointers, so that way we can do those constant time insertions. If we have something simple like a singularly linked list where we only have a head pointer, then insertion is going to require us to iterate or recursively go through the entire linked list so that we can find the end and insert on to the end and that's too costly. It's far simpler to keep a tail pointer in addition to the head pointer so that we know where the end of the list is and then we can insert on to the end of the list, and we can remove from the front of the list.",
                    "",
                    "So it's very possible to use a singularly linked list in this structure. We're going to just, again, use the STL list class to make it easy. But the STL list class is a doubly linked list and of course there's overhead in that. If we want to save some memory, we could create a singularly linked list where we have the head pointer pointing to the next element, in other words the oldest element in the list, and we can have the tail pointer pointing at the last element. When we do a removal, we just advance the head pointer to the next element remove that first element and when we do an insertion we just make the tail pointer point to the new node and then we can connect the previous last node to the new node. So we have options for doing the queue as a singularly linked list. But for our purposes we're just going to use the STL list class, so that it's easier and we can demonstrate it and it has all the big three already built into it so we're not really worried.",
                    ""
                ]
            },
            {
                "title": "Queue Code 1.15",
                "data": [
                    "So here we have the code for the queue class. And again, you can see it looks very much similar to the stack class. We have the data storage as our private data members, are only private data member. It's a linked list of type T, so we have a templated and we have the enqueue and the dequeue functions, where we push on to the back of the linked list and we pop from the front of the linked list. Ehe top element just returns again, the iterator, the dereference of the iterator. And then the isEmpty, int size and the clear actually completely unchanged from the stack versus the queue. So the fundamental difference here is that the enqueue function is going to push on to the back, whereas the dequeue function is going to pop from the front. And by doing that we've changed the way that we're inserting and removing into this list and that means we go from having a stack to having a queue. First in, last out for the stack; first in, first out for the queue, and that makes all the difference in the world.",
                    ""
                ]
            },
            {
                "title": "Queue \u2013 What is it Used For 1.16",
                "data": [
                    "So now that we understand how a queue is built, what do we use it for? The fundamental way that we use queues as storage buffer, so there's going to be a lot of situations where we're bringing information in. For example you've already worked with the IO stream and, whether we're talking about a file stream or whether we're talking about keyboard input, we are talking about bringing in a large amount of data at first, storing it temporarily, and then slowly feeding it into your user program. So in the case of reading a file, because it's an expensive endeavor to actually read data from the secondary storage device, what we're instead going to do is grab a large portion of the file, perhaps all of it, store it temporarily in a structure in main memory, and then as the program asks for it it's going to be fed in slowly. Now the best structure for that of course is a first in first out buffer, or a queue. So there's one situation is as there. We might use it in operating systems, later we're going to talk about memory management, and we're going to talk about removing the oldest page of main memory for example. And how do we know which one is the oldest? Well we keep a FIFO list, a FIFO queue, of all the pages that are in main memory and when we need to throw one away we throw away the front of the queue, so there's options for that. Really anywhere in computer science where we need in an ordered list; where we have first in first out property. That's where we're going to use a queue. So there's a lot of situations that you're going to see throughout your career in computer science that you're going to use a queue and they are a very popular structure.",
                    ""
                ]
            },
            {
                "title": "Queue Image 1.17",
                "data": [
                    "So here we have just a simple sample of how we would use a queue. We have a couple of calls to the enqueuer function; we have values of twenty, fifty, and thirty. And when we insert twenty, or when we enqueue twenty, it's going to be the only thing that's on the list so it's pointers going to point to null. But when we insert fifty, the next pointer of the twenty node is going to point to fifty, meaning that twenty is still the head of the list or the head pointer still points to the node that stores the twenty. But the next pointer inside the twenty node is going to point to fifty, and so to the same happens when we have the thirty. When we start to dequeue these objects they come out in the exact same order as they went in, which is of course different from the way that the stack works. But that's because this is a first in, first out buffer, instead of the first in, last out buffer.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 19,
        "module_name": "Trees and Binary Search Trees Script",
        "file_name": "Module 19 Trees and Binary Search Trees Script.docx",
        "transcript": [
            {
                "title": "In this Module 1.2",
                "data": [
                    "We've got a lot to cover in this module. We're going to talk about trees and binary search trees. And we're going to start by talking about the definition for trees and some of the definitions of words that we use in conjunction with trees. We're going to talk about tree storage methods and how we can take this model of what a tree is and put it into main memory. We're going to talk about the design of binary search trees and what they are, and how they can help us. We're going to talk about why we need binary search trees and then we're going to talk about tree traversals. And what happens when a binary search tree doesn't meet our requirements that we need and then we're going to talk about balance binary search trees including AVL trees, and a little bit on red-black trees. So we've got a lot to cover in this module; stick with me.",
                    ""
                ]
            },
            {
                "title": "What is a Tree 1.3",
                "data": [
                    "The question that we have to ask first is what is a tree, and the answer to that is that it's a data structure designed to hold items in a hierarchical pattern. Now we talk about a hierarchical pattern and what we mean by that is that there's one node that's at the very, very top. And that there's a bunch of nodes that are down further, and some that are at the very very bottom. This is often used for searching; the tree can be used in a lot of for an index for a database for example. Or we might use a binary search tree, hence the name search tree, for searching information that we might store. ",
                    "",
                    "Every node has one and only one parent node, and that gives us that hierarchical fashion so that every node has a parent. Now of course there's one node that's a special node which is at the very, very top and it has no parent and that's called the root node. Every node will have at least some storage for pointing to children. So, every node has zero or more children; it may not actually have children but each node is going to have the capability to point to children nodes. The storage for this is actually as a pointer, just to the first node in the tree to the highest node in the tree, and that's the root. So, when we look at the class that we're going to design. It's really only going to have one pointer, very much in the way that we did a linked list is going to have one point there which is the pointer to the root of the tree and that's the highest node in the tree. It's not necessarily the first node in the tree it's not the lowest value, but we may keep it as the center; this is the highest node in the tree. We'll talk about what that looks like. ",
                    "",
                    "The great thing about trees is that every node looks like its own sub tree. So every node looks like a tree by itself. Now we know that naturally the root is the natural start of the entire tree, but each node could pretend to be a tree by itself. And that leads us to a discussion of course that going to come up again and again and that is that recursion is going to happen a lot in trees, because each node looks like a tree and there's one node that is the root. But we can treat all the nodes as if they were their own tree.",
                    ""
                ]
            },
            {
                "title": "Some Definitions 1.4",
                "data": [
                    "We need to get through some definitions first. The first one that we want to talk about is a binary tree. And a binary tree is a simple tree, remember has a parent, has some children, except the binary tree has a maximum of two children. What we usually refer to them as is the left and right child only for simplicity sake we don't have any reason to call them left or right, but we do have a maximum of two children in the binary tree; doesn't mean that we have always two children, of course some nodes will have no children and some nodes will have one child and then some nodes will have two children, but the maximum is two children.",
                    "",
                    "We can talk about the size of a tree. The size of a tree is the number of nodes that are in that tree. So if we look at the size of the root node of the tree, we're looking at the size of the entire tree. If we're looking at the size of a node that has no children, that size is just one because it's just that node. And if we're looking at a size of a node with a couple of children well then we have to do some calculations to try and figure out how many nodes are actually on this tree. And we\u2019ll see that in a later slide.",
                    "",
                    "We could talk about the height of the tree which is the distance or the number of links from the root to the farthest child. Now we could say that the height of a null, for example the height of a null so no node would be negative one, because it doesn't have any height it\u2019s actually a negative height. If we're talking about the height of a node with no children at that height would be zero because the distance from the node from the root to its far this child is actually itself, so that's a distance of zero. And if the root node has only one child and that no doesn't have any children, then the root nodes height is one and the only child's height is zero. So that's a simple way to look at height and give us an idea. We'll use that in a later slide to talk about the balance binary search trees.",
                    "",
                    "We can talk about depth which is the distance from the node to the root and that's actually the inverse of the height. So the root node, for example, has a depth always of zero the distance from the root node to itself will always be zero, of course. And if we're talking about the most sub-child than that's the actual depth that means the depth would be equal to the height of the root node; so the farthest child from the root depth is actually equal to the roots height. So they are inverse to each other.",
                    "",
                    "We could talk about a leaf node, which is a node that has no children. So the left in the right pointers for example in a binary tree would be null. And we can talk about a full node which is the maximum number of children. Obviously, if we're talking about a generic tree, where there's no maximum number of children then it's impossible to have a full node. In a binary tree, any node that has two children would be considered a full note; it can't have any additional children. So we use these definitions throughout the discussion so that we have an idea of what we're talking about. The importance here is that you take away these definitions because they're used in our language for talking about trees and binary search streets.",
                    ""
                ]
            },
            {
                "title": "Tree Storage 1.5",
                "data": [
                    "For trees that have an unlimited number of children, so not binary search trees, but trees that have an unlimited number of children. The child storage pointers have to be stored either as an irregular as a linked list. And for that we really come up with two different forms of storage that are common for this.",
                    "Now this is looking at each individual note as it exists in main memory, so in main memory if we look at the individual node what exactly does that look like. ",
                    "",
                    "The first one is a parent-multi-child and the parent node has an arrray of child nodes contained into it. Oftentimes, this is just a vector of child pointers: so, pointers to tree nodes for example. We would store in the node, we would store the data; would probably also store a parent pointer, but I haven't shown that here. We have a data section and then we just have an array or a vector of pointers to children for however many children we have. Obviously, if we have no children we don't want to use any storage any additional storage, so that's where the vector comes in handy because it keeps track of that and it'll adjust itself. ",
                    "Now the other option is that we have a parent-child-sibling structure, which more closely represents the linked list. The parent would point to the first or the favorite child, we won't get into that discussion report to the first child and the first child would point to its sibling and the sibling or point to its sibling and so on and so forth. So what we've done is we've allowed the parent to access all the sibling, all the children, by accessing from the first child and then the first child has access to the sibling and the sibling has access to a sibling and so on and so forth. So it much more closely represents that of a linked list: the child nodes\u2019 are on a linked list and the head pointer for that link list is actually the child pointer from the parent. So we can represent this using a either a parent multi child solution or parent child sibling solution.",
                    "",
                    "Now in the case where we have a value, a limited number of children, an N value as we'll call it. If we have an N value, which is the number of children that this node might have, then we can just set up the node with that number of pointers and it's not terribly difficult. So we're going to go forward and consider that we do have a limited number of children because the unlimited number of children problem doesn't often come up. The unlimited number of children problem means that each node has a variable number of children and that complexifies has the issue quite significantly, so we want to kind of avoid this. But I wanted you to see it just so that you know in case you get into trees that might need an unlimited number of children.",
                    ""
                ]
            },
            {
                "title": "Binary Search Tree 1.6",
                "data": [
                    "The most common form of tree that we're going to work with is actually a binary search tree. And I'd like to go over that and talk about it, because we are going to use it extensively especially in this module. The binary search tree as I mentioned before, only has\u2026 It's a binary tree first off, so that means it only has a left than a right pointer. And it actually has an additional property, and the additional property helps us serve the order of the nodes, helps us decide the order of the nodes. So, in other words which one goes on the left and which one goes on the right. ",
                    "",
                    "And the order property in the binary search tree says that: all the values in the left sub tree are going to be values that are less than the value of this node, and all the values in the right subtree are going to be values that are greater than the value of this node. And that means whatever the data is in the data section, and I usually use just integers just to describe it and you can see that here in this in this graphic, the value of the nodes are what the value is in the data section. And if we have this templated then of course that means we need to overload the less than operator for whatever the classes that were storing, but that's outside the scope of this discussion you can look at the previous modules and operator overloading to talk about the less than operator. But if we have that then we can keep these in a regular order. ",
                    "",
                    "So here now in this graphic what I have is module, sorry, value twenty is that the tops of the root is value twenty that was the first to know that was inserted into the tree. So, we inserted twenty and then we inserted five, now five is the value less than twenty. So, of course five is going to be on the left hand side of twenty five would be the left child of twenty. We then either inserted three maybe, or ten, or thirty, I'm not sure which the order of insertions wasn't doesn't really matter but if we insert three then three should be on the left of twenty. Unfortunately, five is already on the left to twenty so what we can do is progress down the tree to insert three as the left child of five. So what we're doing is we're looking for a place where, when we do insertion where that value should belong but there's actually a null. And then when we find that we can go ahead and insert a new node at that location. So when we go to insert ten, ten belongs on the left there are twenty but it belongs on the right of five because it is a value greater than five. If you take a look at that subtree, that three five ten sub tree, that is the left of twenty all the values in that sub tree are less than twenty. And all values in the left subtree of five are less than five and all the values in the right subtree of five are greater than five. So we can evaluate where an insertion should happen based on the comparison of the current value that we're trying to insert, with the value that's already inside the node. and if you take a look at the values on the right of twenty There's a value of thirty on the right of twenty but if it's left child is twenty-five. Twenty-five is a value between twenty and thirty, we can describe it that way. If we were going to insert one into this tree, the only place acceptable in this entire tree that we could insert one would be on the left of three. The only place settable to insert thirty-five, for example or a value any value greater than thirty, would be on the right sub child of thirty.",
                    "",
                    "So the binary search tree holds the properties that all the left's children are less than and all the right children are greater than and that happens recursively over and over again. So we have to maintain that property, not just for the tree as a whole but for each node individually. Now if we also allow equal values then it's very common to say that the right sub child will hold all the equal values but in trees it's very common to prohibit equal values inside of the tree. So you may you may recognize that we don't allow equal values. In other words, insertion of twenty five onto this tree simply wouldn't have any effect on the tree at all.",
                    ""
                ]
            },
            {
                "title": "What we need BSTs For 1.7",
                "data": [
                    "Now that we understand what a binary search tree is and how it's created, let's talk about what we need it for. The best thing that we can do with binary search tree, the benefit, the huge benefit that we get is that a binary search tree provides in best case scenario that searching the binary search tree will take Log of N time. So, we can go through a binary search tree in Log N times because what we've done is spread out the accesses, so that each level creates a doubling effect of how many nodes we can have on that level. So for example, if we take a search which would have let's say an array of a million elements. If we were to have a linear search through that on average it would take five hundred thousand accesses through the array to find it; we have to go through half of the array before we find the thing that we're looking for. But if we have it inside of a binary search tree the magic number there, the number of searches that it takes is twenty, not five hundred thousand and twenty because the log of a million is twenty. Each time we look at her node we're saying whether we should go to the left or go to the right or if we're lucky we found it inside the node, so inside the node once we make the decision to go to the left and go to the right or go to the right we've eliminated half of the possibilities. So, every iteration through this searching algorithm eliminates half of the possibilities from the from the set and that means that we have a log N problem. So searching in a right of a million elements would take five hundred thousand accesses; searching a binary search tree of a million elements takes twenty, which is a huge difference of course.",
                    "",
                    "Insertions also should take Log of N time. Deletion should take a log of N time or close to that we're going to see that it's not exactly that in a later slide but it's pretty close. So, the benefit here is that we get for large sets, we get a really significant savings in time when we use a binary search tree to keep this in order. We can use these for in order storage of any items which can be compared using a less than operators, as long as the less than operators overloaded for that class, you can go ahead and use that. for in order storage of elements the binary search tree is a really effective and efficient solution for the storage. If we don't need to keep it in order then it's not so much a concern, we can keep it in an array, but if we do need to keep water on these values than a binary search tree is really beneficial.",
                    ""
                ]
            },
            {
                "title": "BST Node Code 1.8",
                "data": [
                    "I wanted you to see the code here for the binary search tree node, and this helps us a little bit to understand what we're looking at when we see the binary search trees no order when we're talking about the binary search tree. What I've got here is just a data element and it looks very much like we had with the linked lists. There's just a data element and I have a templated, so it's a template type T. And then we have a pointer to the parent, pointer to the left sub-child or pointer to the rights sub-child, and that's it. That's all the storage that the entire BST node class has. I had a constructor in the same way that I have a constructor for the linked list; it's just useful for setting things up and it makes creation of nodes a little bit easier later on. I have a friend just like I did in the linked list, so the very common, a very much common with the linked list here except that we've got two pointers, a left and right child.",
                    "We're going to get into this getSize class, in just a minute of the getSize function in just a minute, what it's going to do is just tell us how many nodes there are as children and this node for the node. So, we're talking about the size really that's by definition if you remember back from the definitions we did a couple of slides ago, this is just the size of this individual node. So this is the format for the BST node class and you're going to see a lot of commonalities with the linked list.",
                    ""
                ]
            },
            {
                "title": "Recursion in Trees 1.9",
                "data": [
                    "I mentioned a couple of slides ago that recursion is going to be popular in trees and believe me it is. I wanted to give you an example of that here with that get size function inside the BST node class. So here what we're saying is, we\u2019ll start out since we are in a node already we can assume that there is one value the size at least one, so we\u2019ll start that counter at one. And then all going to do is just check to see if this node has a left: if it does then add the size of the left, if it has a right then add the size of the right and then return count. And if you take a look at that count, plus equals left arrow get size, that's a recursive function call to a different node. So, what we're doing is checking to see if that node exists first, because we don't want to de-ref null of course, if that no does exist then we'll call its get size function and we'll add its size to our size. So if we have any children, we can assume that our size is the sum of the child sizes plus one. ",
                    "",
                    "And that's really all there is to recursion in trees, trying to do this with iteration is really, really hard because you have to keep track of what level inside the tree you're on what depth you're at and then move backwards and forwards. Remember if, for example, there's no\u2026 There's no association; there's no pointers between a child node and its cousin node. So, if we have to go back up to an uncle node which would be the parents sibling node, that takes quite an effort and even worse if this these cousins are separated significantly by more and more uncles and aunts in the family tree that looks really crazy. But with recursion it's rather simple because we can just call the recursive function to take a pointer to the child nodes, or in this case we embed it inside the node class and we can use the getSize function for the child nodes inside the parents get size function. So, recursion is going to happen quite often in trees.",
                    ""
                ]
            },
            {
                "title": "Tree Traversals 1.10",
                "data": [
                    "When we're working with binary search trees, there are times that we want to process every node in the tree. And the way that we do that, and not by searching I mean we actually want to go into each node in the tree and do some activity\u2026 If we're going to do that we have a number of ways that we can do that and the order of it really depends on how we want to go through the tree. Now if we're going to do the same thing to every node really doesn't matter, we can just do it using any of these orders, but if we care about the way that the processing happens. So, for search for example on a file system, which is designed as a tree, on a search on a file system we probably want to search the higher stuff first and search the later stuff later. That's called a level order traversal, but we may want to do different types of traversals which are: in-order, pre-order, or post order depending on when we want to search this note or when we want to process I'll call it, this node. ",
                    "",
                    "So what we're asking here is if we look at it in a recursive fashion, when do we want to search the root node. So, we start the process does that mean that we search the root node first or does that mean we search the leftmost node, the minimum node first. So, in a binary search tree really in any binary tree, we have to decide what the order of traversal is.",
                    "",
                    "So, in-order traversal processes the left nodes recursively first and then it processes the so-called this node and then a processes the right nodes. So, the pre-order traversal would just be that we do this node first and then the left and then the right. And the post-order would be that we do the left and then the right and then the this; so it really decides when we process the this node. Level order traversal processes it based on depth and this might also be known as a breadth first search, it\u2019s actually an interesting problem but what we're doing is we're looking at the tree and going out word on the tree. So we're not going down one side and then coming back and doing the other side we're going outward on the levels of the tree. And we'll see the code for this and it will become a lot clearer in the next slides.",
                    ""
                ]
            },
            {
                "title": "Implementation 1.11",
                "data": [
                    "So here I want to show you the code for the in-order, the pre-order, and the post-order traversals in the tree. And what you can see is that we've done, we're taking in a pointer to a node. That may not be the best solution to, in fact we might have a driver function which calls this for the root, but we do need to do this recursively so it will take in a node pointer to the node that we care about. And what you can see with these three pieces of code is that in reality they are the same; the only difference is where we process. And in this case it's just an output statement where we process the current node, the node\u2019s data section. So, in the in-order traversal it\u2019s processed in between the left and the right processing. In the pre-order traversal, it's processes the head of the left and right processing. And the post-order traversal it's done after. And since this is a recursive function that really is going to consequently change the output of the tree based on how we want to go through each individual node.",
                    ""
                ]
            },
            {
                "title": "Level Order Traversal 1.12",
                "data": [
                    "The level order traversal gets even more complex and I'm going to tie this into the previous module that we did which was talking about stacks and queue's. And here we're using a queue to store the binary search tree pointers, the node pointers. So what we're doing is we start off with the level order traversal by pushing the root node pointer so we push the root node pointer on to the tree, onto the queue excuse me, and then as long as the queue is not empty what we do is look at the front node look at the top of the of the queue. And then pop that off process that node and then push the left and push the right as long as there are not know. What that does is process the root node first of course but then a process is the left child. And if the left child of the root has any children those get pushed onto the queue after the right child of the root. So what we're doing is processing the tree from the top down and outwards. Which is known as a breadth first search, we're going to breadth of the tree first before we go to the next level. So, you can see that this level order traversal tends to operate on the higher nodes the nodes with the less depth first. And it's just another way of going through the same tree and it might help us if we're searching a very, very large tree or for doing some operation on a very, very large tree; it might help us get the nodes that we care about most done first.",
                    ""
                ]
            },
            {
                "title": "Traversal Results 1.13",
                "data": [
                    "I wanted to show you what the traversal results look like, so I\u2019ll come back to that same tree that we came that we showed you earlier with the twenty at the root node. And what I want you to see is that the pre-order, in-order, post-order and level order traversals really produce very, very different outputs. And so what we're doing is we're saying that the in-order traversal would actually process all the left nodes first and then the node, and then the right node first. The pre-order traversal tends to process the root node first and then it processes the left and processes the right recursively, and that's the important characteristics here. ",
                    "",
                    "So, take a look at the pre-order traversal; it means we process twenty first of course because we're doing the pre-processing. So we process twenty first, and then recursively process the left sub-child which means a five comes out next, but that means recursively we have to do the left sub-child of five before we can go back and do anything else. Three comes out and because three doesn't have any left child or right child, we're done and we move back to five in which case we have to process the right sub-child of five. We only process the left; it's now time to process the right. So, if we if you take a look back at the code after we're done processing the left child of five, we're going to have to process the right child of five, which is ten and then we're done because it has no left and no right child. When we come back the five, we're done with five, we go back to twenty and we still have to process the right children. And so that's the way that the pre-order traversal is going to operate. The output would be twenty-five, three, ten, thirty, and twenty-five because we're doing it pre-order.",
                    "",
                    "In-order; you can see that this actually is in order so it's sorted if you will. We're processing all the left children first; when we go to process the left child of three since it has no left child we can simply process the node at three and then process its right child. It has no right child, so we process three first. In other words, the minimum value is the value processed first which makes sense. Likewise, the maximum value is the value that's processed last. So, the output from the in-order traversal would be three, five, ten, twenty, twenty-five and thirty. Just simply because we're going through this in order and since it is a binary search tree it's kept in order. ",
                    "",
                    "The level-order is the most interesting one here. Which is that we do twenty first and then if you look at the queue, when we push twenty we're then going to push five and thirty, which are the nodes on the left and right child of twenty. When we're done processing twenty and we start processing five, we'll push three and ten but three and ten are actually after thirty on the queue. So, if you look internally at the queue, what we're going to see is three, thirty, three, ten, and then it's not until we're finished with ten and we process twenty-five that were done with the entire tree. So the level-order traversal would be twenty, five, thirty, three, ten, and then twenty-five and that is exactly in the order of the levels if you look at the picture.",
                    ""
                ]
            },
            {
                "title": "Insertion into Trees 1.14",
                "data": [
                    "Insertion into a tree is not terribly difficult. The overview is kind of that we check to see if the tree is empty and if it is of course, this is just the first note on the tree will create it that way. If it's not then we have to go about and finding the insertion point. So, what I've done here is given you the code, the first one is just the first thing is just an if statement to check to see if the tree is empty and if it is just pushing it on to the tree. If the trees down empty it's time to find the insertion point and I do that by using my two pointers temp previous. And again, this should look very similar to what we did with linked lists, in that eventually temp will become null; temp will be the null pointer. And we're going to keep previous as one node back from where we were. when temp falls off into null, we can look at previous and decide if we're going to push on to the left or the right sub-child. Now inside that while loop what we're doing is comparing the item that we're going to try to insert with the data that's inside the node. And in doing so we can decide if we should go to the left or we should go to the right, based on whether or not this is less than. So if we go to the left and temp becomes null, than the previous node is the node we have to update to point to the new node that we create. And then that last if statement there just does that same check again to decide whether this should be a left child of the node or whether it should be a right child of the node based on whether it's less than an order greater than the node. ",
                    "",
                    "There's no real recursion in this in this algorithm; all it is, is a loop to find the null. And then once we found the know all to go to the node immediately before that we saw immediately before that an add the node on as a left child or right child. So as not it's not terribly difficult code; It does involve the little bit of complexity of pointers. But if we keep track of where we were just a moment ago using the previous pointer, then when the temp pointer goes know all we know where the nodes should be connected to.",
                    ""
                ]
            },
            {
                "title": "Removal From a Tree 1.15",
                "data": [
                    "So if we look at removing from a tree element; insertion is easy, removal does require a little bit more work. So removing from a tree is easy if the node is empty. If the node is a leaf node, then just remove it and update the parent to point to the null pointer. So there's not much work that has to be done if the node is a leaf node. If the node has just one child then that's not difficult either, because we can promote that child and the child will match the characteristics of the parent. If, for example, it's a left child then the left child is going to be promoted; it's going to have a value less than the parent but the parent relationship to the grandparent will not change and so that should all still work out well. ",
                    "",
                    "The problem comes when the node has two children, and we have to choose what's called a candidate replacement. So now we're not going to accept the fact that we can promote; we can't promote a child to the parent position because what if that child has children of its own. We can't just move that child up; what do we do with its children and what do we do with the other child of the grandparents. What we're going to have to do is decide whether we choose the maximum of the right sub-child so in other words go right and then go left, left, left, left, left, left, left, left, and we'll end up with is the maximum value of the\u2026 Sorry the minimum value of the right subtree. Or what we can do if we're talking about going to the left is we can go to the left, and then right, right, right, right, right, or we can go to the right and left, left, left, left.",
                    "",
                    "Either way that we decide to go, what we're choosing is either the previous node from the node that we're going to delete or the next node from the node that we're going to delete in order traversal. So, if we look at this traversal in order, it would either be the node immediately before which would be the minimum of the left-most, sorry the minimum of the right subtree or sorry the maximum of the left subtree or would be the minimum of the right subtree. In other words, we're going to delete a node and let's choose to promote, the one of the nodes that are either the on the left or are on the right.",
                    "So, one of the nodes adjacent to the node that we're going to be deleting in value, is going to be promoted to the current node. So, that's the way that we're going to handle this; we're either going to go to the left and then go right, right, right, right, right, to find the find the maximum node. Or we're going to go to the right and then go left, left, left, left, left, to find the minimum node. So, the point is that we can actually process this by promoting one of the children and then deleting that child instead of deleting the actual node that we're going to trying to leave. We're going to see that in just a just a minute.",
                    ""
                ]
            },
            {
                "title": "Removal, Given the Node, No Children 1.16",
                "data": [
                    "Here's the code for deleting a node if it has no children. And what we're given is a pointer to this node, actually it's a reference to a pointer to this node. The reason for that is that we're going to be changing the pointer; we actually make a change so that temp pointer. So, what we do is to say if temps left is equal to the null pointer and temps right is equal to the null pointer: it just it has no children. So, there's no there's no work that really has to be done here. We\u2019ll find out if the parent is the null pointer, if it is then this is the root node of the tree; it has no children if the last note on the tree were done. So, we can just set root equal to null pointer. So, if we\u2019re removing the last node in the tree just remove the node. And then of course will have to delete it later.",
                    "",
                    "And then we'll check to see if the parents left is equal to this node. So, if the parents left is equal to the node that we're looking at, so we go up a level and then we go back to the left then we're going to have to set the parents left pointer equal to the null pointer. Remember this node has no children at all; so if the parents left is this node that we're looking at then let's delete the parents left. If the parent's right, obviously if the parent if it's not the parents left in the parent exists then it's this must be the parent's right pointer. So, if the parent's right pointer is this pointer, then we update that parent's right pointer to get rid of this node and then we actually get rid of the node. So, in the no children case this is actually pretty simple. We basically just update the parent\u2019s pointer so that it points to null and then delete the node.",
                    ""
                ]
            },
            {
                "title": "Removal, Given the Node, One Child 1.17",
                "data": [
                    "If we have one child, then what we want to do is promote that one child's data value and delete the child node. So, this assumes that we've already done the check that we did in the previous slide to see if both children don't exist. If we don't have any children then we're going to take care of it with the previous slide\u2019s code but now we recognize that there is one child in either; we have a left child or we have a right child. And what we're going to do is we're going to take a look and say if temps left is equal to a null pointer, so we don't have a left child that means by default we must have a right child. And the reason for that is that if we didn't we would have gotten caught in the if statement up above from the previous slide. ",
                    "",
                    "So, we're assuming now that we have this right child and we're going to want to do is we're going to want to get it to delete pointer. This is just another pointer to say that this is the node that we're going to delete. And what we can do is promote the data value up to the to the parent so now we copy to delete data up to temps\u2019 data, so we\u2019re copying the value from that right node up into the parent node. And then we'll copy the pointers up into the parent node, and then what we can do is make the pointer from our parent point to the new node that we're going to delete. And then we can delete the two delete values, so in other words we're going to copy the value up and then we're going to delete the child node that does exist. So that makes it a little bit easier that we don't necessarily have to update the parent pointers; all we have to do is make the data values bring those pointers up to the to the object that we're working in right now. So the pointer for temp is actually going to remain exactly where it is; the node temp is remain exactly where it is. And what we're going to do is move all the pointers and the data values from the child nodes up to the new temp. We do have to take into account if the temps left pointer exists, that its parent now is going to change. So, if we have a node on the left of the to delete, we're going to have to change that to deletes parent pointer to deletes left parent pointer to the appropriate new value. So, keeping in mind that we're not actually going to delete the node that we are trying to remove, we're actually just going to promote the value and delete the node that's to the right, and this case is to the right.",
                    "",
                    "We're going to delete the node to the right and promoted it\u2019s data value. If we're deciding that we're going to go to the left then it's the exact same thing just dealing with the left pointers instead of the right pointers and keeping all that in mind. So it doesn't really require a lot of different code it's really just flipping of the left versus the right. So it's just almost a copy of the same thing. So that's if we have one child as opposed to no children. The problem with two children is well, bigger and we're going to see that in just a minute.",
                    ""
                ]
            },
            {
                "title": "Removal, Given the Node, Two Children 1.18",
                "data": [
                    "So here we are in the else condition. And in the else condition we've already checked to see if we have no children or if the left child is equal to null, or if the right child is equal to null. So, we've already decided that what we ended up with is that we have two children of the Node that we're trying to delete. And that's not going to be the easiest solution, again we're going to either have to decide to find the minimum of the right subtree or the maximum of the left subtree. So, we have to make a decision and this is just a programmer's decision; it's entirely personal preference. Now for me it's easiest to find the minimum value of the right subtree; so, to go right and then go left, left, left, left. And ultimately what I'm going to decide to do is do that, go right and then go left, left, left, left, left, and take that last value that I find, when the left's left pointer, when the the items left pointer is equal to null that's the minimum value of that right subtree. And what I\u2019m gonna do is copy the data value up and then delete that node. ",
                    "",
                    "So this is recursion; this is a recursive algorithm that simply says once we found the data value that we want to delete which is not the data value that somebody told us to delete. We are going to find the minimum value of the right subtree, we\u2019re to go right, left, left, left, left find value and promote that value but then we're going to have duplicates. And so what we do is remove that value that's down further in the tree. Now here, I've guaranteed that it's not infinite recursion. And the way that I'm guaranteed that is by making sure that the minimum of the right subtree is the one that I'm using. And if I'm using the minimum of the right subtree it can't have a left child. It cannot have a left child because if it did that would be the minimum of the the right subtree. ",
                    "",
                    "So here we've proven that we can use recursion, but we're only going to go recursion on one level further. Because what we're actually going to be doing is removing the minimum of the right subtree, which means that by definition it can only have it at most one child, hopefully it has zero. But even if it does have one we solve that in a previous example and we don't have to worry about infinite recursion from that sense. So, in fact what seemed to be a very complex problem of removal of the node given that it has two children, actually turns out pretty easy because we're just going to deal with deleting some other node and promoting its data value.",
                    ""
                ]
            },
            {
                "title": "When BSTs Fail 1.19",
                "data": [
                    "We know that in a binary search tree, best case scenario, the binary search tree is going to result in big O of Log of N time for everything. So insertion, searches, removals, all that's going to take a Log N time. Unfortunately, where BST's fail is if the insertions that we're doing are already in order. And if they are, then all the insertions that we do are going to result in right hand side operations only. So we're going to insert five and then we're going to insert ten as the right sub child of five, and then we're going to insert fifteen as the right sub child of ten, and then going to insert twenty is the right sub child of fifteen and then we are insert twenty five as the rights of child of twenty. And what we end up with is all right children and nobody on the left; the left is all equal to null. And so what we end up with really is a linked list and we know that searching a linked list is linear time, not Log N, linear. So going back to our previous example of one million elements now we've got to search five hundred thousand elements because there's no left elements at all; they're all right elements. And so this doesn't really work out very well in the, in the easy binary search tree solution because the operations that we're doing are a little bit too simplistic. What we really need to make sure is that we're not constantly using the right subtree. That we actually end up using those left pointers to some extent and that we have a wider breadth than we do depth, or at least equal breadth and depth.",
                    ""
                ]
            },
            {
                "title": "Balanced Binary Search Trees 1.20",
                "data": [
                    "The solution of course is a balanced binary search tree. And the balanced binary search tree guarantees that we use Log N or that we have Log N search time. To do that we need to do some additional work and insertions and removals. The d binary search tree does protect the big O Log N insertion time. Unfortunately as a result, insertions and removals do take a little bit longer, but they're still going to be close to Log N. So, we're not changing this to a linear insertion like we would have with a linked list or within array; god help us. But it does take longer than big theta Log N so it's still going to be big O of Log of N, but it's actually only something closer to like two Log of N so the theta to does change but the big O pretty much stays the same.",
                    "",
                    "There's two types of very popular balance binary search trees. There's one that's really easy to understand but the performance isn't all that great, called an AVL tree, and then there's one that's really hard to understand the but the performance is fabulous and that's called a red-black tree. If you were create a tree, if you were to use a tree inside STL. So, the STL actually does have two trees: one is called a set and the other is called a map. And of course, there are multi-multi-mapped trees in case we're interested in that. But the STL set and the STL map are actually implemented as red-black trees. So here we actually have something that exists in real life or in our real code life at least that we're that we're studying at this point.",
                    ""
                ]
            },
            {
                "title": "AVL Trees 1.21",
                "data": [
                    "AVL trees are named for their creators and it's two guys named Adelson Belsky and Landis. What they do is they record a height for each node of the tree and it's a balanced binary search tree because the AVL tree puts in a stipulation that the height of the left and right subtrees can differ by no more than one. So the heights, by definition, can differ by no more than one and that means that we never get to the point where we end up with a linked list where everything is always going to the right or even worse everything's going to the left, but the point is that the heights are restricted. Now one thing that the AVL tree does, or actually the point of the AVL tree is that when we determine that the heights differ by more than one then it's time for a rotation to rebalance the subtree. And we're going to talk about rotations in just a minute, but the heights of the trees should cause a rotation to happen. If they differ by more than one, we're going to have to do a rotation to reproach to choose a better route of that subtree so that we can balance out the tree into.",
                    ""
                ]
            },
            {
                "title": "Good AVL Trees 1.22",
                "data": [
                    "Here's an example of a good AVL tree. So what we did was we did some insertions, like we inserted the value of ten and we inserted five and twenty and three, and those all got inserted onto the tree. What we have here is that ten has a height of two. Now how we calculated that was by looking at the height of the left sub child and the height of the rights sub child, and determining that the maximum of those two heights is one and then adding one to that. So that's why the height of the node at three, for example, is a zero because the height of the left sub child is a negative one and the height of the right sub child is a negative one and we can add one to that, and add one to the maximum of that which is the same value, and we get zero. ",
                    "",
                    "So what we can do is say that this is a good AVL tree because the height of all of the nodes are, the height of all the left sub child for every node in the height of all the right sub child for every node, is equal to a value that differing by only no more than one. So that the largest difference here is actually if we get to the root node, if we take a look at the root node, the height of the left sub child five is one and the height of the right sub child is zero, the value twenty. We can also look at that node five which has a height of the left sub child of zero and the height of the right sub child as negative one. So, that's a difference of only, of only one. So, in no case do we differ from the left sub child to the right sub child by more than one and so this is an acceptable AVL tree.",
                    ""
                ]
            },
            {
                "title": "Bad AVL Trees 1.23",
                "data": [
                    "Here's an example of a really bad AVL tree, or rather I should say one that doesn't meet the AVL properties, right. We call it a bad AVL tree but it just doesn't meet the AVL property. So, what happened was we inserted ten, we inserted five, we inserted three, which sort of looks like we had it before except the ten\u2019s left child is five and five\u2019s left child is three. Now the height of three is zero, which means the height of its left child is negative one, the head of its right child is negative one; that's fine. They differ by no more than one. The height of the node at five is one; the height of its left sub child is zero, the height of its right sub child is a negative one. So that differs by no more than one. ",
                    "",
                    "Where the problem comes as at that root node ten there, which is a height of two. And the height of two; the left sub child, five, has a height of one but the right sub child has a height of negative one. So the difference there is two and now we've got a problem. And we can see this starting to form the same classic problem where we had the right, right, right, right, right problem from the previous slide, where all the nodes are being inserted as greater values. Here the nodes are all being inserted lower values, lesser values, so it's all going to the left. Regardless, if we let this propagate this is going to end up as a linked list, we're going to have the really poor insertion and searching times that we had with linked lists. ",
                    "",
                    "Another example of a bad AVL tree here. We can see that we did, it looks like the insertion of ten, five, three, and twenty and that all was fine; everything was okay from the same as the previous example. Except now we take the added component of adding four, so here we added four onto the right sub child of three. And what happened was the node at four is balanced, left sub child is negative one, right sub child is negative one. The height at three is balanced; left sub child is negative one, right sub child zero. The height at five, well that one we've got a bit of an issue here because the height of the node five, the left sub child is one and the right sub child is a negative one. So, that causes us a little bit of heartache we're going to have to deal with that. Unfortunately, we have to deal with these as two different problems, because the solutions that we use are different based on what whether or not, well we'll see them in a later slide, but they're based on a little bit of difference in how we did the last  insertion. Either way these trees do not meet the AVL property. So, we do have to do some work to get them to balance again.",
                    ""
                ]
            },
            {
                "title": "Rotation Solutions 1.24",
                "data": [
                    "Let's look at a problem where we have a grandparent, parent, child kind of situation that's unbalanced. So, we have a grandparent node which has a child of a parent node, and the parent node has a child of the child node. So, we have that sort of situation and it doesn't really matter whether we're doing insertions on the left or insertions on the right, but the grandparent parent child situation is set up here so that we have an unbalanced situation. If the left parent\u2019s left sub child is greater than the left parent\u2019s right sub child, or if we have the heights what we're talking about is the height, or we have the right parent\u2019s right sub child is greater than the right parents\u2019 left sub child. Or what I like to call it is either an outside-outside insertion versus an outside-inside insertion; outside-inside is a different problem, outside-outside. ",
                    "",
                    "So if we go right right, that's one condition; if we go left left, that's one condition and that's solved by a single rotation. And what we're talking about with the single rotation is taking the parents and making it the new grandparent position. So, in the case that I have described here, we've got a node twenty and it's left child is ten and the left child of ten is three and what we've got is we're going to promote ten. Now when we promote ten, it's like picking up the node ten and the rest of the node sort of drop like Christmas balls. So,if we pick up ten, the twenty node converts (turns into) the right node of ten; the twenty node becomes the right note of ten.",
                    "",
                    "So the reason that that works, picking it up and then having ten\u2019s right node become twenty is because in this condition, ten doesn't have a right child. What would happen if ten did have a right child? Well if ten didn't have a right child, then the left child of twenty would become empty; it would become null pointer. Because we're picking up then the left child of twenty is no longer important, because that was the pointer that pointed to ten, but we don't need that anymore. So, the perfect solution is to make the left child of twenty, actually point to the right child of ten. ",
                    "",
                    "And if we do that remembering the fact that all the nodes to that are greater than ten would be on the right of ten. Then it's important to recognize that this still holds the binary search tree property, because when we pick up ten and its right pointer now points to twenty; that's a value greater than. And remembering that all the values that were greater than ten were on the right side of ten. So, if we make the left pointer of twenty point to the values that are greater than ten (were greater than ten before) then we still have that binary search tree property maintained and everything still works out the way that we wanted to. ",
                    "",
                    "So, this is what's known as a single rotation, in this case actually it's called a single clockwise rotation because we're kind of going clockwise. and what we can do is recognize that this works. Now it only works in the case of an outside-outside, so if the heavier node that the node with the higher height, the bigger height, the greater height is right right or left the left this work. But if we have a problem, we do have a problem, if we're going left rights or if we're going right left and that's the higher node. So, we're going to see that as another solution in the next slide.",
                    ""
                ]
            },
            {
                "title": "Double Rotation 1.25",
                "data": [
                    "What happens when we do have that outside-inside insertion? And in either way whether we're talking about left right or whether we're talking about right left, what we're saying is that the heavier node is a different child than what we decided on the parent. So when we're looking at the grandparent node, in this case we have twenty: the left child of twenty is ten, the right child of ten is fifteen. We've got is a problem in which a single rotation would not solve this. What we really need to do is reorganize these as a double rotation. And a double rotation is actually pretty simple because all we need to do is first do a counter clockwise rotation around ten, so that we end up in the same situation.",
                    "",
                    "So first we'll do a counter clockwise rotation about ten. Which means the twenty's left sub child will be fifteen, and then fifteen's left sub child will be ten. So, initially we sort of make the problem not go away but we put the problem in a solution that we can solve because we're going to generate an outside-outside problem. So initially the single rotation would not solve this at all. If we only used a single rotation we would still have the problem of, even if we promoted ten to be the root its right sub tree would be twenty and the left sub child of twenty would be fifteen. So that's doesn\u2019t solve the problem but if we do a single rotation about the parent, not about the grandparent but about the parent, if do the single rotation about the parent first. Then what we end up with is a problem that we can solve.",
                    "So we end up with twenty; its left child being fifteen, fifteen\u2019s left child being ten and that's just a single rotation that we can solve. The end point is that we have fifteen as the root; its left some child is ten and its rights of child is twenty. ",
                    "",
                    "And that's the way that double rotation is done. So in fact if you look at the code, a double rotation is actually done by two calls to the single rotation function: once clockwise and the others counter-clockwise so there's not really much work that has to be done there. But the double rotation problem does exist and we do have to recognize it.",
                    ""
                ]
            },
            {
                "title": "Red-Black Trees 1.26",
                "data": [
                    "Red-black trees are designed to try and solve the problem of the balanced binary search tree. One of the issues that comes up with AVL trees that we kind of glossed over Is the need to calculate those heights. And unfortunately, whenever we do an insertion, the heights are going to change. Whenever we do a removal, the heights are going to change. Whenever we rebalance, the heights are really going to change, so if we do a rotation, really going to change. And unfortunately, that means that we're going to have to calculate those heights which takes Log of N time, so really we're adding Log N time on top of Log N time and that's two Log N for big Theta. It's not terrible, by no means, I mean it would be much worse to put this into a linked list but we can do a little bit better. ",
                    "",
                    "So the red-black tree aims to avoid the problem of having to go back and recalculate heights every time. And the way that it does that is by four laws and the four laws are kind of cryptic and really doesn't make sense until you see it. So, we're going to see it in a later slide, but let's go over the laws and just get a good idea. ",
                    "",
                    "So, the basis of the laws is that all nodes\u2026 First law, one, right. All nodes are colored either red or black. Well that's not hard because all we have to do is put a Boolean in there, in each node, to say it's either red or black. It's just a Boolean value so that's very simple. The root is always black, and for that we just say if we ever find that the root is red we'll just recolor it to black; it has no impact on anything. So law two is very easy to comply with; if the root is ever turned red, we just make it black again. Here's where life gets a little bit tougher, but it's good to keep in mind that the purpose of these laws is to determine when a rotation is going to be necessary. So, what we'd like to do is try and determine when the rotation is necessary. Keep that in mind as we go through laws three and four. Law three says that a red node cannot under any circumstances have a red child. Now, don't read any more into it than just that, because it doesn't say that a black node can't have a red child and it does not say that a black node can't have a black child. All it says is that if we have a red nose, we cannot have a red child. So that's the only restriction there. What happens if we try to insert a red node onto a red child well then we're going to recognize that it needs to do a rotation or we need to do a rebalance or something is going to have to change. Perhaps the color of the parent node is simple enough that we can just recolor it to black; maybe it's the root. But we'll take care of that as we go through more and we look at the actual code for this. The core of this is that law three: a red node can never have a red child. ",
                    "",
                    "The really hard one to maintain is that all paths from the root to all children have to pass through the same number of black nodes. And this is where life gets really, really confusing because if we're going through a node we need to count it. Now, how do we go about recognizing this without going through all the paths. It's important to keep in mind that the purpose of the laws is only to recognize when a rotation is necessary. So, what we do is we'll say that we start off with one node, the root, and the root is always black; there's two nulls, one on the left than one on the right of the root node. So if we go through and find the left null, we've gone through one node, if we go through and find the right null that we've gone through one black node and that's no problem. ",
                    "",
                    "Insertions into a red black tree are going to be red nodes. So whenever we do an insertion of a new node. We'll color the new node red. That's not a law, that's just an implementation factor. So when we implement a new node we're going to insert it as a red note. So, if we had a value, for example of twenty and we inserted ten is the left sub child Then the left sub child will have two nulls and that changes the number of paths but it doesn't change the number of black nodes that we go through. We're still only going through the one black node at the root. So that's why this makes this makes some semblance of sense.",
                    "",
                    "Now you're going to ask yourself what happens if we need to insert another node onto the left some child of ten. And the answer that is well it would be a red node of a red child and all that means that we're violating a law three, and that we need to recognize that a rotation is necessary. So, we've got these sort of solutions that we can use to not violate the laws. And we\u2019ll talk about that later in the next slide or in a later slide. But what we're going to say is that these are the laws of red black trees and we need to recognize and maintain them; how we do that is a much bigger problem.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 20,
        "module_name": "Computer Architecture Part 1",
        "file_name": "Module 20 Computer Architecture Part 1.docx",
        "transcript": [
            {
                "title": "Computer Organization Introduction 1.2",
                "data": [
                    "Hi. My name is Jerry and I will be your instructor for the Computer Architecture and Organization Section of the CS-Bridge Program. So far, we have covered several paradigms and techniques of computer programming. And these techniques have enabled us to write software code for different purposes. So in this section, we will cover how the CPU interprets this code and executes it. Then we will discuss what is inside the CPU that allows it to execute such code. ",
                    "",
                    "More specifically, we\u2019ll cover the assembly language and mnemonic machine code, which are human-readable representations of machine-level instructions, we will go over the CPU execution steps, and we will detail the microarchitecture of the CPU.",
                    ""
                ]
            },
            {
                "title": "A Little Bit of Background 1.3",
                "data": [
                    "So, let\u2019s cover a little bit of the background on computer organization.  This is just for us to get a flavor of what\u2019s inside of a CPU so we can get a better feel for how it executes the code that we write in a language like C or C++. So, at a high-level, there are three main hardware components inside of a CPU: the functional units, the registers, and the memory hierarchy.",
                    "",
                    "The functional units are hardware components that perform logic, arithmetic, and other operations. Examples of functional units we may find are: the Arithmetic Logic Unit , or ALU, which performs integer addition, subtraction and logic operations such as AND, OR, XOR, etc; the floating Point Unit, or FPU, which performs the same operations as the ALU, but for floating point data; the integer multiplication unit, or IMUL, which  performs integer multiplication and division; then there is the floating point multiplication unit, of FMUL, which performs floating point multiplications and divisions.",
                    "",
                    "The General purpose registers or GPRS, are components inside the CPU that hold the data for the functional units. So every data that a functional unit operates on must first be located in a general purpose register. Similarly, once a functional unit finishes an operation, it puts the result back in a general purpose register.",
                    "",
                    "The other important component in the CPU is the memory hierarchy. This hierarchy includes the hard disk, your main memory or RAM, and several other storage components known as caches. The memory hierarchy stores both the software and the data of a given program and different levels of the hierarchy are used for different purposes. For example, the hard disk is used for permanent storage, the RAM and caches are used for temporary storage. We will cover specific aspects of the memory hierarchy towards the end of the computer architecture section",
                    "",
                    "So, let us go through a simple example to illustrate how these three main components work in concert.",
                    "Let\u2019s say we have a simple program, written in C or C++, such as the one we have next to the Memory block. The crux of the code is to perform the addition B+C and to put the result back in memory location of A. Since B and C are integer values 3 and 4 respectively, the ALU functional unit will be used. Remember that each data used by a functional unit must first be available in a general purpose register. So the first operations of the CPU are to move the data for B and C to general purpose registers. So, for the first operation to implement A = B+C is to copy the value of B into a general purpose register. The second operation copies the value of C into another general purpose register. At this point, both values are in registers so the ALU can use them to do the addition. As we see here, in the operation, the ALU performs the addition of 3+4 and puts the result back into a general purpose register. The last operation for this code is to copy the value of the result register back into the memory location for A. So in total, we have four operation to perform A = B+C. The first two operations copied the values of B and C into the registers, because the functional units only operate on data that are in those general purpose registers. The next operation performs the addition and puts the result into a general purpose register, and the last operation copies the value of the result from its register to memory. Later in this section, we will cover this example in more intricate details",
                    ""
                ]
            },
            {
                "title": "A Little Bit of Background 1.4",
                "data": [
                    "So, let\u2019s quickly go over two types of CPU architectures in the market. One is called Reduced Instruction Set Computing or RISC, and the other is called Complex Instruction Set Computing, or CISC. The decision of which of those architectures to use impacts how the machine-level instructions of the CPU work, how the hardware and data path of the CPU are implemented, and how the compiler for that CPU works",
                    ""
                ]
            },
            {
                "title": "A Little Bit of Background 1.5",
                "data": [
                    "Let us first discuss the RISC architecture. The basic idea of RISC is to have simple instructions that perform simple operations. Let us recall the A = B+C function from before. In a RISC processor, each of the four operations needed to implement this function can be implemented as a machine-level instruction. So that is four RISC machine-level instructions for one high-level C or C++ line of code. So there would be an instruction to load B to register, and instruction load C to register,  an instruction to add registers for B and C and put the result back in a register, and an instruction to store the result back to memory allocated for A.",
                    "",
                    "The benefit of having instructions that perform simple operations is that they make it easy to implement the CPU at the hardware level. So the CPU hardware is less complex and incurs less power and space in your computer. The main downside is that it is more difficult to translate the high-level code into machine-level instructions. This makes it more difficult for the compilers. The most popular RISC architecture used in the market is ARM, which is commonly found in smartphones, tablets, and low-battery platforms",
                    ""
                ]
            },
            {
                "title": "A Little Bit of Background 1.6",
                "data": [
                    "CISC architecture does the reverse of RISC. That is, it has very complex machine level instructions, where an instruction can perform multiple high-level operations. For example, an instruction can do both a memory access operation and an addition operation.",
                    "The main benefit of this is that it makes it easier to translate high-level code, such as C or C++ into machine instructions, simplifying the compiler. The downside is that the hardware of the CPU is more complex because of the multiple operations that an instruction can perform. The most popular CISC architecture is x86 from Intel.",
                    ""
                ]
            },
            {
                "title": "What We Have Covered 1.7",
                "data": [
                    "So we have gone over a high-level view of the most important components of a generic CPU, discussed general purpose registers and the functionality and we have differentiated the RISC and CISC computer architecture. In this class, we will use a RISC architecture known as MIPS to cover the different topics.",
                    ""
                ]
            },
            {
                "title": "Assembly Language (Pt.1) 2.1",
                "data": [
                    "In this module, we will introduce some basic concepts of assembly programming and then correlate it to machine-level instruction using a mnemonic format. More specifically, we will introduce some basic assembly instructions in the MIPS architecture. We will then go in detail on how to map MIPS assembly into MIPS mnemonic format, which is the format we will be focusing on throughout the class, and we will cover some registers available in a typical MIPS architecture",
                    ""
                ]
            },
            {
                "title": "MIPS Instruction Set Architecture 2.2",
                "data": [
                    "So in this class, we will use the MIPS processor architecture as reference. MIPS stands for Microprocessor Without Interlocked Pipeline Stages. MIPS uses RISC architecture, is open-source and is commonly found in gaming systems such as Playstation 1, 2, Playstation portable, and the like.  In this class, we will use MIPS64 architecture. Each MIPS64 instruction is 32 bits long. The default word size is 32 bits. However, the 64 and MIPS64 indicates that the processor can do operations on data as large as 64 bits, or double word.",
                    ""
                ]
            },
            {
                "title": "MIPS Assembly: Introductory Example 2.4",
                "data": [
                    "So, let\u2019s go through a detailed example of converting a high-level code in C or C++ to MIPS assembly and ultimately to MIPS mnemonic format. Consider the high-level code we have here as an example. Note that the values for A, B, and C, are initialized in hexadecimal format",
                    "",
                    "So when we successfully compile this high-level code using our MIPS compiler, we get the software binary that is compatible only to a MIPS processor. This binary is the executable file of the code. And if we try to open this binary, what we see is a bunch of ones and zeros representing the bits for the machine-level instructions that implement this high-level code",
                    "",
                    "This binary itself is generally broken down into two segments: a code segment that contains the machine level instructions that implement the functionality of the high-level code and a data segment that contains the declared static variables in the high-level code. Each segment has a start address that we call the base address. Note that the addresses are given in hexadecimal. In our example, the code segment base address is F00 and the base address of the data segment is A000. Note that in the data segment, each static variable is initialized to 64 bits. This is the default configuration of the MIPS64 processor.",
                    "",
                    "So ideally, what we would like to do is to be able to understand how the high-level code A = B+C maps to machine code However, this is too complicated because working with the bitwise representations of machine-level instructions is tedious and error-prone",
                    "",
                    "MIPS Assembly: Introductory Example 2.5 ",
                    "Instead, we start by working with assembly language. The assembly language is a symbolic representation of the machine code. In this respect, symbolic means it simplifies some of the complexities of machine code by using labels and symbols. Nevertheless, given a machine-level code, there is a one-to-one mapping between a machine instruction and an assembly instruction",
                    ""
                ]
            },
            {
                "title": "MIPS Assembly: Introduction Example 2.6",
                "data": [
                    "Let\u2019s illustrate this with the A = B+C example. Recall that MIPS64 is a RISC architecture, so there must be four machine-level instructions to perform A = B+C: two instruction to load B and C to registers, one to add the registers, and one to store the result back to memory location of A. Since there is a one-to-one mapping between machine code and assembly, we can convert this high-level code into four assembly level instructions. The first instruction copies the value of B from the data segment to a register; let us assume the assembly labels the register using the symbol t1. The second instruction copies the value of C from the data segment to another register, say register t2. At this point, the value of B and C are in t1 and t2 respectively. The third instruction adds the the registers t1 and t2 and puts the result into register t3. The last instruction copies the value of t3 into the data location that is allocated for A.",
                    "Here we see the more formal assembly language code for this high-level operation. The first two assembly-level instructions are load double word, or LD. In this respect the double word indicates to the processor that the data to be loaded from memory to register is of size double word, or 64 bits. The third instruction is a double word add, or D--DD, where the ALU adds the two registers t1 and t2 and puts the results in t3. The final instruction is store doubleword, or SD, where the content of t3 is copied into memory address of A. Note the syntax of the assembly-level instructions. For load doubleword, we see the destination register is put first, followed by the label of the memory address. For the doubleword add, the destination register is put first, followed by the source register operands. For the Store Doubleword, the source register is put first, followed by the label of the destination memory location.",
                    "",
                    "So, since assembly language has a one to one correlation to machine code, we can map each assembly instruction of the A = B+C operation to a machine-level instruction.",
                    ""
                ]
            },
            {
                "title": "MIPS Assembly: Introductory Example 2.7",
                "data": [
                    "However, assembly code doesn\u2019t fully represent machine-level instructions. This is due to symbolic notations of the registers and the memory addresses. The processor doesn\u2019t understand a memory address labeled as B or a register labeled as t1.",
                    "",
                    "Let us consider the first Load doubleword instruction we had earlier and let\u2019s compare it to the real machine instruction. Using this example, we will see that the assembly notation doesn\u2019t have some of the fields of machine-level code.",
                    "",
                    "So in MIPS64 architecture, the most significant six bits of a machine-level instruction represent its opcode. The opcode is a simple mnemonic notation to encode the operation of the of the instruction. In our example, the opcode 110111 means this is a load double word instruction. The assembly instruction also has a load doubleword opcode. ",
                    "",
                    "The next 5 bits of the machine-level instruction represent the base field. This is the number of a general purpose register number that holds the base address of the data segment. In our example, this register is register number 8, or R8. Note that there is no such base register field in the assembly instruction. Instead, it simply uses the label name of the data as it was declared in the high-level code. The CPU cannot understand such high-level label.",
                    "",
                    "The next 5 bits of the machine-level instruction represent the Rt register field. This field indicates the destination register where the double word will loaded. In the machine-level instruction, this Rt register is register number 9 or R9 and is labeled as t1 in the assembly code. This is still similar to the assembly level code, though the labeling is different",
                    "",
                    "The last 16 bits of the machine code is the offset field. It indicates where within the data segment the requested value is located. In our example, the offset is 0 because B is the first value in the data segment. Note that the assembly-level instruction does not have such field. ",
                    ""
                ]
            },
            {
                "title": "From Assembly to Mnemonic Format 2.9",
                "data": [
                    "Although assembly-language provides a one-to-one correlation with machine code, it still has some limitations with fully illustrating how the CPU interprets the machine-level instructions. And since this is a computer architecture and organization course, we want to have as good of a feel as possible of the CPU inner-workings.",
                    "",
                    "With that in mind, we use MIPS mnemonic format to represent software code. We can think of mnemonic format as a readable version of machine-level code. It interprets all of the fields of machine-level code to better illustrate the inner-workings of the CPU.",
                    "",
                    "When writing in MIPS mnemonic notation, we generally use hexadecimal format, except for register names. For some cases, the hexadecimal notation of number may take too much space. In such cases, we use the decimal notation of that number and explicitly say it is the decimal notation. For example FFFF in hexadecimal is equal to -1 in decimal, so we put -1 in parentheses and add the 10 subscript to indicate this a decimal notation. This is just to make it easier to write MIPS in mnemonic format.",
                    ""
                ]
            },
            {
                "title": "Introductory Example 2.10",
                "data": [
                    "So here we see the MIPS mnemonic version of our working assembly example.  We notice some clear differences from the offset. The first thing we notice is that each instruction has an associated address. This is the address of the instruction in the code segment. Note that these addresses are given in hexadecimal format.",
                    "",
                    "Recall that since each instruction is 32 bits, or four bytes, it is easy to calculate the address of each instruction once we know the base address of the code segment. The base address itself is the address of the first instruction in the segment. We also notice that the registers are given using numerical notations, as is done in machine code.  There are 32 general-purpose registers in MIPS64, so they are labeled R0 to R31.",
                    "",
                    "The last thing we notice is that the mnemonic notation has the same fields as the machine code. Using the first Load doubleword example, we see that the mnemonic notation has the base and offset fields, just like the machine code discussed earlier",
                    ""
                ]
            },
            {
                "title": "Introductory Example 2.11",
                "data": [
                    "Here we put the full version of the code segment in MIPS mnemonic notation. The semicolon indicates the beginning of a comment. So anything following the semicolon is a comment. This is similar to the forward slashes in C or C++.",
                    "",
                    "Regarding the data segment, we know that each double word data is 64 bits, or 8 bytes, because we are using MIPS64. So once we know the base address of the data segment, we can find the addresses of all data in the segment. Note that this is the same approach that the CPU uses to determine the address of data that it accesses. That is, the CPU uses the base address of the data segment and the offset of the target data within the segment. So putting it all together, we get the following notation for the code and data segments in MIPS mnemonic. Note that all addresses are in hexadecimal. ",
                    ""
                ]
            },
            {
                "title": "What We Have Covered 2.12",
                "data": [
                    "We have introduced MIPS assembly language and covered some fundamental instructions. We have also seen that, although assembly has a one-to-one correlation to machine level code, it doesn\u2019t fully illustrate how the CPU executes the code. We have then covered MIPS mnemonic notation, which gives a good middle ground between assembly and machine-level code. Next, we will use a tabular approach to illustrate how the CPU updates registers and memory when it executes instructions",
                    ""
                ]
            },
            {
                "title": "Assembly Language (Pt.2) 3.1",
                "data": [
                    "We\u2019ll now pick up from the MIPS mnemonic notation and start discussing some aspects of CPU execution. We will use a table to show how the CPU updates registers and memory when executing each instruction. We will then discuss the three instructions we have covered so far and detail their fields and how they work. Finally, we will review the 32 MIPS general purpose registers and what their roles are",
                    ""
                ]
            },
            {
                "title": "MIPS Assembly: Introductory Example 3.2",
                "data": [
                    "We will now show how to execute this MIPS code in table representative of the components of the CPU. So the table has the following columns: an instruction column that lists the instructions in MIPS mnemonic format; a PC column, where PC stands for program counter. The Program Counter is a special register in the CPU that holds the address of the next instruction to execute. Then, there is a column for each register used in the MIPS mnemonic code. Since our example uses registers R8, R9, and R10, there is a column for each. There is also a column for each data in the data segment. Since there are 3 pieces of data in the data segment, there are three columns and we use their addresses to label each. The last column is a memory access column to indicate what kinds of memory accesses the associated instruction makes. Since we are using the MIPS mnemonic format for the table, the content of the table will be in hexadecimal, unless stated otherwise using the subscript notation. ",
                    "",
                    "The first column of the table is ALWAYS the Initial column. This column illustrates the state of the CPU before execution of our code. At the initial state, the data segment has the original data of the variables in the code. So we write those values in their respective columns in the table.",
                    "",
                    "In addition, during initialization the CPU writes the address of first instruction in the Program Counter and writes the base address of the data segment in the R8 register. Remember that the Program Counter stores the address of the next instruction to execute. So since the next instruction to execute is the first one, it make sense that PC has that address before any instruction is executed. Since we don\u2019t initialize the values of the other registers, we set them as question marks. ",
                    "",
                    "For each instruction, the first thing the CPU does is it reads the instruction from the code segment; that is known as an instruction fetch or instruction read.  We concatenate it as INS read in the memory access column. The CPU uses PC to get the address of the instruction to fetch",
                    "",
                    "Since the Program Counter points to the address of the next instruction, once an instruction is fetched, the Program Counter has to be updated to point to the next instruction. Generally, this update is done by simply incrementing the Program Counter by 4, because instructions are 4 bytes long. Later in this course, we will when Program Counter is not always incremented by 4 due to branch and jump instructions.",
                    "",
                    "We then analyze the instruction to see what it does and update the columns of the table accordingly. In our example, the instruction is a load double instruction, where the base address of the data segment is in register R8, the offset is 0, and the destination register is R9. So, the CPU first calculates the address of the data in memory. This is done by adding the content of the base register to the offset. Once the address is obtained, the CPU reads the memory at that address to get the data. Since this is a memory access, we update the memory access column to indicate a data read. Once the value data is obtained from memory, the CPU writes it in the destination register.",
                    "",
                    "For all registers and memory locations that are not modified by this instruction, we put an NS label to indicate Not Stored. In other words, this instruction does not store anything in those registers and data locations and their values remain the same as before",
                    ""
                ]
            },
            {
                "title": "MIPS Assembly: Introductory Example 3.3",
                "data": [
                    "So at the end of this instruction, we see the value of R9 has been updated as desired by the Load double word and the program counter points to the next instruction. The next instruction is also a load double word and we see that the memory access column indicates an instruction read to fetch the instructions from the code segment, and a data read, to read the double word data from the data segment. In addition, we see that the Program Counter is incremented by four to point to the next instruction to execute and we see that the destination register R10 of the load doubleword is updated with the appropriate value. Everything else remains unchanged.",
                    "",
                    "The next instruction is a Doubleword add. This instruction takes the content of registers R9 and R10, sends them to the ALU for addition, and puts the result in the destination register R11. The instruction only does an instruction read because it doesn\u2019t access the data segment. Its operands are only registers. We see that the Program Counter is incremented to point to the next instruction and R11 has the result of the addition in hexadecimal.",
                    "",
                    "The last instruction is a store doubleword. It works in a similar fashion to load doubleword, except that instead of loading memory to register, it stores from register to memory. Store doubleword uses its base register and offset to calculate the target address of the memory, then writes the content of the register operands to the memory location indicated by the calculated address. As a Store doubleword, the CPU does an instruction read and data write for this instruction. We also see that the PC is incremented by 4, even if there is no such instruction at address F10 in our code segment. This is because the CPU does this increment automatically. ",
                    "",
                    "At the end, this is content of the table. It illustrates the different registers and memory locations that are relevant to the code and how they are impacted by each instruction in said code.",
                    ""
                ]
            },
            {
                "title": "MIPS Assembly: Introductory Example 3.4",
                "data": [
                    "Let us now discuss the syntax and semantics of the Load doubleword, doubleword add, and store doubleword instructions that we have already covered. Here we see the syntax and semantics of the Load doubleword. Rt is the destination register, Rs holds the base address of the data segment and the offset holds the position of the data relative to that base address. The offset is always at most 16 bits. To calculate the address of the target data, the CPU adds the value of the base address in Rs to the offset. But Rs is a 64-bit register, and the offset is 16 bits. The addition requires both values to  be the same size.  To change the size of the 16-bit offset to 64 bits, we perform what is called a sign extension. ",
                    "",
                    "The basic idea of a sign extension is to increase the number of bits used to represent a value, while keeping the sign. So the na\u00efve approach to extend a 16-bit value to 64 bits would to append 48 zeros to the left of the original 16-bit value.  However, this approach doesn\u2019t work if the number is negative.  This is because if we extend a negative number with zero bits, then it becomes positive because the most significant bit is now 0 instead of 1. So sign extension works by first looking at the most significant bit of a binary, and using the value of that bit to extend it to the desired bitsize.  The three examples here show how to sign extend different 16-bit values to 64 bits",
                    ""
                ]
            },
            {
                "title": "MIPS Assembly: Introductory Example 3.5",
                "data": [
                    "The double word add instruction adds two registers Rs and Rt and puts their result into Rd register. The ALU may need to perform some sign extension to ensure that the content of both Rs and Rt are 64 bits",
                    "",
                    "The store doubleword instruction has a similar syntax as load double word but the semantics are obviously different. Store doubleword copies the content of register Rt to the memory location indicated by the address calculated. Similar to load doubleword, the calculation of the address requires the offset field to be sign extended to 64 bits.",
                    ""
                ]
            },
            {
                "title": "MIPS General Purpose Registers 3.7",
                "data": [
                    "Let us now go over the list of general purpose registers in the MIPS CPU. Remember there are 32 of them, labeled R0-R31. R0 always contain the value 0 and it\u2019s read-only. That is, no code can write to it. It is only used whenever we want to use the value 0. R1 is reserved for the assembler. R2 and R3 are used to return results of function calls. If you have a function that returns an integer, then it will be stored in either R2 or R3 after the function completes. One interesting question is if we have a function that returns something like a struc of 20 integers,  how does MIPS return the 20 values when it only has 2 registers. I will leave it to you guys to find out. R4 to R7 are used to pass parameters of a function call. Similarly, I will let you guys find out how to call a function that passes more than four parameters. R8 to R15 and R24 to R25 are used to store temporary values that are declared in a function. So when then function completes, the values of these registers are not saved. We will primarily use these registers in our MIPS mnemonic codes. R16 to R23 are also used to store values in functions, but they are stored and saved when the function completes. So they can be used across multiple functions. R26 and R27 are reserved for the O.S. R28 is the global pointer and it points to the static data segment of the code. R29 and R30 point to the stack and frame pointers respectively. The stack and frames are structures in the CPU that store local variables of a given function. R31 holds the address to return from a function call. For our course, we will primarily use R0, R8 to R15, R24, R25, and R31. Note that we do not include the Program Counter in this list because it is not a general-purpose register. It is a special register and cannot be accessed by any user-level code.",
                    ""
                ]
            },
            {
                "title": "Another Example 3.8",
                "data": [
                    "As an exercise, I am encouraging you guys to try the following exercise. First, write the MIPS Mnemonic code that performs the high-level code above. This code does A = B or C. Note that this is a logic or operation. Once you have written the mnemonic code, show its execution in the table as we did before for A = B+C example. The code and data segment addresses are specified, as well as the original values of A, B, and C.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 20,
        "module_name": "Computer Architecture Part 2",
        "file_name": "Module 20 Computer Architecture Part 2.docx",
        "transcript": [
            {
                "title": "Outline 1.2",
                "data": [
                    "Hi everyone. Today, we will cover the second half of the computer architecture section of this course. We will focus on branch and jump instructions, which are fundamental in all instruction set architectures. We will discuss the purposes of these instructions, how they are mapped to high-level C and C++ codes that we write every day, and we'll go over two examples of their use cases"
                ]
            },
            {
                "title": "Instructions Covered So Far 1.3",
                "data": [
                    "So far, we have covered basic instructions such as Load and store instructions to get data from and to memory, as well as some arithmetic and logic instructions.   The arithmetic and logic instructions we have covered so far require that all operands to be first put in registers. However, there are cases when we write code and we use hardcoded values, instead of variables, to perform arithmetic and logic operations.",
                    "As the high-level code example highlights, the second variable is hardcoded as 17. MIPS64 provides Immediate Arithmetic and Logic Instructions for such high-level code. The basic idea of an immediate ALU instruction is to store the value of the hardcoded operand as part of the 32-bit instruction. Several immediate ALU instructions are provided including doubleword ADD I, for addition, doubleword OR I for OR operation, and doubleword AND I for AND operation. The I at the end of each of these instructions stands for immediate",
                    "Each immediate ALU instruction has a 16-bit immediate value to store the hardcoded operand. This 16-bit immediate is labeled, do I M M, for Do Immediate in the semantics. For an immediate arithmetic instruction such as doubleword ADD I, this do immediate value must first be sign-extended before the arithmetic operation is performed. However, for an immediate logic instruction such as D OR I or D AND I, the do immediate value does not need to be extended. It simply needs to be padded with zeroes to reach 64 bits before the logic operation is performed.",
                    "Knowledge Check 1.4 ",
                    "Answer A",
                    "That is correct. First, you took negative 4, convert it into hexadecimal which is FFFC. Then we took the original value of R8 add it to our hexadecimal value of negative four, and the answer is correct. ",
                    "Answer B",
                    "Remember, here the immediate value is negative four. So you must first convert the negative four value in its representative hexadecimal format, sign-extend it, then do your addition.",
                    "Answer C",
                    "Here, you have to keep the size of the operands consistent. Remember, this is a DADDI instruction, so the ALU is going to use double word, or 64-bit values. Therefore, you must sign-extended both the register R8 and the immediate value to 64 bits before you do the addition.",
                    "Answer D\t",
                    "Pay close attention to the semantics of the instruction. The register at the Rt position is the destination and the register at the Rs position is the source. For our case, the source is R8."
                ]
            },
            {
                "title": "Jump Instructions 1.5",
                "data": [
                    "We will now discuss Jump instructions. So far, the instructions we have covered only allow us to write sequential code. But what happens if we have function calls, conditional statements, and loops in our code. As we know, code that we write generally involves these situations.",
                    "This is where jump instructions come into play. These instructions allow the CPU to skip chunks of sequential code in the binary and to jump to locations specific\u2026 to jump to locations of specific instructions in that binary. We call these locations target instructions. There are two types of jump instructions: unconditional jumps, where we always jump to the target instructions, and conditional jumps, where we branch to the target instructions based on a given condition. ",
                    "From now on, we will refer to unconditional jumps simply as jumps and conditional jump as branches. Jump instructions are generally used for calling functions and returning from them, whereas branch instructions are used for if/else statements and conditional loops",
                    "Another important factor in jump and branch instructions is how far can the instructions jump and how to calculate the target address of that instruction we are jumping or branching to. There are generally two methods: one is called relative branch or jump. In this method, the target address is a function of the program counter of the branch or jump instruction. The other method is called indirect branch, where the target address is independent of the program counter. ",
                    "Relative branch and jump instructions restrict how far the target addresses can be, usually it is bounded within a few kilobytes of the program counter. On the other hand, indirect branch and jump instructions obtain their target addresses from registers or a memory and can, in theory, take any value.",
                    "Knowledge Check 1.6 ",
                    "Answer A",
                    "Remember, by default, the CPU goes to PC+4 for non-branch and non-jump instructions. Therefore, there is no need to use a jump instruction to do PC+4. Jumps and branch are thus NEVER used to go to the next instruction in the code sequence.",
                    "",
                    "Answer B",
                    "When the application finishes its job, it should, in fact it must, jump to the function that calls it. This is usually an operating system function, or another application. So it wouldn't jump to its own code if it has finished executing.",
                    "Answer C",
                    "That is correct. If the unconditional jump instruction jumped to itself, then the target address will be again the unconditional jump instruction. Therefore, the unconditional jump instruction will continually jump to itself over and over again resulting in an infinite loop.",
                    "Answer D",
                    "Remember, jump instructions are unconditional. So there is no condition to check. The CPU will ALWAYS go to the calculated target address."
                ]
            },
            {
                "title": "MIPS64 Unconditional Jump Instructions 1.7",
                "data": [
                    "Let us now detail the MIPS64 jump instructions.  There are three such instructions: Jump, Jump and Link, and Jump register. Jump is commonly used when performing unconditional jumps within a function or procedure. This can be used for example for goto statements. Jump and Link is used for function calls and has two jobs: it first saves the return address in the R31 register, and then updates the program counter to the address of  first instruction of the function. Note that the return address in this case is the address of the instruction we must return to when the function call has completed. ",
                    "Jump register is used to return from a function call.  Jump and Jump and link both use relative jump, whereas Jump register uses indirect jump and holds the address of the target instruction in a register."
                ]
            },
            {
                "title": "MIPS Unconditional Jump Target Address 1.8",
                "data": [
                    "Let us now discuss how to calculate the target addresses of the different jump instructions. As mentioned before, Jump register uses indirect jump, where it gets the target address from a register. Therefore, there is no calculation done, as the address of the target instruction is simply copied from the register RS to the program counter",
                    "On the other hand, both Jump and Jump and Link use the same function to calculate the target address. We label this function func1 and it uses the program counter of the jump instruction and another 26-bit parameter that we call address. Note that this address parameter is not the target address we are looking for. ",
                    "Let's take a close look at the formula for func1. The first step is to increment the program counter by 4. In this instance, the program counter is the address of the jump or the jump and link instruction. The next step is to take the most significant 4 bits of the result from step 1. This assumes that the program counter is 32 bits. If we encounter a case where the program counter is less than 32 bits, then we have to pad it with zeroes to make it 32. The third step is to take the address parameter\u2026 to shift the address parameter left by 2, which has the same effect as multiplying it by 4. This turns the 26-bit parameter to a 28-bit parameter. The last step is to concatenate the most significant 4 bits from step 2 with the 28 bits from step 3. The result is the 32-bit address of the target instruction. "
                ]
            },
            {
                "title": "Let us take a look at an example of how calculate a target address for jump and link instructions. Here we have a jump, where the program counter is 4DC and the address parameter is 13B. Note that all numbers are in hexadecimal. We expand the program counter to 32 bits because the formula for func2 expects 32-bit program counters. The first and second steps are straightforward; this is because even after we do PC+4, the most significant 4 bits of the program counter are still 0.",
                "data": [
                    "For step 3, we multiply the address parameter 13B by 4. Remember that this is a hexadecimal multiplication, so the correct answer should be 4EC. Then we expand the multiplication result to make it 28 bits. This is done by padding the result with zeroes. The last step concatenates the results from steps two and 3 respectively, giving us the target address highlighted.",
                    "Knowledge Check 1.9 ",
                    "Answer A",
                    "Pay close attention to the formula for func1. Note that the PC+4 addition is a hexadecimal addition. Also, the PC is assumed to be 32 bits",
                    "Answer B",
                    "Pay close attention to the formula for func1. Note that the PC+4 addition is a hexadecimal addition. Also, the PC is assumed to be 32 bits",
                    "Answer C",
                    "Remember, you must take the most significant four bits of PC+4. The formula assumes that the program counter is 32-bits. So if it's not, you must first pad it with zeroes to make it 32 bits. Then take the most significant four bits [31:28] of the result",
                    "Answer D ",
                    "That is correct. We first took the address parameter, which is 13 bit, shift it left by 2 which has the same effect as multiplying it by 4 given us four EC. Since the most significant four bits of the program counter is zero, then the final answer itself is 4EC"
                ]
            },
            {
                "title": "MIPS64 Conditional Branch Instructions 1.10",
                "data": [
                    "Let's now go over branch instructions in MIPS64. The two most fundamental branch instruction in MIPS64 are Branch if equal, or BEQ, and Branch if not equal, BNE. Both branch instructions use relative target calculation and have the same Function, labeled func2, to calculate the address of the target instruction. Both instructions compare the values of two registers to check their condition. Branch if equal checks if the values of the two registers are the same. If so, the CPU branches to the calculated target address. Otherwise, the CPU goes to the next instruction in the sequential code which is PC+4. Branch if not equal checks if the values are different. If that is the case, the CPU branches to the calculated target address",
                    "Several other branch instructions derived from branch if equal and branch if not equal. Such instructions are Branch if equal to zero, branch if not equal to zero, branch if less than zero, branch if less than or equal to zero, any many more. We focus primarily on branch if equal, branch if not equal, and branch if equal to zero. There is one terminology that is commonly used when talking about conditional branches. It is the \u201cbranch is taken/not taken\u201d term. When one says a branch is taken, it means that the condition that the branch checks is true and the CPU branches to the target address calculated.  Branch not taken means the opposite.",
                    "Knowledge Check 1.11 ",
                    "Answer A",
                    "When we say a branch is taken, what we mean is that the comparison of the branch condition is true. We don\u2019t necessarily mean that the registers are equal. So if the branch checks that two registers are not equal, such as branch if not equal does, and the registers are indeed not equal, then the comparison of the branch condition is true and the branch is taken ",
                    "Answer B",
                    "That is correct. The branch not equal instruction checks if the two registers are not equal; in our case r8 is 3 and R0 is always 0. So the two registers are indeed not equal, so the branch condition is true and the branch is taken.",
                    "Answer C",
                    "R8 is equal to 3 in our example. R0 is ALWAYS equal to zero; therefore the two registers are not equal",
                    "Answer D",
                    "R8 is equal to 3 in our example. R0 is ALWAYS equal to zero; therefore the two registers are not equal"
                ]
            },
            {
                "title": "MIPS64 Conditional Branch Target Address 1.12",
                "data": [
                    "Let's go over the func2 formula that calculates the target addresses for branch instructions. Since branches use relative address calculation, the formula uses the program counter of the branch instruction. In addition, the formula uses a 16-bit immediate parameter that we label as do immediate. ",
                    "Here is a little bit more detail on the formula for func2. It first takes the program counter and increments it by 4. Recall that in this situation, the value of the program counter is the address of the branch instruction. It then takes the 16-bit immediate value and sign-extend it to 32 bits. In the third step, the formula takes the sign extended immediate value and shift it left by two. This has the same effect as multiplying it by 4. The final step adds the results from step 1 and 3 to get a 32-bit target address.",
                    "Here is an example to illustrate how func2 works. We have a branch if equal instruction with the program counter and the do immediate parameter given. The first step adds 4 to the program counter and the second step sign extends the do immediate value. Since the Do Immediate is a positive value, the sign extension simply pads zeroes to make it 32 bits. The third step takes the sign-extended Do Immediate and multiplies it by four, and the last step adds the results of step 1 and step 3 to get the target address highlighted",
                    "Knowledge Check 1.13 ",
                    "Answer A",
                    "We first check if the branch condition is true. R8 is 3 and R0 is always 0. Since the branch condition checks if the registers are not equal, it is indeed true.  Therefore, the CPU goes to the target address and doesn't go to PC+4. Use the formula in func2 to calculate the target address.",
                    "Answer B",
                    "First, we compare the branch condition. Is R8 not equal to zero? Yes. So the branch is taken and the CPU branches to the target address. Now use the formula in func2 to see if the target address is the end of the application",
                    "Answer C",
                    "That is correct. We first check if the branch condition is true; the registers are not equal, so the branch condition is true and the branch is taken. To calculate the target address, we take the immediate value, sign extend it which gives us all F\u2019s because it is a negative one in decimal. Multiply it by four which should give us FFFFC. And we add it to the value of the program counter which should give us PC+4, which should give us back the original value of the branch if equal program counter. Hence, the branch instruction jumps back to itself resulting in an infinite loop.",
                    "Answer D",
                    "The branch condition can be determined from the information available. We know that R8 is 3 and R0 is always zero. The branch checks if R8 is not equal to R0. In our case, the branch condition is true."
                ]
            },
            {
                "title": "One instruction that is strongly correlated with branch instructions is SLT or Set less than instruction. SLT uses three registers, Rd, Rs and Rt. It checks if Rs is less than Rt. If so, it sets Rd to 1. Otherwise, it sets Rd to 0",
                "data": [
                    "Several branch instructions in MIPS64 are formed by combining SLT with either branch if equal or branch if not equal. For example, Branch if less than zero can be formed by SLT and branch if equal as the example here shows. A similar method can be used for Branch if greater than zero instruction."
                ]
            },
            {
                "title": "MIPS64 Branch/Jump Instruction Uses 1.15",
                "data": [
                    "Let us now see some use cases of branch and jump instructions. Consider this high-level C code. This is a function that calculates the absolute value of a number. That is, if the number is negative, it turns it to a positive; otherwise, it keeps it as is. The value us returned from the functional call. As we see from the high-level code, there is an if condition to check if the number is negative. This if condition in high-level code will be translated to a conditional branch instruction. We also notice there is a return instruction to end the execution of the absolute value function. This return instruction will be translated to a jump register, as it is the jump register instruction that is used to return from function calls.",
                    "Here are several approaches to translate the if X is less than 0 condition into machine code. Let us assume that X is in register R4. The simplest approach is to use the branch if less than zero instruction. In that case, if X is negative, the CPU would skip the sequential instructions and branch to the part of the code that perform X = minus X in order to get the positive value. Another way to is to use the branch if greater or equal to zero condition. In that case, X is positive, we would skip the sequential instructions and branch to the part of the code that does return X. Otherwise, we would subtract.",
                    "Another way is to combine SLT with branch if equal instruction. The SLT instruction checks if X is less than zero. If it is, it set R8 to 1. Otherwise, R8 is set to zero. The branch instruction then checks R8 to zero. If R8 is equal to zero, then x is positive, the branch is taken, and the CPU branches to the return instruction. We will use this approach to further develop the code for the absolute function."
                ]
            },
            {
                "title": "MIPS64 Branch/Jump Instruction Uses 1.16",
                "data": [
                    "So, now we have the code that handles the if condition. How do we write the rest of the code to complete the function?  First, what instruction should be right after the branch instruction? In the sequence of the code, the instruction following the branch must handle the case where the branch condition is false and the branch is not taken. ",
                    "So in our example, the branch instruction condition checks if X is positive. Therefore, the next instruction in the sequence handles the case where X is negative. For our absolute function, when X is negative, we need to negate it.  For that, we can simply use a doubleword sub or DSUB instruction to negate R4 and turn it into a positive value.",
                    "To find out what the instruction after the subtraction should be, we again look at the high-level code of the absolute value function. We observe that once the subtraction is performed, the next operation is to return from the function call. And we already know that if the branch condition is true and X is already positive, we would simply return from the function call. In this respect, there is nothing left to do in this code but to return from the call. For this return instruction, we use the jump register. This jump register instruction is also where we would do the branching to in case our branch if equal condition is true."
                ]
            },
            {
                "title": "MIPS64 Branch/Jump Instruction Uses 1.17",
                "data": [
                    "The code is still not complete. More specifically, we need to replace the Do immediate parameter with its correct value in the branch if equal instruction. And we need to set the correct value of Rs in the jump register instruction."
                ]
            },
            {
                "title": "MIPS64 Branch/Jump Instruction Uses 1.18",
                "data": [
                    "To set the correct value of the Rs in the jump register instruction, we need to first understand the interaction between the jump and link instruction the and jump register instruction. Consider this use case of the absolute value function. We have a header file Lib dot H that holds the declaration of the absolute value function and a c file Lib dot C that holds its implementation. In the main file, we call the absolute value function. Recall that the jump and link instruction is responsible for function calls. Jump and Link does two things. It first sets the address to return to after the function call in R31, and then it jumps to the function. So in this respect, the return address is in the R31 register and the jump register instruction must use R31 to return from the call. Hence Rs in the jump register instruction is equal to R31.",
                    "Knowledge Check 1.19 ",
                    "Answer A",
                    "The target address is given as 400400. Use the inverse of the formula in func2 to find DoImm",
                    "Answer B",
                    "Pay close attention to the inverse formula of func2. Note that that program counter is incremented by four. Also remember, shift right by 2 is the same as divide by four. And this division is the last operation of the inverse formula.",
                    "Answer C",
                    "Remember the first part of the formula of func2 is to do PC+4. ",
                    "Answer D",
                    "That is correct. Using the inverse formula f12, we first do PC plus 4 subtract it from the target address. And then we either shift it right by 2 or divided by 4 to give us the immediate value."
                ]
            },
            {
                "title": "MIPS64 Branch/Jump Instruction Uses 1.20",
                "data": [
                    "There is one more factor to consider. As we see in the high-level code, the absolute value function call must return the result to the main code. In our current MIPS mnemonic code, we have the result of the absolute value in the R4 register. If we recall, R4 is a register used to pass parameters to function calls. It thus cannot be used to return data from function calls. For that, we need to use either R2 or R3.",
                    "So in the final version of the code, we need to initialize R2 to X in order to keep the value in R2 in case X is already positive. We also need to make sure that the result of the subtraction that turns the negative value into a positive is also put into R2. This way, when the function call returns, the absolute value is in the R2 register and it can be used as the result variable in main code."
                ]
            },
            {
                "title": "MIPS64 Branch/Jump Instruction Uses 1.21",
                "data": [
                    "Now let's take a look at an example of how jump and branch instructions are used for loops. Consider the following example. We have two integer variables X and Y, both greater than zero. The goal is to perform X times Y, without using the multiplication instruction. ",
                    "One way to implement this function in a high-level C or C++ code is as follows. We have a while loop that uses a decrementing counter initialized at Y. Inside the loop, we add X to an accumulator as long as the counter is not zero. Note that for this approach, it doesn't matter if C and Y are always greater than 0. Another approach is to leverage the fact that X and yYare both greater than zero. In that case, we know that the loop will be executed at least once. So we can use a do while loop for the code. We will use that second approach to implement the MIPS mnemonic code."
                ]
            },
            {
                "title": "MIPS64 Branch/Jump Instruction Uses 1.22",
                "data": [
                    "So let's detail the MIPS mnemonic code for our X times Y. Let us assume that X and Y are in R4 and R5 respectively and that the code segment starts at address 4D0.  ",
                    "If we look at the high-level code, we see that the first two operations initialize the counter to Y and set the accumulator R E S to zero.  We can do the same for our MIPS code by initializing the register R8 to zero and the register R9 to the value of Y. In our MIPS code, we will thus use R8 for the accumulator and R9 for the loop counter. Since we have a do while loop, we know that we are going to add to the accumulator and decrement the loop counter at least once. To add to the accumulator, we simply use the doubleword ADD, or D ADD, R8, R8, R4 instruction to keep adding X to itself. To decrement the counter, we use the immediate add instruction D ADD I, where the immediate value is negative 1. ",
                    "The next part of the code is to check if the counter is zero in order to determine if we need to get out of the loop or not. There are several ways to do this. We can use a branch if not equal to zero instruction, a branch if equal to zero, a branch if not equal, and probably other approaches. We choose to use branch if not equal. Since we need to compare the loop counter to zero, we use R9, which holds the loop counter, and R0, which is always zero, as the registers of our BNE instruction. If the counter is not yet zero, the branch is taken, and we need to go back to the two instructions that add the accumulator and decremental loop counter. This allows us to iterate through the loop one more time.",
                    "As we discussed before, the sequential instruction after a branch instruction must handle the case when the branch is not taken. For our code, the branch is not taken when the counter is equal to zero and we need to return from the function call.  But instead of simply handling the return, we first need to store the final result of the accumulator in R2 or R3 register so that it can be available as a return value from the function call. So here we use the R2 register to return the value of the accumulator.  This is similar to what we did before for the absolute value function. The last instruction handles the return from the function call and for that we use the jump register instruction. Assuming that the function would be called using jump and link, we know the return address will be in R31 register."
                ]
            },
            {
                "title": "MIPS64 Branch/Jump Instruction Uses 1.23",
                "data": [
                    "We still need to determine what is the value of the do immediate parameter in our branch if not equal instruction. For that, we use the reverse of the func2 formula to get Do Immediate sign extended. We use the same approach as before the get the inverse of func2 and use the inverse to get the value Do Immediate. We observe that this Do Immediate is a negative value. Looking back at the complete code, we can see why Do Immediate is negative. This is because if the branch is taken, we go back up in the code. When the value is positive, as it was in the previous example, the branch was taken; we went down in the code.  "
                ]
            },
            {
                "title": "MIPS64 Branch/Jump Instruction Uses 1.24",
                "data": [
                    "Here we have the MIPS mnemonic code for the other high-level code that works whether X or Y is zero. In this code, we also initiate registers R8 and R9 to hold the accumulator and the loop counter. The next stage is to implement the while loop. For that we use a branch if not equal instruction. If the branch condition is true, we go inside the loop to decrement the counter and to add to the accumulator. ",
                    "If the loop condition is false, then we need to quit to loop. The way we do that in this case is by using the Jump instruction. This allows us to skip the instructions that are within the loop. We also see that there is a jump instruction after the instructions within the loop. This second jump instruction targets the branch if not equal instruction so we can go back and check the condition of the loop before we quit the function. The last two instructions are similar to the other code as they are responsible for copying the value of the accumulator in the R2 register and for returning from the function call."
                ]
            }
        ]
    },
    {
        "module_number": 21,
        "module_name": "Computer Organization Part 1",
        "file_name": "Module 21 Computer Organization Part 1.docx",
        "transcript": [
            {
                "title": "Topics Covered 1.25",
                "data": [
                    "(MIPS 64 Instructions)"
                ]
            },
            {
                "title": "Outline 1.2",
                "data": [
                    "Hi everyone. Today we will start discussing the computer organization section of this course. We will learn about the innerworkings of the CPU that allows it to execute its instructions and we will discuss different methods to optimize CPU performance. ",
                    "",
                    "So this is the outline for today. We will go over instruction types and formats, which are critical in determining how instructions are executed. We will then detail the execution steps for a small processor that we will design. We will then discuss processor clock rate to estimate how fast our processor is, and go over processor pipelining, its benefits, and the issues it presents.",
                    ""
                ]
            },
            {
                "title": "MIPS64 Instructions 1.3",
                "data": [
                    "Let us discuss how instructions are encoded and formatted in MIPS64. First, let\u2019s go through basic characteristics of the layout of an instruction. Recall that each instruction in MIPS64 is 32 bits. Each instruction has an opcode. The opcode is 6 bits long and it is the most significant bits of the instruction.  The role of the opcode is to indicate what operation the instruction performs. For example, the load double instruction has its own opcode and the store double instruction has its own opcode. Some instructions have the same opcode for formatting purposes. In such cases, the instructions have second opcodes also known as functions. These second opcodes play the role of indicating the operations of the instructions. In addition to an opcode, each instruction has one or many operands. The goal of the operands is to indicate where the data needed by the instruction is located. An operand can be a register number or an immediate value that is hardcoded in the instruction.",
                    "",
                    "MIPS64 defines three formats for its instructions: register format,  or R-format, Immediate format or I-format, and Jump format or J-format. The purpose of instruction formatting is to allow the processor to understand the role of the instruction and what operands it needs. This provides a compact method to design the processor. If each instruction was allowed to have its own format, then the CPU would have to be very complicated to handle any formatting approach.",
                    ""
                ]
            },
            {
                "title": "MIPS64 Instruction Type: R-Format 1.4",
                "data": [
                    "R-format instructions have only registers as operands. An R-format can have between one and three registers and one of the registers can be a destination register. Another interesting aspect of R-format instructions is that that they all have the same opcode of 0. Therefore, each R-format instruction needs a second opcode, or a function, to detail its unique operation",
                    "",
                    "The figure illustrates the layout of R-format instructions. The opcode for each R-format is the same.  The instruction has three 5-bit fields for at most three registers, Rs, Rt, and Rd respectively. Another field of R-format instructions is the shift amount field. This field is only valid when the instruction is shift left logical or shift right logical.  Otherwise, the shift mount field is zero. The last field of the instruction is the 6-bit second opcode or function.  As we see, when we add the size of each field, we get 32 bits.",
                    "",
                    "Let us illustrate how the processor determines what to do when it fetches an R-format instruction. Consider the example of this figure. The processor first reads the opcode and it sees it is all zeroes, meaning it is an R-format. The processor then reads the function field, which is the least significant 6 bits, to determine the operation of that R-format instruction. The function 101100 is for the D ADD instruction. The MIPS64 manual details the function field for each R-format instruction. Once the CPU knows the instruction is a doubleword ADD, it knows that it needs the operands Rs, Rt and Rd. Moreover, since it is not a shift instruction, the shift amount field is not used. The processor then converts the register numbers for the operands of the instruction and can form the complete instruction.",
                    "",
                    "Knowledge Check 1.5 (Slide 9)",
                    "Answer A",
                    "Actually, each register can hold 64 bits of data. This is why we can do instructions such as double word add, where we add two 64-bit data.",
                    "Answer B",
                    "Actually, each register can hold 64 bits of data. This is why we can do instructions such as double word add, where we add two 64-bit data.",
                    "Answer C"
                ]
            },
            {
                "title": "That is correct. Since there are 32 possible labels for a register, they number of bit you need to represent a register field is log base 2 of 32, which is 5.",
                "data": [
                    "Answer D",
                    "Since each instruction is restricted to be 32 bits, the register field cannot be of unlimited width.",
                    ""
                ]
            },
            {
                "title": "MIPS64 Instruction Type: I-Format 1.6",
                "data": [
                    "An I-format instruction has a 16-bit immediate value as an operand. An I-format instruction can also have up to two registers, Rs and Rt. Rt itself can be a source or a destination registers, whereas Rs can only be a source register. Unlike R-format instructions, each I-format instruction has a unique opcode. Therefore, there is no need for a second opcode. ",
                    "",
                    "Let us see how the processor breaks down an I-format instruction. Consider the instruction above. The processor first reads its opcode. The opcode matches to the double word add immediate instruction, which is an immediate instruction or an I-format instruction. Since the CPU knows that D ADD I uses two registers Rs and Rt, it reads the two fields for Rs and Rt to determine the register numbers. The CPU then reads the 16-bit immediate value. Once that is done, the CPU has all of the operands of the instruction and can go onto the next step.",
                    "",
                    ""
                ]
            },
            {
                "title": "MIPS64 Instruction Type: J-Format 1.7",
                "data": [
                    "A J-format instruction only has one operand: a 26-bit immediate value. J-format instructions are used for jump instructions. Therefore, the 26-bit immediate value is used as an address parameter to use to calculate the target address of the jump instruction.  Just like I-format, each J-format instruction has its own opcode and does not need a 2nd opcode. Just like all instructions, the CPU first read the opcode to determine the format. In the case of this example, the opcode matches to the Jump instruction.  Since the jump instruction is a J-format, the CPU just reads the next 26 bits to determine the value of the address field.",
                    "",
                    "Knowledge Check 1.8 (Slide 16)",
                    "Answer A",
                    "Remember, the only operand of a J-format instruction is the 26-bit address. Jump Register doesn\u2019t have such operand. ",
                    "Answer B",
                    "The I-format instruction doesn\u2019t store the immediate value in a register. Instead, the immediate value is hardcoded into the instruction itself.",
                    "Answer C",
                    "That is correct. The Jump Register instruction uses one only the operand Rs, which is a register. Therefore, it is an R-format instruction",
                    "(Let\u2019s Build a Processor)",
                    "Instruction Execution Steps 2.1 ",
                    "OK. Now, we are going to the steps the processor go through when executing instructions. We will use the example of building a processor to illustrate these steps.  Our processor is based on MIPS64 and has only 7 instructions. Our CPU can execute four R-format instructions: D ADD, D SUB, D OR and D AND. It can execute Load double and store double, as well as branch if equal to zero.",
                    "",
                    "When executing an instruction, a CPU generally goes through the following steps: instruction fetch, instruction decode, instruction execute, memory, and WriteBack. During instruction fetch, the CPU reads the instruction from memory to determine what it is. The CPU uses the program counter to determine the address of the instruction in memory.  During decode, the CPU determines the format of the fetched instruction, the operation the instruction is supposed to do, and gets its operands. The execute step performs the operation of the instruction. The processor goes through a memory step if the instruction needs to do a data memory access.  For example, a store instruction would go through the memory step.  The processor goes through a WriteBack step if the instruction needs to write the result of its operation in a general purpose register. For example, the DADD instruction needs to write the result of its addition to the destination register and must go through the WriteBack step. As we can see, an instruction may not go through all the steps we have described.  However, each instruction goes through the fetch decode and execute stages.",
                    "",
                    "Knowledge Check 2.2 (Slide 19)",
                    "Answer A",
                    "That is correct. The load double instruction needs to accesss the memory to get the data and write the data to the destination register. The first part is done in the memory step, and the second one is done in the WriteBack step.",
                    "Answer B",
                    "The WriteBack step is only used to write to general-purpose registers R0-R31. The program counter is not a general-purpose register.",
                    "Answer C",
                    "The immediate value of DADDI is hardcoded in the instruction itself. So there is no need to go to memory to get it.",
                    "Answer D",
                    "The memory step is used to access data. Memory accesses for instructions are done at the fetch instruction fetch step.  Moreover the BEQ instruction only updates the program counter.",
                    "Answer E",
                    "The JR reads the value of R31. The writeback step is used to write to general-purpose registers",
                    ""
                ]
            },
            {
                "title": "Instruction Fetch 2.3",
                "data": [
                    "Let us discuss each execution step in detail.  In the instruction fetch step, the processor uses the program counter to access the memory and get the instruction to execute. The syntax M and PC in brackets indicates a memory access where PC is the address is used for the access. When the memory returns the instruction, the CPU puts it in a special register known as the instruction register, or IR. This register always holds the 32 bits of the instruction being executed. The last part of the instruction fetch is to update the program counter to point to the next instruction. This is done by adding four to the current program counter. The value four is used to increment the program counter because each instruction is 32 bits or four bytes. Therefore, the CPU has to jump four bytes of address space to go point to the next instruction.",
                    ""
                ]
            },
            {
                "title": "Instruction Decode 2.4",
                "data": [
                    "During the decode step, the CPU determines the operation that the instruction needs to perform, obtains the register values that may be needed, and allocates the functional resources that the instruction needs for its operation. For the purposes of our CPU, we only need one functional unit: an ALU. This is because our CPU does not have complex instructions. The same ALU is used for all operations that the CPU needs to perform.  This ALU has two outputs ALUOut1 and ALUOut2. Here is a little bit mor e detail on the decode step for our CPU. During that step, our CPU automatically accesses the registers for the Rs and Rt operands and sign-extends the 16-bit immediate value, regardless of the instruction type. ",
                    "",
                    "The CPU then puts the value of Rs in a special register called A, It puts the value of Rt into a special register called B, and it puts the sign-extended value into a special register called I MM, or immediate. Note that A, B, and I MM, are special registers like PC and IR and cannot be accessed by the code. They are only used to store temporary parameters that the CPU needs to complete the instruction.",
                    "",
                    "Knowledge Check 2.5 (Slide 25)",
                    "Answer A",
                    "Remember, during the fetch step, the CPU only increments the program counter. It cannot perform anything of relevance for the instruction because it will known what the instruction is until the decode step.",
                    "Answer B",
                    "That is correct. The fetch step only increments the counter by 4, under the assumption that the next instruction to be executed is the sequential one. The CPU doesn't yet know if the instruction is a branch. That is what the decode step is for.",
                    "Answer C",
                    "Here you used the wrong formula to calculate the target address. Remember, the first part of the formula is to increment PC by 4.  But remember, for this question, the CPU just completed the fetch step. So it doesn't yet know that it is doing a branch instruction. ",
                    "Answer D",
                    "Keep in mind the operations that the CPU performs during the fetch step. It first sends the value of the program counter to get the instruction, then it increments the program counter by four to point to the next instruction in the code sequence",
                    "",
                    "Instruction Execute 2.6 ",
                    "Regardless of the instruction, the processor goes through the same procedures for fetch and decodes steps. Things begin to change when we get to the execute step. This is because each instruction has a different role. As a result, we have to look at the execute step for each instruction. ",
                    "",
                    "Let us first look at the load double and store double instructions. During the execute step, the CPU calculates the memory address that either the load or the store instruction needs. Recall that both load and store use the Rs register to hold the base address of the data segment, and use the 16-bit do immediate as the offset within the data segment. Also, remember that to calculate the address, the CPU needs to first sign extend the do immediate, then add it to the base address of the data segment. During the decode step, the CPU had already moved the value of Rs to A and had already sign-extended the do immediate and had put into I MM. Therefore, during the execute step for load double or store double instruction, the CPU just adds A and I MM to get the address of the memory access. The result of the addition is stored in ALUOut1.",
                    ""
                ]
            },
            {
                "title": "For the arithmetic and logic R-format instructions, the ALU of the CPU simply performs the operation using the values of the registers Rs and Rt. Recall that during the decode step, the CPU already has Rs and Rt in A and B respectively. The only thing that is an issue is how can the ALU determine what operation to perform for that R-format instruction. Recall the second opcode or function field of the R-format instruction. The ALU has a lookup table that contains the arithmetic or logic operation to perform based on the value of the function field. So, the CPU simply sends the value of A and B registers and the value of the function field of the R-format instruction to the ALU. The latter used its table to perform the operation and stores the result in ALU Out 2.",
                "data": [
                    ""
                ]
            },
            {
                "title": "Instruction Execute 2.8",
                "data": [
                    "For the branch if equal to zero instruction, the CPU must perform several operations during the execute step. It must calculate the target address of the branch instruction, check the branch condition, and update the program counter. ",
                    "",
                    "Let us first see how the CPU calculates the target address for the branch instruction Remember the formula for func2 that is used to calculate the target address. During the fetch step, the CPU already incremented the program counter by 4, and during the decode step, the CPU sign-extends the 16-bit immediate value. Those two calculations from fetch and decode steps are needed to calculate the target address. Therefore, during the execute step, the CPU can simply shift the sign-extended immediate value and add it to PC to calculate the target address. This target address is put in ALUOut1. The next operation is to check the condition of the branch if equal to zero.  To check if a value is equal to zero, the CPU used the ALU to subtract that value with zero. The ALU has what is called a zero flag. That is a one-bit output that is set to one when the result of its operation is zero.",
                    "",
                    "So, to check the condition, the ALU used the BranchOp operation, which is a subtraction of the Rs, which is in the A, and R0, which holds 0. If the subtraction is zero, then the zero flag of the ALU is set to one. The last operation is to update the program counter if the branch condition is true. For this, the CPU simply writes the target address to the program counter if the value of the zero flag is one. Otherwise, the program counter remains its original value from the fetch step where it was incremented by 4. It is important to realize that although the branch if equal to zero instruction writes to the program counter register, this is not done at the WriteBack step. This is because the WriteBack step is only for general purpose registers R0 through R31. The program counter itself is not a general-purpose register.",
                    " ",
                    "Knowledge Check 2.9 (Slide 37)",
                    "Answer A",
                    "Remember: A is just a register within the CPU that holds the value of Rs. And Zero is a single-bit flag that is raised to one when the result of an ALU operation is zero.",
                    "Answer B",
                    "Remember: A is just a register within the CPU that holds the value of Rs. And Zero is a single-bit flag that is raised to one when the result of an ALU operation is zero.",
                    "Answer C"
                ]
            },
            {
                "title": "That is correct. A is a temporary register that holds the value of RS, which in our case is FFFC. 0 is a 1 bit flag that holds the result of the operation of the ALU. Since the operation for this instruction is RS minus 0, the result is not 0. Therefore, the zero flag is remaining 0.",
                "data": [
                    "Answer D",
                    "Remember, the value of the Zero flag is raised to one when the result of the last ALU operation is one. The last ALU operation performed was for the comparison of R5 and R0 for the BEQZ instruction.",
                    ""
                ]
            },
            {
                "title": "Instruction Memory 2.10",
                "data": [
                    "Let us now look at the memory step. This step is only used by load and store double instructions in our CPU.  For the load double instruction, the CPU sends the memory address to the memory controller in order to obtain the read the data. Since the address of the data was already calculated during the execute step, the CPU can simply make the memory request using the address stored in ALUOut1. When the data is returned from memory, the CPU puts it in the load memory double or LMD register.  LMD is a special purpose register that temporarily holds the value of data obtained from memory loads. ",
                    "",
                    "For the store double instruction, the CPU must write the data from the register Rt to the memory using the address calculated in the execute step. Since Rt is put in the B temporary register during decode stage, the memory step is fairly straightforward for this instruction.",
                    "",
                    "The WriteBack step is only useful for the Load double and the R-format arithmetic and logic operations. This is because only these instructions of our CPU need to write to general-purpose registers. For load double, the CPU takes the content of LMD and writes it into the Rt register. For the R-format instructions, the CPU needs to write the result of the ALU operation in the Rd register. Since the result of the ALU operation was put in ALUOut2 during execute step, the CPU can simply use the content of ALUOut2 to put in Rd.",
                    ""
                ]
            },
            {
                "title": "High-Level Diagram 2.11",
                "data": [
                    "Ok. Now, let us design a high-level state diagram of our small CPU.  The high-level diagram will break down the steps to execute each instruction into states and will show when to transition from one state to the other. ",
                    ""
                ]
            },
            {
                "title": "High-Level Diagram 2.12",
                "data": [
                    "The figure here shows the state diagram of our CPU. Each state in the diagram has a unique numerical ID, where the first state has the numerical ID zero. Each state encompasses an execution step of the processor for a given operation.  State zero and state one of the diagram are common for all instructions of our CPU because the same operations are performed for fetch and decode steps. The diagram starts diverging at the execution step because that is when the operations vary according to the specifics of the instruction. We see that from state 1, many arrows in the diagram that transition from one state to another and they are labeled by specific instructions. This is to indicate which instructions need those states at the end of the arrows. For example, coming from state 1, if the CPU is executing a Load double instruction it has to go to state 2. ",
                    "",
                    "One important thing to note is that the last state of each instruction always transitions to the state zero, which is the instruction fetch state. For example, the last state for the Load double instruction is state 4. Once that is complete, the CPU goes to state 0, which. This allows the processor to execute the next instruction upon completion of the current one. ",
                    "(Processor Pipelining)"
                ]
            },
            {
                "title": "Processor Clock Rate 3.1",
                "data": [
                    "Let us now discuss the performance of this CPU. First, let's go over the concept of CPU clock rate. Usually, when we buy a computer, one of the factors that we look at is the CPU frequency.  The frequency essentially is representative of the clock rate, which defines how fast each operation can be performed. The metric of the clock rate is the clock cycle. To determine out how many seconds a clock cycle last, we simply take the inverse of the CPU frequency. For example, if we have a 1 gigahertz CPU, one clock cycle takes 1 divided by 1 gigahertz which equals 1 Nano second. What this means is that each operation of the CPU takes 1 nano second to perform. In this respect, an operation can be thought of as the work of one state in our CPU high-level diagram.",
                    ""
                ]
            },
            {
                "title": "Processor Clock Rate 3.2",
                "data": [
                    "So if we know that each state takes one clock cycle, to find out how many states it takes to execute a given instruction. Once can simply add up all the states it goes through. Consider for example the load double instruction. Like all other instructions, Load double goes through states 0 and 1 for fetch and decode. Then load double goes to state 2 of the diagram for execute, state 3 for memory, and state 4 for WriteBack. The instruction thus goes to 5 states and thus takes 5 clock cycles. Using this approach, we can calculate how long it would take for our CPU to execute a given code. For example, using this A = B or C code, we use the high-level state diagram to calculate the number of cycles each instruction, then add the number of cycles of the instructions together, and we estimate that our CPU takes 18 cycles to complete this code.",
                    "",
                    "So, how fast is this CPU? Well, considering the fact that modern low-end CPUs such as the ones used in smart phones can execute at least 4 instructions per cycle, our design is not very fast. This is primarily because in our current design, the CPU can execute only one instruction at a time.",
                    ""
                ]
            },
            {
                "title": "Processor Performance 3.3",
                "data": [
                    "Let us consider the following example. Given two instructions load double and double word add, our CPU will do the following. It will first execute the load double instruction. That is, it will go to states 0, 1, 2 3, and 4, requiring 5 cycles to complete.  Then the CPU transitions to state zero to start executing the double word add instruction. Since doubleword add takes 4 clock cycles, the two instructions require 9 cycles to complete. This type of CPU that can only execute one instruction at a time is called an unpipelined CPU. The opposite is called a pipelined CPU.",
                    ""
                ]
            },
            {
                "title": "Processor Performance 3.4",
                "data": [
                    "So here are several methods that CPU designers use to optimize the performance. For this course, we\u2019ll focus on pipelining. If you want to find out more information about the other methods, click on the following links.",
                    ""
                ]
            },
            {
                "title": "Processor Pipelining 3.5",
                "data": [
                    "Let us discuss pipelining in more detail.  When a CPU is pipelined, it breaks down each execution step into a stage, where each can be used independently by an instruction. In addition, each stage uses buffers, commonly known as latches, to obtain information about what the instruction did in the previous stage. So let's use this example to illustrate the benefits of pipelining. When one instruction starts executing, it first goes in the execution, instruction fetch stage in the pipeline. Just like the states in the high-level diagram of the CPU, each pipeline stage takes one clock cycle to complete.  ",
                    "",
                    "When the instruction completes the fetch stage, it goes to the decode stage of the pipeline in cycle 2.  This means the fetch stage is free at that cycle because no instruction is currently using it. The pipelined CPU can thus send another instruction in the fetch stage to begin its execution. At this clock cycle 2, there are multiple instructions being executed at the same time. Hence we have what is called instruction level parallelism or ILP. At clock cycle 3, the CPU sends the first instruction to the execute stage, the second instruction to the decode stage, and the third instruction to the fetch stage. Using this approach, the CPU can have one instruction being executed in each stage at all times. However, note that we cannot have multiple instructions in the same stage at the same time. This is done only by Superscalar CPUs. Right now, we only have a pipelined CPU. Using pipelining method, our code can take 7 cycles instead of 18 cycles, doubling its performance.",
                    "",
                    "One interesting note about pipelining is that some instructions may need to go to pipeline stages that they don\u2019t use. For example, the double word instruction doesn\u2019t need the memory stage, because it doesn\u2019t read or write data. But since, it must go to the writeback stage to write the result in the register, it has to go through the ",
                    "Memory. This is because in the pipelined design, the CPU simply moves an instruction from one stage of the pipeline to the next stage until it completes.",
                    " "
                ]
            },
            {
                "title": "Hazards 3.6",
                "data": [
                    "The main issues with a pipelined CPU are conflicts that it encounters when executing multiple instructions simultaneously.  These conflicts are commonly known as hazards and are categorized in three groups: structural hazards, data hazards, and branch hazards.  Structural hazards occur when multiple instructions need to access the same CPU resource at the same time. Data hazards occur when an instruction needs to read a data that is being modified by another instruction. And branch hazards occur when a branch instruction is being executed and the CPU needs to know if the branch is taken or not before it fetches the next instruction.",
                    ""
                ]
            },
            {
                "title": "Structural Hazards 3.7",
                "data": [
                    "Let us take a look at structural hazards in more detail. Consider the code here. ",
                    "Remember that in our CPU, there is only one ALU. That same ALU is used by the fetch stage of the pipeline to increment the program counter by four and is used by the execute stage for operations. In this example, at clock cycle 3, the first instruction is in the execute stage and the third instruction is in the fetch stage. Therefore, both instructions need to use the ALU at the same cycle. Here is another example of structural hazard. Let us assume our CPU has only one memory port. The same port is used by the instruction fetch stage and by the memory stage. In this example, the first instruction needs to use the memory port at cycle 4 in order to load the data from memory and the fourth instruction needs to use the port at cycle 4 to fetch the instruction.",
                    "",
                    "The main way to mitigate structural hazards is by replicating resources. Therefore, we can add two ALUs in our CPU, one for the instruction fetch stage to increment the counter, the program counter. And another in the execute stage to perform the functionalities of the instructions. Note that structural hazards rarely occur in modern processors because they have several replications of resources for different stages.",
                    ""
                ]
            },
            {
                "title": "Data Hazards 3.8",
                "data": [
                    "Data hazards are relevant for instructions that have dependency issues. Let us use this example to illustrate such dependencies. At clock cycle 5, the third instruction reads the value of R10 because it needs it for its double word OR instruction. However, the correct value of R10 is written at clock cycle 6 by the second instruction, that correct data is not available when the OR instruction needs at cycle 5. This is known as a read after write data hazard.   ",
                    "",
                    "Note that although we use data in register to illustrate data hazards, they can also use for data that is in memory. One way to overcome data hazards is via data forwarding. The basic idea of forwarding is to send data to the execute stage as soon it is available in the CPU. Using our example, the value of R10 is available in the CPU at clock cycle 5, because that is when the memory accesses and puts it in the load memory data register. Therefore, the CPU can forward the data from that load memory data register to the ALU in the execute stage to guarantee that the correct value of R10 is used for the doubleword OR operation.",
                    "",
                    "Another approach to mitigate data hazards is by instruction re-ordering. The basic idea here is for the compiler to find independent instructions to put between dependent ones to avoid data hazards. This can be done as long as the re-ordering does not modify the correctness of the code. Consider the following example. The instruction at address C8 depends on the instruction at address C4 because the instruction at address C4 first writes to R10 then the one at C8 reads R10. We also observe that the two instructions at addresses D0 and D4 did not depend on any of the instructions between C4 and D0. Therefore, the result of the previous instructions will not affect them if we move it up in the code. We can thus move the independent instructions at D0 and D4 in between the dependent ones. This creates two clock cycles extra between the instruction that writes to R10 and the one that reads from R10. Therefore, when the instruction that reads R10 is doing so at the decode stage, R10 is at least being written to at that same cycle. ",
                    "",
                    "We know that since the compiler is responsible for doing this re-ordering work, the addresses of the instructions that are re-ordered are modified to reflect where they are in the code. We also note that this instruction re-ordering is not the same as out-of-order execution. Out-of-order execution is an additional re-ordering that is done dynamically by the processor, not by the compiler.",
                    ""
                ]
            },
            {
                "title": "Branch Hazards 3.9",
                "data": [
                    "Branch hazards occur when there is a branch instruction the CPU needs to fetch the next instruction in the pipeline.  Since the result of the branch condition and the target address of the branch will not be available until the execute stage, the CPU does not what to fetch. Consider the example here. At cycle 4, the CPU fetches the instruction at address E0. In the next cycle, that instruction moves to the decode stage.  To keep the pipeline busy, the CPU needs to fetch a new instruction at that same cycle. However, the CPU does not yet know if the branch is taken or what that target address is in that case. Although we have described the branch hazards for conditional branches, the same issue occurs for unconditional jump instructions because the target address is calculated at the execute stage of the pipeline.",
                    "",
                    "There are several ways to overcome branch hazards. The most na\u00efve way is to simply stall the CPU execution until the branch instruction goes to the execute stage to calculate the target address. This is known as a pipeline bubble. In fact, pipeline bubble can be used to mitigate just about every hazard. The issue with pipeline bubble is that it does not optimize the use of the pipeline. If we have to always stall due to hazards, then the benefits of pipelining are limited. Another approach to mitigate branch hazards is for the compiler to add a branch delay slot after each branch instruction. A branch delay slot is essentially an instruction within the program code that can be executed whether the branch is taken or not. This is similar to the instruction reordering approach.",
                    " ",
                    "The most common approach to mitigate branch hazards, however, is by using branch prediction. The basic idea here is to predict if the branch is taken or not based on past history. The CPU has a table to keep the history of each branch instruction. For example, for each branch instruction, the table holds its last target address and whether the branch was taken the last few times it was executed.  The CPU then uses that pattern of the last few times the branch was executed to predict if it will be taken and to provide the CPU the target address at the fetch stage. Branch predictors are over 99% accurate in modern processors. ",
                    "",
                    "Knowledge Check 3.10 (Slide 59)",
                    "Answer A",
                    "Remember. Since only one instruction uses the CPU at a time, we don't get resource conflicts or structural hazards in a unpipelined CPU. Moreover, since each instruction completes before the next one is fetched, we don\u2019t get dependency issues or data hazards, as well as branch taken/not taken issues or branch hazards. So there are no branch hazards in an unpipelined CPU.",
                    "Answer B",
                    "That is correct. Pipelining only allows one instruction in a pipeline stage at a time.  Therefore, we can't have 2 instructions in the decode stage at the same time.",
                    "Answer C",
                    "Remember, in an unpipelined CPU, the instruction must complete all of its steps before the CPU fetches the next one. Therefore, only one instruction can use the CPU at a time.",
                    "Answer D",
                    "To maximize the throughput of the pipelined CPU, it must always have a new instruction in the fetch stage. The only way that occurs is if the instructions in the other stages are moving along. This way, there is an instruction at each stage of the pipeline at all times.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 21,
        "module_name": "Computer Organization Part 2",
        "file_name": "Module 21 Computer Organization Part 2.docx",
        "transcript": [
            {
                "title": "Outline 1.2",
                "data": [
                    "Hi everyone, today we will cover the second part of the computer organization section of this course.",
                    "We will focus on the memory hierarchy, which is one of the most important aspects of CPU design. We will discuss the need for caches, which are smaller memory components between the CPU and the main memory, how they are organized and how they facilitate faster access to code and data during execution",
                    ""
                ]
            },
            {
                "title": "Memory Hierarchy 1.3",
                "data": [
                    "So far, we have assumed that every memory access that the CPU makes takes one clock cycle. That is, each time an instruction is fetched, or a data is written to or read from memory, it only requires one cycle. However, in reality, each memory access takes several cycles. This is because of two reasons. The first and most important reason is that main memory is generally implemented as DRAM, which stands for dynamic random access memory. In this instance, dynamic means that the data needs to be refreshed from time to time, in order to keep it available in memory. This refreshing delays the access time of data because it may need to be refreshed before it is brought to the CPU. The second reason is that the main memory is so large, usually in the gigabyte range, that it has to be put off the CPU chip, causing it to run at a lower frequency than the CPU itself. Therefore, the main memory is running at a slower speed than the CPU, causing additional delays. ",
                    "",
                    "In many cases, access to main memory can take as many as 100 cycles for each access. Therefore, if we consider a simple pipelined CPU, each instruction fetch stage requires 100 cycles. Similarly, each memory stage where the instruction reads from memory or writes to memory also requires 100 cycles. In this small example, we can see how the execution time goes from 7 to at least 700 cycles when we consider the real access delay of memory. ",
                    ""
                ]
            },
            {
                "title": "Caches 1.4",
                "data": [
                    "The primary approach to overcome the main memory access delay is by adding caches. A cache is another memory component that is put between the main memory and the processor core and its role is to hold instructions and data that the CPU often uses. So what makes a cache different from DRAM? First off, a cache is implemented as SRAM instead of DRAM. The S in SRAM stands for static. This means that there is no need to refresh the main memory, which makes it faster to read from and write to. However, SRAM requires more space to hold one bit of data than DRAM. Usually, SRAM can require up to 8 times more area to store one bit than a DRAM. So, although it consumes less power, SRAM takes more area. As a result, we can\u2019t have gigabytes worth of SRAM. Usually, an SRAM cache is in the range of 2KB to 2-MB. Given that a cache can be in the kilobyte size, it is small enough that it can be put on the same chip as the CPU without it consuming all of the power budget. So with the benefits of using static RAM and being able to put the cache on the same chip as the CPU, the access delay of a cache can be between 1 and 12 cycles depending on the size and other factors that we will discuss. Now someone may have the intuition to simply use SRAM for main memory instead of DRAM. But remember, SRAM can require up to 8 more area than DRAM. Therefore, you cannot have a large 8 GB main memory as SRAM because it will take too much space on your computer motherboard. "
                ]
            },
            {
                "title": "Caches 1.5",
                "data": [
                    "So this is a generalized view of the memory hierarchy that incorporates the cache. This memory hierarchy is representative of the hierarchy found in a typical laptop. So the idea is the following: starting from the bottom of the triangle, the higher you go, the less is the access delay, but the smaller the memory gets. Therefore, we get a trade-off between speed and size of the memory. ",
                    "",
                    "At the lowest level, we have the hard disk, which is nowadays in the range of terabytes and takes millions of cycles to access. To put it in perspective, if we have a standard laptop, it takes 0.5 milliseconds to access the disk, which is pretty slow by processor performance standards.  At the next level, we have the main memory, which can up to 8 gigabyte of DRAM nowadays. A typical access to DRAM takes several thousands of cycles. Next we have the caches, which are divided as levels. Typically, the processor of a laptop has two levels of caches: Level 2 or L2 is usually in the range of 128 kilobyte to 2 megabyte, can be implemented on the CPU chip or off, and can take up to 12 cycles to access. Level 1 cache or L1 cache is on the same chip as the CPU. An L1 cache can range between 2 kilobyte and 32 kilobytes can take between 1 and 4 cycles to access. In most processors, there are two level one caches, and instruction level 1 cache that is used for the instruction fetches of the CPU pipeline, and a data level 1 cache that is for the memory stage of the pipeline. The upmost level of the memory hierarchy is the general-purpose registers, which are part of the CPU and take one cycle to access",
                    "",
                    "So with this illustration, we can imagine how instructions are moved across the different levels of the memory hierarchy. When we start an application, its binary and its static data are copied from the hard disk to the main memory. When the application starts executing, code and data are copied from the main memory to the caches. But since the caches are significantly smaller than the main memory, data and instructions have to be moved back and forth between the caches and main memory in order get the correct ones in the cache for execution"
                ]
            },
            {
                "title": "DRAM and Cache Blocks 1.6",
                "data": [
                    "So now let us look at how caches are organized and how data and instructions are moved from the main memory to the cache. Let us first go over some basics. First, the main memory is organized into blocks, each block being of the same size.  The CPU designer can configure the memory blocks to be 16, 32, or 64 bytes. Each cache in the CPU is also organized into blocks that are the same sizes as the main memory blocks. But since the size of a cache is much smaller than that of the main memory, there are significantly less cache blocks than there are main memory blocks. For example, if we have a memory hierarchy with 16-byte blocks, a 2 gigabyte main memory will have over 1 million blocks, whereas a 32 kilobyte cache has a little bit more than 2 thousand blocks.  So the key question is how does the CPU move data and instructions between the caches and the main memory? ",
                    "",
                    "Memory Hierarchy \u2013 Locality Principles 1.6.1 (Temporal Locality)",
                    "For that the cache uses two locality principles known as temporal and spatial locality principles of temporal and spatial locality.  Temporal locality basically states that if a data or instructions is used once, it is likely to be used again soon. Therefore, once an instruction or data is brought into the cache, temporal locality suggests to keep it the cache as long as possible. One of the best examples of temporal locality is loop code. When we go in a loop once, we are very likely to go through it again. Therefore, it is best to keep the loop code in the cache, rather than overwrite it or move it back in the main memory. ",
                    "",
                    "Memory Hierarchy \u2013 Locality Principles 1.6.2 (Spatial Locality)",
                    "Spatial locality states that if a data or instruction is used, then the instruction or data next to it is also likely to be accessed soon.  The clearest example to illustrate spatial locality is with arrays; once you access the elements in the one cell of an array, you are likely to access the elements in the next cells. Using this spatial locality, when a data or instruction is accessed, instead of bringing only data or instruction from the memory to the cache, the whole block that holds this instruction or data is brought in. Consider the example here. We have a code that accumulates the numbers of an array. Let us assume that we have 32-byte cache and DRAM blocks and the elements of the array are 8 bytes.  Once we go to the main memory to get eight bytes for the first elements of the array, we bring the 8 bytes for that element as well as the 24 bytes next to it. In this case, if the second, third, and fourth elements of the array are needed, they are already in the cache and are faster to access from there than if we had to go back to the main memory to get them."
                ]
            },
            {
                "title": "Cache Configuration 1.7",
                "data": [
                    "So spatial locality is a bit easier to implement. This is because, the main thing to consider with that is the size of the cache block. The larger the cache block, the more data or instructions that are next to each other we can bring and the more spatial locality we have. And since the cache block size is the same as the DRAM block size, there is not much the cache designer can do to impact that. On the other hand, temporal locality requires a bit of work. Ideally, what we would like to do is keep all the D RAM blocks in the cache. But we know that is not practical because of the size differences. So the CPU designer has to make several design choices in the cache in order to maintain temporal locality as much as possible. For that, the designer uses the following parameters: the associativity of the cache to determine which locations in the cache can a D RAM block be moved to, the replacement policy to determine when to evict a block from the cache, and the write policy which determines how to synchronize the data between the cache and the D RAM.  We will discuss how each parameter impacts not only the temporal locality but also the delay at which the cache is accessed",
                    "",
                    "Knowledge Check 1.8 (Slide 11)",
                    "Answer A",
                    "Since a DRAM block can be as large as 64 bytes, and each instruction is only 4 bytes, then a DRAM block can have up to 16 instructions",
                    "Answer B",
                    "A cache block is the same size as that of a DRAM block. Therefore, a cache block can hold only one DRAM block at a time.",
                    "Answer C",
                    "The furthest memory from the CPU is the hard disk and it is the biggest size. The closest memory to the CPU is the set of general-purpose registers and it is the smallest size",
                    "Answer D",
                    "That is correct. The general-purpose registers are the closest to memory and they only need one cycle to access.",
                    ""
                ]
            },
            {
                "title": "Direct Map 1.9",
                "data": [
                    "So let us discuss cache associativity. Cache associativity dictates where in the cache can a DRAM block be copied into. Associativity is the main factor that determines the trade-off between temporal locality and access time of the data in the cache. There are generally three associativity methods:  one is direct mapped, the other is fully associative, and the last one is set associative",
                    "",
                    "In a direct map cache, there is a one-to-one mapping between the cache blocks and the D RAM blocks. In other words, a D RAM block can be moved to only one specific cache block. We use this color coding to indicate direct mapping. Blue blocks in the D RAM can only go the one blue block in the cache. Yellow blocks in the D RAM can only go the yellow block in the cache, so on and so forth. The main benefit of the direct map cache is it is easier and faster to search the cache. This is because, since a D RAM block can be moved to only one specific cache block, we can directly go to the exact cache block to look for the data. Usually, the way that is done is by using the index number of the cache block. In this example we have here, when searching for the instruction or data in a D RAM block 0 or 4, the cache controller simply goes to cache block 0 to get the instruction or data because that is the only place in the cache that D RAM blue blocks can go to in the cache. ",
                    "",
                    "However, direct map doesn\u2019t provide good temporal locality. This is because the blue cache block may have to be evicted several times if another blue D RAM block needs to be brought onto the cache. When we say eviction we essentially mean that cache block has to overwritten or replaced with new content coming from the D RAM. Since direct map only allows one location to put all the blue D RAM blocks, if the blue cache block is busy, then we have to kick the current content of the blue cache block out. If we end up needing it again, we have to go back to the D RAM to get, causing more delay. In summary, direct mapping provides very good access speed because it is easy to search, but has bad temporal locality."
                ]
            },
            {
                "title": "Fully Associativity 1.10",
                "data": [
                    "The next cache associativity we will discuss is full associativity. In a fully-associative cache, a D RAM block can go into any cache block that is unoccupied. In the figure, the blue D RAM block can go to any cache block that currently doesn\u2019t have data or instruction. So the fully-associative cache is the opposite of direct map. It tries to keep the content in the cache for as long as possible by allowing it to be put anywhere that is free. Therefore, fully-associative cache has very good temporal locality. However, since the data or instruction can be anywhere in the cache, we may have to look to the whole cache to find it. If we look at the cache as an array of blocks, it may take several cycles to get the data or instructions, leading to more access delay. ",
                    "Knowledge Check 1.11 (Slide 19)",
                    "Answer A",
                    "Remember, in a direct-mapped cache, a DRAM block can only go to one location in the cache. So there is no need to search for the whole cache, one can simply go to that one location. Thus it takes one clock cycle to search the cache",
                    "Answer B",
                    "That is correct. In a fully associative cache, a DRAM block can go to any location in the cache. Therefore, when searching the cache, we have to search all possible locations to find the data. ",
                    "Answer C",
                    "A direct-mapped cache uses a one-to-one approach, where there is only one location in the cache that a DRAM block can go.",
                    "Answer D",
                    "A fully-associative cache uses a one-to-any policy, where a DRAM block can go to any available cache block.",
                    ""
                ]
            },
            {
                "title": "So we see how the two extremes of cache associativity work. What we want to do is to get the best from both extremes. This is where set associativity comes in. In a set associative cache, the cache is broken down into sets of N equal blocks. In the figure we have here, the cache is broken down into sets of 2 blocks. The idea is to make the cache direct-mapped with respect to the sets, but fully associative within a given set. Here is what we mean; each DRAM block can only go to a specific cache set. For example, the D RAM blocks in blue can only go the set 0 of the cache, and all D RAM blocks in yellow can only go cache set 1. In this respect, the cache is direct-mapped with respect to which set the D RAM blocks can go to. However, within a set, a D RAM block can go to any free cache block of that set. For example, with 2 blocks in each set, a blue D RAM block can go to any block that is free in set 0. Similarly, a yellow D RAM block can go to any block in set 1.",
                "data": [
                    "",
                    "So with this hybrid design, the set associativity principle attempts to find the middle ground between direct mapped and fully-associative. Though we use 2 blocks per set in our example, this number can be different. Usually, a set-associative cache in modern processors has 4 or 8 blocks per set. The general label for a set associative cache is N-way set associative, where N is the number of blocks per set.",
                    ""
                ]
            },
            {
                "title": "Cache Block Replacement Policy 1.13",
                "data": [
                    "Let us now discuss the replacement policy of a cache. The goal of replacement policy is to ",
                    "try to keep the block in the cache for as long as possible based on the probability that it will be used again in the near future. This is done to improve temporal locality.  ",
                    "",
                    "If we have a direct-mapped, then there is not much of an option. Since blue D RAM blocks can only go to only one cache block, if a new blue D RAM block is being brought into the cache, then we have to remove the current content cache block 0. So direct mapping doesn\u2019t provide very good options for replacement. This is another reason why it is not ideal for temporal locality.",
                    "",
                    "If we have a fully-associative cache, things are a little bit different. This is because in a fully-associative cache, we have options. Remember, in a fully-associative cache, a D RAM block can move anywhere in the cache. If a block is free in the cache, then we can always move the new D RAM block to that free location. So there is nothing to evict in that current case. Let us consider the worst case that all the blocks of the fully-associative cache are occupied, and we must evict something. ",
                    "",
                    "The most common replacement policy is called the least recently used policy, or LRU policy. The idea behind the LRU policy is to evict the cache block that is the oldest to have been accessed.  The intuition here is that if the content of a cache have not been accessed recently, then they are unlikely to be accessed again in the near future. So we can evict it. On the other hand, if the contents of a cache block have been accessed recently, they will likely be used again soon. So it is best to keep it in the cache to maintain temporal locality. To determine the recency of use, each cache block has what are called age bits. The age bits of a cache block work as a time clock that indicates the last time a cache block was accessed. Whenever a cache block is accessed, its age bits are updated to represent the current time of accessed. ",
                    "",
                    "So in our example, we have a fully associative cache and all cache blocks are occupied. So the way the LRU policy works is that it first reads the age bits of all cache blocks and find cache block with the lowest value age bits. Remember, the least recently used block will have the smallest value in its age bits. This oldest block is chosen for replacement. This way, blocks that have been used recently can stay in the cache because they are more likely to be used again soon according to temporal locality. ",
                    "",
                    "So in this example, the least recently used cache block is the last one because its age bits are the smallest when we convert all the age bits to decimal values. So that cache block would be selected for eviction. During the eviction, the content of the new D RAM block is brought into the cache block and the age bits of this block are updated to indicate that it was recently used. As we see, now this cache block is the most recent one to have been accessed in the cache because its age bits have the highest value. The LRU policy also works for set associative caches. But remember, in a set-associative cache, a D RAM block can only go to a specific set within a cache. So consider the example of a 2-way set associative cache.  A new D RAM block needs to be brought up to the cache. Since this D RAM block is blue, it can only go to set 0 of the cache. Since both blocks in that set are occupied, one of them must be evicted. So using the LRU policy, the cache controller reads the age bits of the two cache blocks and notices that the first one is the oldest used. The content of that cache block is thus evicted, the new D RAM block is brought in, and the age bits of the block are updated to reflect the current time.",
                    "",
                    "Although we have only discussed LRU policy, there are other common approaches including least frequently used, which uses counters to keep track how often caches are used, random replacement, which randomly chooses a block to evict, and several hybrid methods that dynamically adjust between least recently used and least frequently used based on the performance of the cache.",
                    "Knowledge Check 1.14 (Slide 32)",
                    "Answer A",
                    "That is correct. In a fully associative cache, the new DRAM block can go anywhere in the cache as long as there is a free block. So regardless of the replacement policy, if there is a cache block that is available, the DRAM block is put in that available cache block. In our example, the 32nd block is free",
                    "Answer B",
                    "Remember, in a fully associative cache, a cache block is replaced only if all the cache blocks are occupied. If there is a free block, we can put it in there. ",
                    "Answer C",
                    "Remember, in a fully associative cache, a cache block is replaced only if all the cache blocks are occupied. If there is a free block, we can put it in there. ",
                    "Answer D",
                    "Remember, in a fully associative cache, a cache block is replaced only if all the cache blocks are occupied. If there is a free block, we can put it in there. "
                ]
            },
            {
                "title": "Cache Write Policy 1.15",
                "data": [
                    "The next aspect of cache design we will look at is the write policy. Though cache policy has some impact on temporal locality, it is primarily done to synchronize the data in the cache with that in main memory. For example, when we have a store instruction, do we write the data in the cache only, in the D RAM only, or in both?  The decision on which to select depends on the bandwidth limitations of the memory, the amount of energy it takes to write to memory, and who else needs the data. For example, if we have a processor for a low-power device like a smartphone. Its memory is likely to have limited bandwidth, so it may take a lot of time and energy to always write to the D RAM for every store instruction. However, if the data that we are writing needs to be used by a co-processor like a graphics processor, then it is best to have it in D RAM where the co-processor can access it. There are two common write policies: write-through and write-back. The basic idea of write-through is to write the data both to cache and the D RAM on a store instruction. This keeps both the cache and D RAM synchronized but consumes more energy. In write-back cache, the data is written to the cache only on a store instruction. The data is not written into the D RAM until the cache block has to be evicted according to its replacement policy.",
                    ""
                ]
            },
            {
                "title": "WriteBack Policy 1.16",
                "data": [
                    "If it worth emphasizing the write-back policy of a cache and the write-back stage of a CPU pipeline. The write-back policy of a cache deals with synchronizing data between the cache and the D RAM. Whereas, the write-back stage of a CPU is for writing registers at the end of instruction execution. So how is the data in the write-back cache synchronized with the DRAM? ",
                    "",
                    "The Write-Back policy synchronizes the DRAM when it has to evict the cache block that a store instruction was performed on. Let us take a look at an example. Assume we have 2-way set associative LRU cache with the write-back policy. Each block of a write-back cache has what we call a dirty bit. The role of the dirty bit is to indicate that a store instruction has been performed on the block. So on a store instruction, the data is written into the cache block only and the dirty bit for that cache block is set to one. In addition, the age bits of the block are updated to reflect the current time. So in our example, the last block of the cache is being written to on the store instruction. ",
                    "",
                    "Later in the execution, a new D RAM block needs to be brought into the cache. Since the cache uses a 2-way set associative policy, we go to the set only set that the new DR AM block can be moved to. The new D RAM block is yellow, so it can only go to set 1 in the cache. The cache controller realizes that all blocks in the set are occupied, so one of them must be evicted.  Since the cache replacement policy is LRU, the cache controller looks at the age bits of the two blocks in set 1. The last block is the oldest and thus must be evicted. Before evicting it, the cache controller checks its dirty bit. If the dirty bit is one, this means that a store instruction was performed on that block and the DRAM doesn\u2019t have the new data. So the cache controller writes the data of that block in the DRAM to updated the DRAM and clears the dirty bit. Then, the new block can be brought in that cache.",
                    "",
                    "In this example here, the content of the dirty cache block is written to the D RAM to synchronize it and the dirty bit for that cache is clear. At this point, the D RAM has the must up to date information for the block. Then the new DRAM block is brought in the recently evicted cache block and its age bits are updated to reflect the current time.",
                    ""
                ]
            },
            {
                "title": "Cache Addressing 1.17",
                "data": [
                    "Ok. So we have covered the different parameters for cache organization. Now let us see how exactly the cache is accessed when the CPU gives it an address during the instruction fetch stage or during the memory stage.  This figure here shows how the cache controller breaks down a physical memory address into different fields to find where in the cache is the data or instruction associated to that address is. All of these field are in bits. The rightmost field of the address is the cache block offset. We will explain the role of the cache block offset later. The middle field of the address is the cache block number if the cache is direct mapped, or the cache set number if the cache is set associative. Therefore, this field indicates which cache block number or set block number that the content of this physical address must be in if it in the cache. The last field is the cache, address, is the cache tag. We will explain the role of the cache tag later. ",
                    "",
                    "The D RAM block number is the concatenation of the cache tag and either the cache set number or the cache block number, depending on the associativity of the cache. Here we see the different formulas to obtain the size of the different fields. Note that all results of the formulas are in bits. Let us consider an example. Assume that we have a memory hierarchy with 16-byte blocks, a 16-megabyte D RAM and with Level 1 caches. Each Level 1 cache is 16 kilobyte and is 2-way set associative. Based on this configuration, we can calculate use our formulas to calculate the size of each field for the physical addresses. Note that here when we say physical address what we mean the location of the data or instruction in D RAM. Also note that it is simpler to calculate the size of the fields by using the two to the power notation. For example, 16 megabyte is 2 to the power of 24, therefore, the address is 24 bits. "
                ]
            },
            {
                "title": "Cache Addressing 1.18",
                "data": [
                    "Now let us now see the role of the block offset and the cache tag fields in the physical address. Remember that each D RAM or cache block is at least 16 bytes. In MIPS64, an instruction is four bytes and an instruction in. So each block can have four instructions. The role of the cache block offset field is to indicate which of the four  instructions in the block to access for a given address. In case of a data access, each data can be at most 8 bytes in MIPS64.  So a cache block of 16 bytes can up to two double words. So the goal of the offset is to indicate which of the two double words the address is requesting",
                    "",
                    "Let us see the role of the cache tag.  Assume we have a 16 megabyte D RAM and a 16 kilobyte cache with a 2-way set associative. Now consider the following addresses: 001C and 004018. When we break the addresses into the different fields, we see that they both have same cache set 0. This means, when accessing these addresses, the cache controller will go to same cache set 1 to find. However, they have different D RAM blocks, which makes sense because they are different addresses. So, the role of the cache tag is to determine which D RAM block that the content of a cache block belongs to. So, if we concatenate the cache tag and the cache set, we get the D RAM block. So, when searching for a data in the cache, the cache controller has to also compare the cache tags to see if it is getting the correct instruction or data. We will see an illustration of this later."
                ]
            },
            {
                "title": "Cache Usage During Execution 1.19",
                "data": [
                    "So let us take a look at how the cache is accessed during the instruction fetch and the memory stages of the CPU pipeline. Let us assume we have 16 megabyte DRAM, and a 16 kilobye, 2-way set associative cache with LRU replacement and WriteBack policy. In addition to the cache age bits and the dirty bits that we discussed before, each cache block has a valid bit and cache tag bits. The valid bit indicates that there is a valid content in the cache block. So initially the valid bit is zero and when something is brought into the cache the first time, the valid bit for that cache block is set to one. The cache tag bits hold the value for the cache tag field for the D RAM address of the data or instruction in that cache block. We will illustrate how the cache controller goes about searching the cache and updating it given an address. By default, when the CPU starts, all the valid bits are zero because there is nothing in the cache. This is known as a cold start. For simplicity, we assume this is the L1 data cache. So it only handles requests for data accesses at the memory stage of the pipeline",
                    "",
                    "At some point in execution, there is a Load double instruction where the memory address is 001C. So the CPU calculates the target address and gives it to the cache controller. The cache controller then breaks down the address into the different fields to find out if the requested data is in the cache. Here we see that the cache controller gets the fields and determines that if the data for this load double instruction is already in the cache, then it must be in set 1. The cache controller searches for each block in set 1. But since all the valid bits of the blocks in that set are all zero, then there is no data in that cache set. Hence the data is not in the cache and must be brought from the D RAM. This is known as a cache miss. Though we are using the LRU policy, there is nothing to replace because the set is initially empty. So the cache controller simply sends the address to the memory controller to handle the cache miss. The memory controller uses the D RAM block field of the address to find out which block is being requested. Here we see that D RAM block is requested. So the memory controller gets D RAM block 1 and sends it back to the cache controller.",
                    "",
                    "The cache controller then takes the data, adds it to an empty blocks in the set, updates the valid bit to indicate that there is some valid content in that cache block, updates the cache age bits to indicate the recency of access for this block and sets the cache tag bits of that block to the value of the cache tag field of the address. In our case, the cache tag field is zero."
                ]
            },
            {
                "title": "Cache Usage During Execution 1.20",
                "data": [
                    "Later in execution, consider the cache looks like this. In addition, we get a new load double instruction where the address now is 0014.  The CPU sends the address to the cache controller and the latter breaks it down into the different fields.  The controller calculates the set of this address and it realizes it\u2019s one. The cache controller then begins searching for the cache blocks in set 1. For each block in the set, the controller first check the valid bit of that block. If the bit is not one, then there is no valid data, so there is no need to continue with that block. If the valid bit is one, the controller then compares the cache tag field of the requested address to the cache tag bits for that block. If the comparison matches, then the cache block has the data we are looking for. This is because when we combine the cache tag and the cache set, we get the D RAM block. Therefore, if they both match, then the content of the requested D RAM block is in that matching cache block. This is known as a cache hit. When there is a cache hit, the data is brought from the cache to CPU, without the need to go to memory. Hence we get much faster access of the data. Cache hit and cache miss ratios are the main metric of performance of a cache. The goal is to maximize the percentage of hits that we have for all memory accesses that the CPU makes."
                ]
            }
        ]
    },
    {
        "module_number": 22,
        "module_name": "Intro to OS Concepts",
        "file_name": "Module 22 Intro to OS Concepts.docx",
        "transcript": [
            {
                "title": "In this Module 1.2",
                "data": [
                    "It's time to start talking about operating systems and in this module we're going to cover a few things about operating systems and get you prepared for the future modules which cover the rest of operating systems concerns or at least some of the basics of operating systems. What we want to take away from this is a fundamental understanding of how computer operating systems first work and what they're expected to do, and then how we can understand them and utilize them either as programmers or if you want operating systems designers. So, in this module we're going to cover what is an operating system. We're going to cover a little bit history of operating systems, so you can understand how we got to this point today. We're going to talk about preemption and how we do scheduling of different programs that are running on the system. And then I'm going to talk about a basic architecture of an operating system.",
                    ""
                ]
            },
            {
                "title": "What is an OS 1.3",
                "data": [
                    "First, we need a definition of an operating system and what I've come up with as a definition is: a program that controls execution of application programs and acts as an interface between applications and computer hardware. And we're going to delve into that a lot more in later slides. But what I want you to understand here is that the operating system is a piece of software; it's not a component of the of the computer. So contrary to Apple's view when you buy a computer, you don't necessarily buy an operating system and that you can mix and match which operating systems you want on which hardware with some limitations. Now the operating system does interact directly with system hardware and it has to be very closely matched to that system hardware. So, it's not like we can mix and match any operating system with any hardware; there has to be some compatibility concerns there. But if we take a look at the hardware for example that Apple uses on all of their laptops and desktops. It's the exact same hardware as any PC In fact, the CPU is the same, the system bus is the same, the DMA controllers the same, the interrupt controllers the same; all of it is exactly the same as one made by HP or Dell or any other PC manufacturer. ",
                    "",
                    "What makes the huge difference is the operating system, and Apple has designed its operating system only to work with their hardware, but the operating system is a piece of software fundamentally. And it runs on the same processor as the user's program code. So, your programs that you're writing, your \u2018hello world\u2019 applications that you're writing, those are running on the processor which is the same processor as the operating system. So, we don't have more than one processor in the system; let's keep it simple and consider a system that doesn't have multi processors, uni-processor systems what we call it. The operating system has to divide the time between itself running code for the operating system to manage the hardware and manage the execution of the whole system, and actually running your program that you really care about. What the operating system doesn't include is applications. And this is going to be a big difference for you to understand, because we'll talk about it in a later slide. When you go and buy an operating system from Microsoft or something like that what you get is a lot more than just the operating system so, we're going to talk about that. ",
                    "",
                    "But I want you to think of the operating system as the fundamental hard software that goes along with the hardware for the computer. It doesn't have to be directly matched to the exact hardware; there can be some compatibility for multiple operating systems to multiple hardware. But fundamentally we can talk about the operating system being a piece of software that manages this system hardware and runs applications.",
                    ""
                ]
            },
            {
                "title": "Layers of Interaction 1.4",
                "data": [
                    "It's good to have an idea, at least a visual, of what we're talking about where the OS is concerned. So, the thing that you've probably experienced as users of computers and we've all been users of computers basically our whole lives we interact with applications. So, as a user, we are only interacting with the application side of the computer. So, when we look at Visual Studio, or when we look at our Web browser, or when we watch this video we're interacting with an application. And we can control that application; we can choose which website we're viewing, we can type code into Visual Studio, or you can pause and restart this video. Don't pause and restart this video right now. The thing that applications do is they'll interact with the operating system. ",
                    "",
                    "So, in order for the for the application to do what it needs to do, it needs to communicate with the operating system. For example, this video is going to need to accept from the operating system a stream of data from the network, and then process that data and then display it on the screen. So, we're using the operating system both to read the data in as well as produce an image on the screen as well as produce the sound of my voice on the audio. So, the application is going to use services that the operating system provides to get that done. In your \u2018hello world\u2019 programs that you've written before you've opened up files and just the act of opening up a file is an operating system call; it\u2019s what we call a system call. We're asking the operating system for some help. So, as the application programmer, you've interacted with the operating system by calling a function like open and when an open opens the file it was using the service of the operating system. The operating system of course is going to need to interact with system hardware. So, in order to open that file, it's got to read from the hard drive and so the operating system directly communicates with System hardware to get the job done. These are the layers of interaction that you see. So, the user is never directly interacting with the operating system and the operating so the application is never directly interacting with system hardware; everything is insulated.",
                    ""
                ]
            },
            {
                "title": "What You Buy in the Store 1.5",
                "data": [
                    "I want you to consider that when you go to the Comp USA, or I guess Comp USA doesn't exist anymore. If you go to Best Buy or even if you go to the Microsoft store and you buy Windows, what you're buying is not an operating system; you're buying a lot of other stuff in that operating system. In that box comes a CD that is filled with ninety nine percent stuff that has nothing to do with your operating system. The operating system as we're concerned is the kernel, and the kernel in Microsoft Windows for example is about twenty five megabytes.  Now you say what comes on a CD for four gigs when the operating system itself is only twenty five or thirty megabytes, well all the other stuff. ",
                    "",
                    "The extra stuff that you get are web browsers, like Edge (I don't know who's using Edge yet) but let's say Edge, Internet Explorer, text editors, device drivers, really any other application that comes on that CD, calculator, minesweeper, solitaire. those will come with the CD but they have nothing to do with the operating system. Linux on the other hand, for example, you can just download the kernel. If you go to www.kernel.org; you can see that what you can download is just the kernel. It comes with no text editors, it comes with no web browsers, it comes with some device drivers for stuff that supported directly in Linux. But fundamentally the thing that we want to concern ourselves with is this kernel. And the kernel is really only the very small components that, we'll talk about this and in a later slide, but really the just the very small components that are necessary to bring the system up online and start running other programs. And really that's not very much but what you buy in the store is a lot more stuff.",
                    ""
                ]
            },
            {
                "title": "The OS as a Resource Manager 1.6",
                "data": [
                    "If we look at the system, as a whole the computer system, what we can see is that it's a collection of resources. And the resources that we have are memory and CPU time and file handles and network connections and it goes on and on and on and on; those can all be seen as a resource. and the operating system, since it is the primary controller of the entire system the first software that will run, the operating system controls all of those resources. And it needs to allocate those resources to some programs that you as a user decide to run. Now the operating system itself is software. So, it's going to need to use some of those resources; it's going to need some memory for itself, it's certainly going to need to run code so that CPU time, it may need files to open up more connections or may need files to open up more information, whatever has to happen. ",
                    "",
                    "The operating system is a client of itself in a lot of cases. But the applications are really what the users care about, when you as a user start up your machine the forty five seconds or two minutes or however long it takes to boot up your computer and get to the screen where you can finally do something: the desktop. You consider that time wasted; you consider that time that you can't do anything with your computer. And we try to minimize that time as operating systems designers; we try to minimize that time, so that you don't have to go to waste that time, you can use that time for other purposes. For example, sleep mode on either a MAC or a PC, when you close the lid the computer goes to sleep rather than shut down altogether so that when you open the lid again on a laptop, it comes back to life and most immediately. The way that we can do that is actually quite simply the computer still running; it just shuts down all the non-essential services like the screen, and the wireless card and things like that and it has uses a very, very, very small amount of power to keep running to continue keeping the memory active. ",
                    "",
                    "But the point is the resources that we have are going to be allocated to different applications as well as to the operating system. And these resources are limited, so we have to keep track of them and we have to make sure that we're not wasting any of those resources. There's going to be a lot of situations in operating systems we concern ourselves with this in the operating systems design classes here at NYU, but one of the things we take into account is that we might have an algorithm which solves our problem perfectly but we can't use it because it takes way too much time to run that algorithm. And if we did that it would actually slow the system down as a whole. So, even though the algorithm works perfectly and doesn't exactly what we need to do, we can't use it because it just takes too much time to run. So, the operating system has to dole out these resources to various applications as well as to itself, in a studious manner so that it takes care of not wasting the resources as well as allocating them effectively so that the programs can do what they need to do.",
                    ""
                ]
            },
            {
                "title": "Back in the Olden Days 1.7",
                "data": [
                    "So let's take a look back at the old days and see what used to happen, and that's going to help us (bring us) up to speed on what we do today. And what I want to do is dial the clock back to the era when mainframes really dominated the computing industry, and what we're talking about is the late maybe 1950\u2019s, early 1960\u2019s, maybe even into the seventy's. We generally had one computer and by that I don't mean, we as an individual had one computer, I mean the whole company, or the whole university or the whole organization would have one computer for the entire organization. So, computing time was very, very limited and we had to allocate that computing time effectively.",
                    "",
                    "The computer ran only one program at a time; there's only ever one program running on the computer at any given time. We would not flip flop; we would not open Chrome to view a web page, as well as, Excel to start typing things into a worksheet or so on and so forth. We would not have two programs open at the same time; there would be one program doing some complex calculus, because it was almost always math based, doing some complex calculus to try and figure out the solution and when it finished and found that solution it was done and it would move on to another solution, another program. One program had complete access to all of the system resources; it had access to all of system memory, save for a small little area for the operating system. It had access to all of the CPU time. This one program was started, processed all of its data, ran through to completion, and when it ended another program was started.",
                    "",
                    "Now there was always a mainframe operator; it was always a human being who decided which order to run programs. So, this was somebody who if you wanted your program to run, if you knew your program had to run early in the morning, you went to Dunkin Donuts and got a dozen donuts and gave it to the mainframe operator and he pushed your program up a little further up in the stack; make sure that you get that guy on your good side or else your program is going to be the last one to run in the day. When one program finished the operating system had to be ready with the next program. So, the next program had to be ready to be loaded into main memory and start executing almost immediately. The processor was really the limiting factor in these older computers, so you'd bring in everything that was needed for the one program to run and then you'd put that program on the processor and let it do whatever it wanted to do until it said it was finished. ",
                    "",
                    "In order to do that you require some JCL and JCL is what we call Job Control Language. Job Control Language tells the operating system what facilities: what printers, what files, what network connections, anything that the program needed. The JCL would indicate that that program needed it before even the program started. So, imagine opening up Word and when you start Word, it says list all the things that you're going to need for this run of word and you have to say I need these three files, and I need this printer, and I'm going to need this to save, and I'm going to need the spell checker, and the grammar checker and all of that. You had to say that at the beginning of the program, even in some cases you had to say that before the program was compiled; so that when you're creating a program you would have to say what resources were going to be needed. So that the operating system could make sure that all of those resources were loaded and ready to go immediately rather than having to pick and choose. So, if you didn't have JCL and your program goes to open a file, the whole system has to stop and wait until the file is actually open and ready because you're the only program that can run, and you only run\u2026 You only stop when you're when you indicate that you're completely done. So JCL was a way for the operating system to sort of pre-load some of the material. ",
                    "",
                    "And what we do is call this batch multiprogramming; this is called batch multi programming because we're getting things ready in main memory and batching them creating a back of programs. So, when one finished, the next program was right there in the operating system and gets it going immediately. That's the olden days; we don't do that anymore. And the reason that we can't do that anymore; there's a lot of reasons actually, but we don't think this way anymore. We don't we don't think about getting one job done and then moving on to the next job. We'd like to do a lot of jobs at the same time and the other downfall of this environment is the JCL, that when the program starts we might not know everything that we're going to need; it might not be until we do some processing that we finally know what we're going to need. So, this doesn't get done anymore and in just a minute we're going to look at what we're doing today.",
                    ""
                ]
            },
            {
                "title": "Today\u2019s Environment 1.8",
                "data": [
                    "Today we have a lot of processing power and we have a lot of memory: eight gigs, sixteen gigs, thirty-two gigs. I've worked on machines that have one-hundred and ninety-two gigs of memory, not hard drive but memory. and we have enough to run multiple programs at the same time what we really want to do is what's called multitasking. We want to have the ability to open up a web browser, as well as open up Microsoft Word, as well as open up Microsoft Excel, as well as open up anything else that we want to open up at the given time. So, in order to do that the operating system has to become a resource manager, and what we do is allocate resources to the various programs that want to run them. And then the operating system decides which programs can run and which programs, and when they can run. ",
                    "",
                    "The operating system will be responsible for stopping and restarting running programs and this is what we call preemption. Now when we restart a running program, we don't start over from the beginning we start from where we left off. So, you may not have realized this in your \u2018hello world\u2019 programs but your hello world programs didn't start and run straight through to the end; they were stopped and restarted many, many, many times. Not from the beginning just from where we left off. So, it's sort of like the operating system lets the program do a couple of instructions, in reality it's going to be a lot of instructions but let's say a couple of instructions, and then it stops and it saves everything in the registers and saves all the information in the CPU and it goes on to doing something else entirely.",
                    "And when it comes back to decide that your program is going to run again, all it has to do is restore those values to the CPU and let it run. The instruction pointer, sorry\u2026 the instruction register, the program counter, all the AX, BX: all those registers that are in the CPU are going to be saved and restored. So, your program picks up from where you left off; you don't actually have to restart the whole process over from scratch it will just stop and restart. And this is what we call a timesharing system. ",
                    "",
                    "Now the reason that your program stops might be because it wants to stop; it might want to do something that takes a very, very long time. So the operating system says, let's say opening up a file which takes a fairly significant amount of time on the order of let's say fifteen or twenty milliseconds. The operating system will say okay, if you're going to open up that file, there's no reason that you're going to run anymore until I have that file open. So, we'll save your settings your registers in the CPU, we\u2019ll call your state and then we'll run something else. We'll do some other code; we\u2019ll run another program, we'll do anything else. And when your file is loaded and ready to go. We'll bring you back and reload those registers into the CPU and let you run again because now we have your file available. So the operating system does this very, very quickly and it really does this hundreds or even sometimes thousands of times per second inside the system, and in doing so it creates an environment that we call a time sharing system. And it allows us to run very many applications.",
                    "",
                    "So, if you're on a Windows PC right now, you can hit control shift escape and you bring up the Task Manager; you go over to performance and you can see how many programs, how many processes are actually running, and that's probably in the order of between seventy and one-hundred, maybe even more processes that are running. If you're on a Mac, you can go to the activity monitor in the Applications Utilities folder and you can see that same information. So, what we're saying is that in today's environment, going back to the batch multiprogramming days, is something that we absolutely could not do; we don't even think that way anymore. If you were restricted to only running Microsoft Word and you could never get out of Microsoft Word and go into a web browser and go into absolutely anything else or any other program, would you even be able to write a paper and I doubt it very much. But that's the way we worked in old environments; today we don't expect to work that way. So today we're doing multitasking and we're doing time sharing.",
                    ""
                ]
            },
            {
                "title": "Monitoring Running Programs 1.9",
                "data": [
                    "In the modern operating system, we load a lot of programs and they\u2019re load all of the same time all in main memory. And if we take that into a bigger account, you may even run chrome multiple times on the same system. So, now we've got two copies of Chrome loaded into loaded into the operating system; how do we keep track of which version of Chrome is running and really which instance, if you will, of Chrome is running? And the way we do that is called a process and we're going to talk about this extensively; in fact, we have an entire module on processes that's coming up next. But what I need to know understand is that when we load a program into main memory, we create an object (if you will) called a process to keep track of that program to see how long it's been running to record when it's ready to record when it's busy to record when a file opened for this for this process. We keep track of it by creating a process and then we have struck the idea that this is a program and instead it becomes a process. The code for the program can be loaded multiple times or not depending on how we how we implement the operating system, but we can consider that this process is a unique object all to itself and doesn't interact with any other process in the system.",
                    ""
                ]
            },
            {
                "title": "OS Levels 1.10",
                "data": [
                    "I want you to look here at this image that I have which is just list the levels of what we consider the operating system, and this is just a rough diagram; it's not really hard and fast rule. But what we've got on the very, very bottom, the bottom four levels, are not really that concerned operating systems per se except that we have to interact with them. So things like interrupts, procedures, processor instruction sets, and electronic circuits as operating systems designers: we don't get any control over that. Those get dictated by hardware designers, computer engineers, not computer scientists, which are going to design the electronic circuits which are going to specify the processor instruction set, which are going to deal with how we can do procedures small bits of code you know the small items that are built into the processor itself, as well as interrupts that have to occur inside the system. We don't get to control much of that stuff, but we do have to interact with it. ",
                    "",
                    "Now in the primitive processes phase, what we're talking about are very low level pieces of code that are parts of the operating system. And in fact, if you look at those two dark lines that I\u2019ve drawn, levels five six and seven are what we consider what's called a microkernel; that's the minimum that's necessary in order to have a functional operating system. Primitive processes deals mostly with scheduling and resource management. Secondary storage obviously we have to access all the other code and all the other components of the operating system, as well as all the other programs and data so that's a very low level concern. And then we have this concept of virtual memory or even we can call it memory management, which we'll talk about in a later module that has to be managed. So, we're going to need some, some fundamental control over what's in main memory.",
                    "",
                    "So, those three components are absolutely necessary in order to have an operating system. Now in the old days and I'm not talking about back in the mainframe days, but really only a couple decades ago. What we can say is the rest of it would have been included in what we call a macro kernel or a large kernel, so the micro kernel has only the smallest and most essential elements. The macro kernel has everything: communication systems communication, subsystems for getting information into and out of the computer, the file systems, Windows even has some components that are built into the kernel, that are purely for the file system. But file systems concerns in an operating systems like Linux and most Unixes, those are an upper level concern outside of the kernel, devices that we might connect to, directories that are on the file system, any user processes, and the shell, the shell being really what you're interacting with. In Windows, this is called explored dot EXE. So, the shell is a component that the user interacts with everything below that is down to Level five, is a concern of the operating system and everything below that is a concern of system hardware. But I just wanted you to get an idea of what we what we look at, what we use, because each of these levels use services of the levels below them. Virtual memory, for example, stores components of processes on the secondary storage device so the virtual memory system uses components of the secondary storage uses services of secondary storage. And because virtual memory is higher, that means we can use the services they exist already. So, what we do is we provide services to the layers above, to the levels above and then we use the services of the levels below. So, that's the way that this whole interaction works but I want to you have an idea of the core levels, if you will, of the operating system.",
                    ""
                ]
            },
            {
                "title": "The Windows Model 1.11",
                "data": [
                    "I want to show you now, and I hope you don't get too concerned by this image that you're going to see, of what the inside of Microsoft Windows looks like or at least what Microsoft will tell us the Windows model looks like. And I really want you to concentrate on a few items that are below the line, below the solid line. What we're seeing here is the components of the kernel, and in fact you can see that indication that shows you kernel mode versus user mode; user mode being above and kernel mode being below. Now the lowest level of here that we want to consider is the HAL, the hardware abstraction layer. We're going to talk about that in just a minute. But I also want you to see that there are some other components for device drivers as well as video drivers, a few things that have to be built in. There's a virtual memory manager; those components that are the fundamental components of the operating system are down there in kernel mode. Now if you remember back from the computer architecture discussion. There is kernel mode as well as user mode, and we're going to talk about this a lot more in later modules. But kernel mode has complete access to the entire system, whereas user mode has a much more limited access.",
                    "",
                    "So, what I want you to get away and take away from this is that Windows is of course a very complex model, it's been around it's been being developed now for over thirty years, so it's a very complex model. But what I want you to see is that at the very bottom there's only a few layers that directly communicate even inside the operating system with system hardware, and there's good reason for that.",
                    "Inside the kernel mode, we just keep track of processes and other things, most of the work of the Windows kernel is actually done outside of kernel mode. And the reason for that is one of security as well as organization and it's much easier to update things that are outside of the kernel, than things that are inside of the kernel. So, take a look at this image just don't, don\u2019t memorize it don't, don't study it too hard. Just take away some of the major components of it that you can at least keep track of. And understand that modular operating system design is really what we absolutely have to do today, because there are so many things that the operating system has to do just even to run the first program.",
                    ""
                ]
            },
            {
                "title": "The HAL 1.12",
                "data": [
                    "The harder obstruction layer is a magic little layer that Microsoft built in, and to understand why Microsoft even needed the HAL in the first place you have to look back again in history. Sorry, but we're going to dial the clock back again. And go back to the mid, maybe the early 1990\u2019s. In the early 1990\u2019s, Microsoft was not the big player in the market that they are today. If you look today, most computers and I say most I mean almost all computers run Windows, run Microsoft, some version of Microsoft Windows. The latest indications that I have here in 2016 are that Microsoft Windows runs on between 91 and 92 percent of the hardware that's out there. So as far as computers are concerned, Apple's market share is around seven to eight percent, at least those among the numbers that I have today; seventy to eight percent, whereas Microsoft is ninety-one to ninety-two percent. And there's that one-percent/two-percent other which includes Linux and some Unix's. In any case, back in the day, back in the 1990\u2019s, early 1990's, that wasn't the case at all. The operating systems were broken down much more towards about thirty percent was windows, about thirty percent was Mac, and by Mac I mean a completely different hardware architecture, and then there was also thirty percent other things like Unix's in these sorts of hardware. ",
                    "",
                    "And Microsoft wanted an operating system which they could market literally to everybody, and so what they did was they designed the architecture in a modular fashion so that the lowest level, the level that actually communicates with the hardware. Not manages things like running processes and scheduling and dividing up memory and allocating it, not that thing but the code that actually communicates directly with the DMA controller, the interrupt controller, the hard drive, things like this that are very, very, very low level. Microsoft designed them into a component they called the hardware abstraction layer. And the hardware abstraction layer was a set of functions that the kernel could call on to perform tasks on different, on the various hardware. So, rather than directly programming the timer in the system to say: let this process run for twenty milliseconds. The hardware abstraction, the operating system would call a function inside the hardware abstraction layer to say program the timer to let this process run for twenty milliseconds. And the reason that that's essential is because Microsoft can change out the HAL and produce an operating system that can now run on a completely different hardware. Without making any other changes to the operating system, it can change just the functions inside the HAL and those functions will, those features will take effect on the new hardware. ",
                    "",
                    "And Microsoft did this in the early 90\u2019s and produced five different operating systems, now they were all Windows NT 4 0 but they were five different versions of Windows NT 4 0. One for the Intel architecture, one for the Power PC architecture, and there were various other architectures that Microsoft could support. Now over the course of the next five/ten years, all of the other architectures disappeared. Even Apple; now if you go and buy an Apple Macintosh, you're going to buy a MacBook let's call it, let's say a MacBook. You're going to buy something that runs on the Intel architecture, and that means that it could run Windows just as well as it could run MacOS. And that's to our benefit of course because we want to do that we want to do exactly that. But Windows has the support for the HAL which has support for multiple hardware, and when Microsoft, for example, released Windows RT. When it came out, Microsoft said we can now support ARM architecture and the way that they did that was in part changing the HAL. So, any changes that we make to hardware, really only require changing the HAL and that saves us from having to rewrite the whole operating system; it's a very effective tool.",
                    ""
                ]
            },
            {
                "title": "Windows Device Drivers 1.13",
                "data": [
                    "Windows device drivers, or device drivers in general, are kernel layer software that's written by the companies that designed the hardware. Now let\u2019s take for example a video camera that's built into your laptop computer, so you might be looking up at the top of the screen right now you see a little video camera. That video camera was designed by a hardware company and we don't know who designed it; it may be some Apple company or some company that Apple contracted with or perhaps it's Logitech, which is one of the most popular web cam companies perhaps Microsoft contracted with, who knows. Some hardware developer designed a web cam and the manufacturer of your computer incorporated that hardware design into their laptop so that they could use the web cam. Now when the hardware designers designed their web cam, it works differently from one designer to the next. So, Logitech\u2019s commands for taking a picture are not the same as Microsoft's commands for taking a picture, or Apple's commands for taking a picture. Whoever the hardware designer is, they designed the chip so that it takes a picture after it's given a certain command. Now Microsoft doesn't want to have to know all of the commands for all of the hardware manufacturers that could possibly create a web cam, so what they do is they provide a facility for incorporating a piece of software that the hardware designers will develop; software that the hardware designers will develop into the operating system and allow it access to the camera so that it can program the camera to take a picture. Now what happens is Microsoft says if you are a web cam designer you create these functions: Take a picture, take video, turn on light, whatever it is. And if Microsoft calls the take picture function then the code from that device driver does whatever task is necessary in order to take a picture and return it back to the operating system in whatever way that Microsoft is expecting it. So, they define this protocol for communications between the operating system and the device driver. and then the device driver can do whatever it needs to do in order to actually take the picture and return it to the operating system.",
                    "",
                    "Now that works great, except for the fact that these device drivers have complete unfettered access to the entire operating system and what they discovered was that while hardware designers might be very good at designing hardware, They're really not good at writing software. So, in the late one 1990\u2019s and even in the early 2000\u2019s, Microsoft had a real problem with device drivers causing device driver\u2019s code, which remember it\u2019s code, crashing and causing the entire system to become unstable and eventually crashing the whole system in what we call a \u201cBlue Screen of Death.\u201d So what Microsoft did was design or specify a lab\u2026 they created this lab that says: if you're a hardware designer and you want us to certify your drivers, send us your software, send us the source code as well as a piece of hardware, we\u2019ll test it out, make sure it's one hundred percent compatible with Microsoft Windows and it doesn't cause a crash inside the operating system. It can't crash the operating system and as long as everything's working properly and we'll certify your driver and give it back to you with the our stamp of approval. And they call this the Windows Hardware Quality Labs or WHQL.",
                    "",
                    "And nowadays it's nearly impossible to install in Windows ten or any of the later versions of Windows, to install any drivers that are not WHQL certified. You have to go through an enormous process to install those drivers, just for testing purposes. You basically certify that Microsoft, that you know that Microsoft has no knowledge of these drivers, and that it may destabilize your system and cause mass floods and virus, and I'm kidding about that. But you know that you're doing something that's outside of the ordinary. So, the whole idea of the WHQL was that this device driver code had so much access to the system that if it was not written perfectly, it would crash the whole operating system and we don't want that. So, we have avoided the whole \u201cBlue Screen of Death\u201d now by having certified drivers that are certified with WHQL. And I think you can we can all agree anybody who lived through that era of blue screens of death, every day, if not multiple times a day, we found the this is actually a very good idea.",
                    ""
                ]
            },
            {
                "title": "UNIX 1.14",
                "data": [
                    "Unix was designed to be a multi-user, multi-tasking operating system. And what that means is that we are designing an OS which is going to run on a centralized machine and have lots of users; not one person sitting at a keyboard a screen but in fact lots of users accessing this all the same time. And in the era of the mainframe computer, when you didn't ever access the mainframe computer; I remember people telling me stories of writing their programs on punch cards and leaving punch cards in companies for the mainframe operators to load into the mainframe. They never directly interacted with the mainframe the whole idea of Unix was actually very nice because you could directly use the mainframe or directly use the computer. ",
                    "Unix was designed to allow users to manage their own tasks and this was one of the first time sharing systems that came about and that's why it was so popular because it decentralized responsibility. Your users only got a very small portion of the time allowed to run on the system and nobody could take too much time on the system or too much resources, I should say. But it was allowed to access directly the mainframe, and do your work directly on the mainframe and have immediate feedback rather than delayed feedback whenever the mainframe operators cited for your program to run. The other thing about Unix that was great was that it was released into the public domain with open source. When you got UNIX you didn't just get the operating system; you also got the source code. And it was released in the public domain with public licensing, public domain licensing, so that if you made any modifications to the Unix operating system You could re-release it as your own, except that you couldn't charge for it. So, you couldn't take the Unix operating system and make one change and sell it for hundreds of dollars; It wasn't allowed. In fact, all they allow for is what's called a media charge So a few dollars for the cost of the media on which you would distribute the source code, and you had to release the source code as well for whatever changes. ",
                    "",
                    "Now Unix comes in a lot of different flavors comes in AIX, Linux, Solaris, etcetera like that. And some of them are branches of the original UNIX, so they're now public domain which means if they made any if they used any of the original source code of Unix, or any subsequent source code of Unix, they have to be public domain. There are some companies like IBM who designed AIX from scratch to look like Unix, but include none of the source code of Unix so that they could license it and sell it and make money off of it. So, there are some versions like that. And that's how Apple gets away with selling, or what they used to, sell MacOS was because they used BSD, which was a completely new implementation of Unix; it didn't use any of the original Unix. So, Unix has been modified many, many, many, many times over the years, but what we have is this basic idea of a multi-user multi-tasking operating system.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 23,
        "module_name": "Processes and Threads Script",
        "file_name": "Module 23 Processes and Threads Script.docx",
        "transcript": [
            {
                "title": "Introduction 1.2",
                "data": [
                    "In this module, we're going to be talking about processes and threads and what is needed for an operating system to manage the processes and the threads. We're going to take a look at some concurrency issues. We're going to take a look at some deadlocks. And we're going to talk about how to take care of all those issues.",
                    ""
                ]
            },
            {
                "title": "What is a Process? 1.3",
                "data": [
                    "So, the process is made up of the code, data and context. And we're talking about the process being a running program in a system state, and we're gonna talk about what system states mean, in detail, in the next topic. But for now, we'll simply say that when the program starts, it has to have all the information that it needs in order to run; specifically, the code as well as any data it may need initially. And then the operating system needs to keep track of a lot of information about the running program, and that's what we're gonna call that the context. The code all has to be run in sequential memory; you know this, you've done some programming. You've done quite a bit of programming and you're gonna have to recognize that the code starts from point one and proceeds sequentially through the entire program. And so that code needs to be loaded into sequential memory. The process is a way for the operating system to keep track of the state of the running program, and the resources assigned to that running program."
                ]
            },
            {
                "title": "5 State Process Model 1.4",
                "data": [
                    "We'd like to talk about the five state process model, and we're going to be talking about each individual state. Now what is a state? Well a state is one condition that a program might be in and the process is going to spend quite a significant amount of time in each state. So each state has to be recognized and has to be recorded by the operating system. So let's go through the five different states that a process could be in, and you'll have a better understanding of why the process is in each of these states. ",
                    "",
                    "So, the first state is the new state and all processes start out in a new state. Now we have to recognize that the process is going to spend quite a significant amount of time in each of these states. So, why would a process spend a significant amount of time in the new state. We have to think about the process being created and the procedure that we would go through to create a process inside the operating system. And the new state is when that process is being executed by the operating system. Now one thing that we need to recognize is that a state is an atomic element. And if the operating system can complete everything atomically, then there's really no reason to have a state. So, the new state happens because the process is being created and the code for that process, one of the reasons is the code for that process will have to be loaded from a secondary storage device into main memory.",
                    "And that's an IO operation and I operate generally take an enormous amount of time, and we'll see this in a later model, but for now we just say that an IO operation takes a really long time. And the operating system doesn't want to sit in wait for the IO operation to complete; it wants to go on to do other things. But it needs to keep track of where this process is in the act of creation and the way to do that is the new state. So all processes in the new state are in the act of being created and their code being loaded into main memory. ",
                    "",
                    "Once the process finishes loading and can be run, then the process moves not to the running state, unfortunately, but to the ready state. And the ready state is that point where the operating system has all the processes which actually have all their parts ready to go. These are only waiting for a processor to become available so that we can load the process onto the processor and actually let it run. Remember from our hardware discussion, that the processor is the only location where code can actually be run in the system. So, the processes that are on the ready state have everything that they need in order to run but they're not quite running yet because there's no processor available. ",
                    "",
                    "Once the processor becomes available, the process is going to move from the ready state into the running state and then it will actually be running on the processor and doing some real work. This is called a dispatch operation. Once the processor either times out, or the process makes a blocking system call, or the process ends altogether, the process is going to stay in the running state until one of those three things happens and then it's going to move to the ready state, the block state, or the exit state accordingly. Now we already understand what the ready state is we've seen that just a second ago. But what's the block state?",
                    "",
                    "Well what happens if the process makes some sort of request which is going to take a really significant amount of time. The operating system doesn't want to stand and wait, waiting for that process to do what it's asking to do. And so it moves the process into the block state to indicate that this process is no longer ready to run; it is not running but it's not quite done yet. And the process is going to wait in the block state until whatever action it's requested finishes, and then it's going to move from the block state instead back into the ready state. ",
                    "",
                    "And then eventually from the ready state to the running state and from the running state back to the ready state when it times out. And this process will happen from ready to running to blocked, ready to running, to ready to running, and so on and so forth until the process finally finishes. And when it's done the process moves to the exit state. And in the exit state the process is just waiting to be cleaned up and terminated. And there's one key element that has to be returned back to the parent process, which we'll talk about later and linkages, and that is the return value. Remember return zero from int main, well that has to be given back to the parent process. And a process that in the exit state has not yet returned back its element, its return value, to the parent process and that's why we have the five states that we have.",
                    ""
                ]
            },
            {
                "title": "Suspension 1.5",
                "data": [
                    "We also add to the concept of the states with the idea that the process may be suspended. Now suspension is a bit of an odd notion in that the process is sort of still ready to run but it isn't actually running or runnable yet. In suspension, the process will be completely removed from main memory; so, obviously its code can't run because this code isn't loaded in main memory. So, in suspension we completely take the process out of main memory and move it to a secondary storage device or even maybe a tertiary storage device, if we really don't have any memory. Why would we have this? ",
                    "",
                    "Well one of the biggest reasons for having a concept of suspension is that we can free up main memory. If we get to the point where main memory is starting to fill up, with so many processes or with such large processes, that we no longer have a lot of processes that are ready to run we might get into a condition where the process list has nothing on the ready state; everything is in the block state and there's no more memory available to allocate to new processes. So, we aim to free up that memory by suspending one or more processes and this is done by what's known as the medium term scheduling algorithm. And the medium term scheduling algorithm will choose which process to suspend, and then it will take that process and put it on the secondary storage device. Now, this doesn't mean that the process is terminated because we can certainly reload all the code data and context from the secondary storage device back into main memory at a later time and then run that process, just as if nothing ever happened. So it's very possible for any of your programs your \u2018Hello World\u2019 style programs from previous modules, that could have been suspended at some point. We might have suspended the process just simply for debugging purposes; we might have suspended the proper process at the request of the user doing a control Z on a Unix system. ",
                    "",
                    "But the point is that we need to add States to our five state process model to recognize the possibility that the process is suspended. And we actually need to add two different suspended states, because we need to recognize if a process is suspended and ready to run or if it's suspended and blocked for some reason. And the movement into and out of the suspended states is relatively easy. If the process was in the block state than during suspension it would be moved to the block suspended state, and if it was in the ready state during suspension it would be moved to the ready suspended state. If it some point in time in the block suspended state, the event that we were waiting for occurs then the process would be moved from the block suspended state to the ready suspended state. So, we evolve our concept of the process model from a five state process model into a seven state process model, where we add these two suspended states.",
                    ""
                ]
            },
            {
                "title": "Process Image \u2013 The PCB 1.6",
                "data": [
                    "So, the operating system is going to need to record quite a bit of information about the process and in total the process is going to be stored in what we know as a process image. The operating system will keep a table known as the process table with pointers to the various process images for those processes that are running inside the system. And one of the components that are in the process image is what's known as the PCB, or in some cases you might consider the PCB to be the actual process image. It includes all the information that the operating system needs in order to run and manage the process. So, the PCB is a really critical component of the operating system; it's usually implemented as a simple struct with quite a bit of information. ",
                    "",
                    "So, what's in there? Well we've got memory tables; we've got what's allocated to the process. We've got what secondary storage is allocated to the process. We've got IO tables, which are what devices the process has access to and where it is inside those devices. We've got file tables with all the file handles, so anytime we opened a file in our previous programs that we create an entry in the file table and that was stored inside the PCB in the process image. And then we've got stack pointers, and all of these components make up the process image, which includes all the code data and of course this is the context.",
                    ""
                ]
            },
            {
                "title": "Contents of a PCB 1.7",
                "data": [
                    "So, if we take a deeper look into the contents of the PCB, what we're going to see is a lot of the context information for the process. We're going to see things like the numeric identifiers. The numeric identifiers that we have just a few examples of, are the process identifier. Now that's one process identifier is assigned to each process in the system and this is a simple word, it's only sixteen bytes, sorry\u2026 sixteen bits large and the PID has to be unique for all the processes. So, you're starting to come to the idea that the maximum number of processes in the system is 65,535 because that's the maximum number of process identifiers that we have.",
                    "",
                    "We're also going to keep track of the parent process identifier. Now each process is going to be created by a parent, and the parent has certain responsibilities and rights over that child process. The parent can terminate the child. The parent can receive the child's information when the child finishes; the return zero value so to speak. So, the parent process identifier needs to be recorded as part of the information that we're going to use to manage that process.",
                    "",
                    "We're going to record the user identifier, of the person who started that process. The UID is important because it leads to the security perimeter restrictions that are placed onto that program. What can this program do is based on who activated this process. And we're going to talk about registers, inside the PCB; we have a number of registers, now those are for when the process is either in the blocked or the ready state. When it's in the running state remember the process is actually running, so the registers are constantly changing. But those registers have to be recorded every time we pull the process out of the running state and they need to be reloaded back into the CPU whenever we restore the process to the running state. ",
                    "",
                    "We're going to have the stack pointers. So, any stack pointers that this process might have in this case is probably only going to be one stack pointer later on, we're going to see where there's a possibility of having more. Scheduling: so how long has this process been on the CPU, how much time has it spent, how long has it been waiting, when should reschedule it to wait, and so on and so forth. Any linkages: if this process is linked to another process in terms of a pipe, so that the output of this process goes to the input of another process, this has to be managed via the PCB. Any into process communication systems that might be enabled inside this process, so any communications that the process is having with other processes in the system. And the resources in use, so IO devices or files and how much memory is being used. That makes up the contents of the entire PCB. and that's what the operating system really uses to manage the process. The PCB. might be moved from state to state, or the PCB. might coincide with the process image, but either way we're going to keep all this information about the process in order to manage it.",
                    ""
                ]
            },
            {
                "title": "Modes 1.8",
                "data": [
                    "One of the other components that the operating system needs to keep track of is what mode a process might be in. And what we're really talking about here is what processor mode the process could be using. For the most part processes are going to use user mode, and that's normal, but in certain instances a process may have to be created that has more permissions, to access more parts of the system then a user mode would be allowed to access. User mode has a lot of restrictions; it's not allowed to directly access system hardware, it's not allowed to access main memory outside of its own bounds. So, the operating system keeps track of what processes are in user mode, and what processes are in kernel mode and there are very few that are in kernel mode. Now let's be clear kernel mode has absolutely no restrictions; you can do absolutely anything that the system can do when in kernel mode and kernel mode should really be restricted to only those processes that need kernel mode. In years past kernel mode was allowed for almost any process that was running on the system, and if you survived or use computers through the late 90's and early 2000\u2019s you know how dangerous that was. The era of Windows 98 and Windows ME constantly crashing and blue screening and the power switch solution to that was partly a result of having every process access to kernel mode.",
                    "",
                    "Now we have a much better solution: we use user mode only for those for those processes that are going to not need kernel mode. And the processor knows the difference between user mode in kernel mode as a result of one register. In the Intel architecture, we call this the program status word register the PSW, and that one register has one bit to indicate whether it's in user mode or in kernel mode. Switching between the modes is easy if you're going from kernel mode to user mode because you simply modify the register for the program status word and it changes into user mode from kernel mode. But switching back from user mode to kernel mode is a bit more difficult. For this the processor, itself needs to recognize when the operating system is going to run and it needs to move back into kernel mode automatically, by itself. And we\u2019ll talk about when this happens there are certain events that occur that cause the processor to move back into kernel mode and when that happens, of course, the processor is going to switch to running the operating system.",
                    ""
                ]
            },
            {
                "title": "Process Switching 1.9",
                "data": [
                    "So, a process that's in the running state will stay in the running state until something happens, and we need to understand what happens to cause the process to come out of the running state, and how that might affect user mode and kernel mode. Well first off, a process switch is going to occur during one of the events either an interrupt a trap or a blocking system call. ",
                    "",
                    "During an interrupt, some piece of hardware has indicated that it needs immediate servicing, and so the operating system must be invoked in order to take care of that hardware. Now the hardware could be any piece, we can think of a wireless device; a wireless access card where it has a small amount of memory and the buffer is starting to fill up. If the buffer actually gets full then any new data that's coming in through the wireless card is going to be lost, because the buffer is full. So, the solution to this is for the network card to send an interrupt to the CPU. And what happens in an interrupt is the CPU will finish executing the instruction that it's running, and then it will switch to running the operating system. And the operating system has a specific point that it uses, it's called the Interrupt Service Routine, the ISR. The ISR runs whenever an interrupt occurs; the processor knows to switch to the operating system\u2019s ISR, and of course since it's switching from a running program to the kernel; it switches from user mode to kernel mode and it runs the ISR. ",
                    "",
                    "We could also have a trap; now this is a situation where the process has done something that causes it to need the operating system, even though it doesn't know it needs the operating system. And we can also have a blocking system call; these are obvious like the program is trying to open up a file. If the program is trying to open up a file, it's going to need to invoke components of the operating system and so the responsibility switches. The process will switch to running the operating system and when it does that the operating system will take care of opening up the file and reading in the data. Now a process which involves quite a lot of steps, most importantly, we need to save the context of that process. We\u2019re gonna save that in the PCB. And then we need to move that PCB into the appropriate queue whether it's the blocked queue or the ready queue to indicate what we're going to next do with this process, and then we need to service whatever was requested; deal with the interrupt, deal with the blocking system call or deal with the trap. And then choose another process that's ready to run so we're going to look to the ready queue and choose one of the processes on the ready queue to move into the running state. And then we're going to actually run that new process by restoring its context. This gets a little bit more complex if we talk about multi processors, and the idea of multiprocessor we might have more than one process on the running state at any given time. and so that's going to have some effect on performance.",
                    ""
                ]
            },
            {
                "title": "Threads 1.10",
                "data": [
                    "So now that we've got the idea of a process down, let's make it a little bit more complex. Because while a simple program might be running one cohesive execution cycle, or execution unit, the idea of having a large program run as only one execution thread just doesn't make sense anymore. So, it was invented and we now use this concept known as a thread. Now we recognize that there are still going to be processes in the system, but the idea, the definition of a process, will doubt change to mean a unit of resource ownership. The idea of a process now no longer means an execution path, we're now talking about a unit of resource ownership. ",
                    "",
                    "So, all the files that are open inside the process, all the memory that's owned by the process, all the IO devices for the process, all of that that can be owned is owned by the process. But there's no real execution happening in the process, and what I mean by that is the execution is offloaded to a concept known as a thread. So, the process becomes the unit of resource ownership but the thread becomes the unit of execution. And we might have multiple threads that run inside a single process and each thread has its own code. Although the code is memory and the memory is owned by the process. So, there might be multiple threads that are running the same code, or the threads might be running different code. All the threads inside a process share all the resources of the process and that includes all the memory, so threads can share the same memory unlike processes; processes can't share memory they're restricted because they're in user mode. But threads all exist inside of a process and the operating system is going to recognize that multiple threads are running inside a process, and it's going to be able to execute any one of the individual threads inside the process.",
                    ""
                ]
            },
            {
                "title": "What is Where in the Multithreaded Environment? 1.11",
                "data": [
                    "So, if we now modify the definition of a process, we need to change where things are stored inside that process. So if we take a look, the PCB still has most of the stuff that we would recognize: the memory allocation, all the memory and IO tables, all the files all the linkages, all that stuff. But the thread now needs to keep track of some of the thread specific things like the context, the stack, the variables that are specific to that thread. All of that needs to be offloaded outside of the PCB and inside maybe a TCB, a thread control block, to keep track of each of the individual threads. Now the fun part about threads is that they all have access to the resources, the memory and all the rest of the stuff, from the process, and that means that we have a great level of communication between all the threads inside of a process. But the process needs to recognize, or the threads really need to recognize that they exist inside of a process and there are some concurrency issues that come up in relation to that. Now what about your old programs, like your \u2018Hello World\u2019 programs that you've written for previous modules, do they have threads? Well they did; they had one thread, the main thread and that main thread could have, although it didn't create other threads to do other tasks each individual task inside the system can be or inside the process rather, can be handled by one thread and each of the threads do their own sort of work. But they all work together on one project one process and they all share the materials of that process.",
                    ""
                ]
            },
            {
                "title": "Reasons for Multithreading 1.12",
                "data": [
                    "I'd like to take a look at some reasons why we would use multithreading, and some of these are rather obvious and you're to see them in in your regular day to day use of your computer. Sometimes you might have foreground and background activity; the perfect example I love to use is Microsoft Word, because it's one that we've all used a million times. And Microsoft Word is broken down into a really large number of threads, but just the few that we might pay attention to are that we might have a thread that's taking in input from the keyboard, and it takes in whatever the user types and it stores it in main memory in what we would call a document. And there's another thread that takes the that main memory from the document and puts it onto the screen; print it in the appropriate way, formats it, makes it look nice based on the font. And then there might be another thread that goes through and checks the document and underlines everything in green or red according to whether or not it's spelled correctly. And then another thread that goes through and checks the grammar. And then we might have another thread that sits in the background and waits for ten minutes, and when that ten minute timer is up that thread will save the document in some temporary location just in case your computer loses power. So, there's a lot of foreground and background activity that's happening inside something like Microsoft Word. ",
                    "",
                    "We might also look at Microsoft Excel, because it's another one that we've used quite frequently. And the act of changing one cell inside Microsoft Excel can cause a cascading change throughout the entire spreadsheet when that one cell changes there's threads that go out and change all the other threads that are related to, all the other cells, that are related to that one cell. So, then again there's a lot of foreground background activity.",
                    "",
                    "There's asynchronous in synchronous processing. So, if processing has to be done or if processing can be done synchronously we can create multiple threads to deal with the synchronous processing. So, if we have a very large data set and we'd like to process portions of it separately, we can process those portions separately using multiple threads. And a block that happens in one thread will not obscure the other thread from running. We might also have asynchronous processing, where the threads need to do some sort of work cohesively so that when one thread ends the next thread begins. We might have infrequent tasks, where one thread doesn't run very often; as my example of Microsoft Word and the saving every ten minutes is an example of infrequent tasks. ",
                    "",
                    "Speed reading, here we have a problem where each thread that does a read operation on the file system is going to be blocked because that's going to take a very long time, so the operating system is going to block that thread. Now the interesting part about threads is that blocking one thread, doesn't block the rest of the threads in the same process. So, we can have one thread whose responsibility it is to read all the data from the file system and bring it into main memory, and another thread whose responsibility it is to process that data, and this is called Speed Reading. So, we're pausing only to deal with the IO operation but at the same time when the IO operation is causing us to pause, we're continuing to process data that's already been brought in. And then it also creates a more modular program structure where we can break down a larger program into smaller portions and we can deal with each of the portions independently, as if they were basically their own program. But they're still working on one goal which is to deal with this memory or to deal with this process as a whole.",
                    ""
                ]
            },
            {
                "title": "Performance Example 1.13",
                "data": [
                    "Here we have an example of a file server. and the file server does some sort of an IO operation for eighty percent of its time. So, what this is we\u2019ll take in a request for some file, and then we\u2019ll process that request, whatever that means, and then we\u2019ll go off and receive the file, recover the file from them from the secondary storage device, and return it to the requester. So, you might think of this is like an old 1990\u2019s style HTTP web server so we take the request, we process the request; that's going to take let's say about two milliseconds. And then we'll deal with the IO operation of actually recovering the file from a secondary storage device and returning it to the to the requester, which maybe takes eight milliseconds. So, in total the processing time takes ten milliseconds. And that's just a perfect example; we've got an IO operation that takes eighty percent. ",
                    "",
                    "If we do this without threats, we do this with just one process that means we've peaked out, we've maximized, our utilization at one hundred transactions per second; each transaction is going to take us ten milliseconds and of course there's a thousand milliseconds in a second, so thousand divided by ten means we can get one hundred of these transactions in a single second. But let's take a look if we add in the idea of threads, and let's create a thread for each transaction. When we do that, we can now synchronously process the CPU and the IO operation. In other words, if we created two threads, then the first thread\u2019s CPU could be done while the second thread was doing an IO operation. And then the second thread could do its CPU time while the first thread is doing the IO operation. So, we can sort of go back and forth using the CPU as much as we can and using the IO completely. Now if we do this, we recognize that the IO can't be done synchronously; we have to we have to do that asynchronously. We have to do that so that one IO operation is completed and then the next IO operation can start. ",
                    "",
                    "The IO device can run multiple requests at the same time, so we can service multiple requests at the same time, that means we have to do it asynchronously. Well what that means is that we have these little eight second eight millisecond blocks that were chopping up this one thousand millisecond second into these eight millisecond blocks, and that means we can use only one hundred twenty-five transactions per second. Now that doesn't mean we're not doing the CPU time, of course we're still doing that two milliseconds of CPU time, but it means that we're doing it while the IO operation is happening for one of the other threads for the other thread. So, ultimately we've increased our capability by twenty-five percent, because what we've effectively done is hidden the CPU time behind the IO delay, and that works out great. ",
                    "",
                    "But we can actually do better because in this example of a web server, it's pretty obvious that a lot of the requests are going to come in to the exact same file; they're going to be for the exact same file. And if we're dealing with each request independently, then the CPU time and the IO delay don't result in any cache; they don't result in anything being retained for the next request. But if we can keep track of each of the requests and when we bring in a file, we store that file in main memory, then in the next request we don't have to do the IO operation; we can avoid doing that eight milliseconds, that expensive IO operation, we can completely avoid that by recovering the file directly from the cache. So, this is one of the benefits of threads that were exploiting by having a shared memory space, which you can't do with processes; a shared memory space where we can store these files temporarily. Now at some point if we don't ever need the file, we have to throw it out of the cache and if we use up too much memory, we may have to throw away some of the files that aren't being used very often. So, there's an extra bit of processing, but even if we increase the processing time by twenty-five percent and increase it then say, do two and a half milliseconds of CPU time, the benefit here is that we're decreasing the amount of IO delay. ",
                    "",
                    "Now, let's say for example we get seventy-five percent hit rate on the cache; now that's abnormally high granted, but it's possible, if we have a web server that's popularly serving one web page. When that happens, we recognize that the IO delay drops to zero for those seventy-five percent of the operations of the transactions, the IO delay will drop to zero. Now the twenty five percent that remains we're still going to have to do that eight milliseconds of IO delay, and unfortunately now we've wasted an additional half a millisecond for the CPU time to sort of look into the cache and realize that it's not even there. So, this extends that, but it reduces the overall time because now on average we're going to have two and a half milliseconds of IO delay for every transaction but then, correction two and a half milliseconds of CPU delay for every transaction, but the IO delay is reduced to only two milliseconds for every transaction. And it's not really two milliseconds for every transaction, it's eight milliseconds for those transactions that we have to do the IO delay, but that's only seventy-five percent of the time. So, we can say that on average each transaction uses two milliseconds of IO delay, so with two and a half milliseconds of CPU and two milliseconds of IO delay, we have four and a half milliseconds total and that means that we can get four hundred transactions in a single second. ",
                    "",
                    "So, we've gone from one hundred transactions in a second to now four hundred transactions in a second just by implementing threads and creating a cache and getting a pretty good cache hit rate, but it's possible. If we want to put some money into the problem, maybe we can add a second processor and set up symmetric multiprocessing so that that CPU delay can be done concurrently, in which case the only slowdown here is the IO operation. We\u2019re IO bound, meaning that we still have those two milliseconds of IO delay that we can't get rid of, and that two milliseconds means that we're kind of limited at five hundred transactions per second. But with a very small investment, we can go from one hundred transactions per second to five hundred transactions per second, and with no investment at all we can go from one hundred to four hundred transactions per second just by implementing threads.",
                    ""
                ]
            },
            {
                "title": "Thread States/Operation 1.14",
                "data": [
                    "So, if we take a look at the concept of a thread now, we recognize that the thread really only has three states. It has ready, it has running, and it has blocked, and all of those makes sense in the same concept that we talked about them in the process level notion. A thread can be ready, meaning it has everything it needs to in order to run it's just waiting for a processor. It has running, meaning it's actually on the processor and doing some work, and it has block, meaning it's waiting for something to happen. ",
                    "",
                    "Threads don't really have a new and an exit state, because we don't need to load code. For example, our old example of loading the code and that was why we needed the new state, we don't have that anymore because the thread already has all of its code loaded in main memory. We're just creating a new thread and how does a new thread get created? Well the operating system creates a TCB to recognize this thread, initializes the linkages initializes the TCB, and then it starts to run. So, it's really an atomic element creating a new thread can be done atomically, and the thread just begins running inside the context of the process. So, we don't need a new state and likewise, we don't even exit state because when a thread completes there's nothing returned back to the running process; the thread just records that it's done. ",
                    "",
                    "Now obviously, if it's the last thread or the main thread, if you would, then that needs to be returned back so we still have the concept of new and exit state in the idea of a process, but in the idea of a thread the new and the exit state aren't really there anymore. Likewise, the idea of suspension is really a process level concept; if we've removed all of the main memory the none of the threads are able to run. So in suspension, we have to recognize that all of the threads have been blocked or all the threads have been stopped altogether. But at the same time, there's no real recognition of the suspended state inside of a thread; that's more of a process level concept."
                ]
            },
            {
                "title": "What are the Downsides? 1.15",
                "data": [
                    "So we've talked about the upsides of threads and we certainly understand how threads can be great, but of course there's got to be some sort of a downside. And what's the downside? Well the biggest one is that all the threads have access to the same memory of the process; well that was an upside too, wasn\u2019t it?",
                    "",
                    "The biggest issue here is that we have concurrency problems; that if all the threads are trying to access the same memory that of the process, then it is possible that the threads could interfere with one another, unintentionally. And there's a lot of examples of this and we\u2019ll go over some of the concurrency examples in the next module, but ultimately what we're looking at is that whenever a thread is accessing memory or it really any resource that's common to the process, it needs to do so very carefully and it needs to keep track of where it is inside the execution cycle. Usually what we do with this is some sort of a mutual exclusion lock, or a some or for to indicate the thread is in execution and maybe has been stopped.",
                    "",
                    "Now why was it stopped? Well could be any of the reasons that we talked about for a context switch. So, any time we could switch a process, we could also switch a thread; so, from one running thread to the other. And unfortunately, if the thread is making some changes to the memory, it might not be complete with all of the changes; it might do only half of the changes leaving that memory in a really inconsistent state, and that's one of the concepts that we know of as concurrency. This is what we call concurrency. ",
                    "",
                    "The other problem of course is deadlock, but deadlock can happen as well in processes and we will talk about deadlocks in a later module. The other problem that we have with threads is that programmers really like them, so they create a lot of threads. And if we create too many threads we lose track, or it's very easy to lose track I should say, of what each thread is doing and that causes a lot of confusion. And two threads might interact with each other in a way that we didn't really intend because we're not being careful with how many threads we're creating.",
                    ""
                ]
            }
        ]
    },
    {
        "module_number": 24,
        "module_name": "Thread Concurrency and Deadlocks Script",
        "file_name": "Module 24 Thread Concurrency and Deadlocks Script.docx",
        "transcript": [
            {
                "title": "Implementation of Threads 1.16",
                "data": [
                    ""
                ]
            },
            {
                "title": "Concurrency and Deadlocks 1.2",
                "data": [
                    "In this model we're going to continue our discussion of threads and we're going to talk about some of the features of having threads work together with one another. So we're going to talk just simply a quick reminder about what threads are we're going to talk about some features of having multiple threads accessing multiple resources. We're going to have a discussion on ways that we can end up with a synchrony and what a synchrony is. We're going to talk about the idea of critical sections. We're going to talk about concurrency issues. We're going to explain mutual exclusion and talk about some software and hardware options or solutions for mutual exclusion. We're going to describe the operating system semaphores and how they work. We're going to describe deadlocks and how we can resolve deadlocks and then we're going to talk about the dining philosophers problem as a wrap up problem. So there's a lot to go through in this module and we'll get started immediately.",
                    ""
                ]
            },
            {
                "title": "Reminder about Threads 1.3",
                "data": [
                    "So a quick reminder threads all share the resources of the process, which means that they have access to all the same memory access to all the files access to everything that the process has access to and each of the individual threads work as a separate program but they're all working towards the same goal. So, if the process that we're talking about is Microsoft Word the ultimate goal is to create a document and print it, save it, do something with it but the problem that runs is that threads can run asynchronously. The problem that we have is that these threads are going to do work and that the work is not done in a synchronous manner in other words we don't have one thread running to completion and then another thread starting up we have all the threads running at the same time and if we have multiple threads running at the same time accessing the same resources we have the potential for conflict on those resources and we have to figure out ways to deal with the conflict and address the potential for catastrophic loss of information.",
                    ""
                ]
            },
            {
                "title": "Features of having Multiple Threads 1.4",
                "data": [
                    "So one of the nice features of having multiple threads of course is that they can communicate with one another. Very easily we can have a lot of threads created and all of the threads are accessing the same memory in the same resources. So, if we just for example created a buffer in main memory and had all the threads have access to the buffer then we wouldn't need the operating system to intervene as we would if we had multiple processes. So, multiple processes can only communicate with each other via IPC via the interprocess communication system, but multiple threads can communicate with one another directly because they're both accessing or they're all accessing the same amount of memory.",
                    "So this is a really effective solution. For example when we\u2019re reading in data from an IO device we could have two threads one that processes data that's already been read in and another thread that's actually doing the read operation. So when the thread that's doing the read operation performs a blocking system call to actually bring in the data from the secondary storage device the operating system blocks the thread, but the other thread that's processing the data is allowed to continue operating.",
                    "So, we have the ability to do a read in operation while we're doing some processing on data that's already been read in. The other nice feature about having a thread is that it's relatively easy to create. It takes a very very little memory and we can create it very quickly and easily as compared to a process but the big downside of having multiple threads is that we run the risk of asynchrony we run the risk of these threads doing something out of a normal order and we're going to see that in this module. ",
                    ""
                ]
            },
            {
                "title": "Asynchrony 1.5",
                "data": [
                    "So what is asynchrony? Well asynchrony occurs when two threads are running seemingly at the same time. So for example we might have a running thread that is interrupted due to some hardware considerations maybe the network card needs processing it doesn't really matter and a different thread is chosen to run. So, we have one thread that's running on the CPU and it stops running. It does some work and it stops running and then when we come back we're choosing a different thread to run. We could also have a thread that runs out of time. Remember we're in a preemptive system. So we're not allowing threads to run until they're finished we're not allowing processes to run until they\u2019re finished. We're allowing them to run only for a certain period of time and when that period of time runs out whether or not the thread is finished we have to pause the thread we have to bring the thread out of the running state bring it back into the ready state and then we can go on to running either this thread or we can choose a different thread to run. So, there's a possibility of asynchrony there and the other possibilities if we have actually multiple CPU\u2019s multiple different CPU\u2019s we can have two of these threads running on the CPU all at the same time. So, what's happening here is that these threads are doing some work and the work that they're doing it is not a complete atomic instruction in other words we don't start it and know when we finish it. We don't do one function call and finish the function call and then pause to run another thread. We could be in the very middle of an instruction or in the very middle of a line of code and we could stop and run a different thread altogether so we always have to keep that in the back of our minds when working with threads that we never really know when the thread is going to be stopped and when a different thread and possibly a conflict thread would be run.",
                    ""
                ]
            },
            {
                "title": "Critical Sections 1.6",
                "data": [
                    "So sometimes code is going to be written with the expectation that once we start running a particular piece of code it's going to run through with to completion without being interrupted and we know from our discussion just a minute ago that that's a fallacy we know that. It's not going to be run as an atomic element. It's not going to be run from start to finish without interruption that interrupts are going to happen in the system and that we're going to stop this thread and started up again at almost random intervals and in fact I would say that we would consider the worst case scenario and the worst case scenario is where we have a real bad luck situation where we run some piece of code stop at a really bad place and choose a different piece of code that conflicts with exactly the first piece of code. So the code is going to be interrupted and we know that asynchrony can occur during interruption. So we have to take that into account. What we do is we identify those pieces of code that have to be run atomically In other words we identify those pieces of code that once we start doing this code we can't be interrupted. Now unfortunately we can never say that we can't be interrupted. ",
                    "",
                    "So the better solution is to say that we if we are interrupted that no other thread is allowed to go into that piece of code that could conflict with us and what we're trying to do is identify something known as a critical section and a critical section is a piece of code that once entered you have to prohibit all other threads from entering the critical section on the same resource. If we have two different resources and two different critical sections. We can run both those critical sections at the same time because the two different resources don't conflict with one another but if we have a critical section that operates on a particular resource then we can't run one critical section in conjunction with another critical section or else we have asynchrony. So, if we reach a critical section in thread one for example then if thread one is interrupted thread two can't enter its critical section until thread one is allowed to complete. So, if thread two wants to enter it's critical section it has to be blocked it has to be paused. The critical section should be as small as is possible because we don't want to spend a lot of time blocking all the other processes from accessing sections of code, their critical sections if we don't have to. So, we want this these pieces of code to be absolutely as small as possible but we do recognize that they exist and that they'll have to be there. So, there will be situations in which we block other threads from running because we don't want to have the possibility that those threads are going to interfere with what we're doing and once we're complete with that critical section we\u2019ll allow all the other threads to continue running.",
                    ""
                ]
            },
            {
                "title": "Supplier/Demand Explanation 1.7",
                "data": [
                    "So the code that I'm going to show you right now is what we call a supplier demander example and the supplier demander example really just has multiple threads running of either a supplier thread or demander thread. In fact we're going to have multiple instances of both suppliers and demanders. So, think about a factory for example where things are coming off the production line there's a lot of people producing things, there's a lot of people taking those things and moving them on to the next stage in the production. So, we have both supplier and demander one of the important considerations here and why this has a critical section is that there is a shared variable there what we're calling the buffer and the buffer count is actually two shared variables there. The buffer count tells us how many items there really are in the buffer. Now you recognize from the code that the buffer can store five hundred items. The buffer count let's say it's initialized at zero there's nothing in the buffer when the morning starts the suppliers come in and they start producing things that go into the buffer. Now the supplier just has a simple piece of code that says as long as I'm not finished then I'll put something in the buffer. Now the reason that we have that while not done is because it's possible that the buffer is full and if the buffer is full we want this thread to just kind of pause, pause for maybe half a second and let one of the demanders take something out of the buffer so it makes space and then we can put this thing in the buffer. We can update the buffer count and then we can be finished. So the suppliers a very simple piece of code the demander is almost the same thing except it checks to see that there's actually something in the buffer. ",
                    "",
                    "So the while not done here is for checking to make sure that there actually is something in the buffer rather than making sure that the buffer is full as long as there is something in the buffer we\u2019ll decrement the buffer count and return the item that was in the buffer at the position.",
                    "So that's the supplier demander idea. Now we do have multiple supplier threads and multiple demander threads they're all active they're all running in the system. However we've got a uni processor system so we can only run one thread at a time we choose either supplier or we choose demander and we have to remember that there are still going to be interrupts they're still going to be preemption there still going to be situations where the thread is interrupted and we don't know when it's going to be interrupted.",
                    "Now the core idea of this is that the buffer count should never exceed five hundred. We should never exceed five hundred because there are only five hundred places to store in the buffer. So we can recognize that the buffer count can get as high as five hundred but it can't ever exceed five hundred.",
                    "Unfortunately as we'll see because of asynchrony because we haven't taken care of the fact that the code is protected by a critical section that buffer count can easily go higher than five hundred. So we'll see that.",
                    ""
                ]
            },
            {
                "title": "Steps to Producing a Problem 1.8",
                "data": [
                    "So here we can see the steps that we can run through to produce this problem. What we see here is that we're running the supplier thread the buffer count is already at four hundred ninety nine which means there's only one slot left in the buffer that we can fill up. So at this point in the code the supplier thread let's call it thread number one supplier thread number one is going to be running. It checks to make sure it's not done, it checks to make sure that the buffer count is less than five hundred, and the buffer count is less than five hundred. It's four hundred ninety nine so that thread is allowed to enter that if statement. It goes past that if statement and then out in thread one right at that point for whatever reason either because of preemption or because of a hardware interrupt we get an interrupt and after entering the if statement we haven't made any changes but interrupts. So, now we have an interrupt and when we run from the interrupt, when we return from the interrupt the operating system has to choose a thread to run and it's not guaranteed that it's going to choose thread one again. ",
                    "",
                    "So, what does it choose? Unfortunately in this example it chooses to run thread two. So now Thread two is allowed to run and what does thread two do? The first thing it does of course is say while not done is equal to while not done so it says Yeah I'm not done checks the buffer count buffer count is four hundred ninety nine. So that's allowed, we enter into the if statement. It adds the element to the buffer it increments the buffer count now to five hundred so effectively at this point the buffer is completely full so thread two is finished thread two ends and we're going to go back now and finish out thread one. But what's already happened is thread one checked to make sure the buffer count was less than five hundred and buffer count was less than five hundred it was four ninety nine the last time thread one ran but we didn't take into account the critical section. we didn't take into account the possibility for asynchrony, so now we don't go back and recheck the buffer count so the buffer count now is five hundred but thread one the last time it ran checked it was for ninety nine so we left off at that opening curly brace and that's what we're going to pick up from and the first thing it does is increment buffer count to five hundred and one. How do we get to five hundred and one when we only have five hundred positions in the buffer. So this is really the problem that we're going to run into we don't have that extra one space and it can get much much worse than this in a real scenario but I'm just demonstrating simply that we can run out of buffer space and continue on if we don't take into account the asynchrony that can happen between these two threads.",
                    ""
                ]
            },
            {
                "title": "Double Update/Missing Update 1.9",
                "data": [
                    "Another example that we've got of a concurrency control problem is one of a double update/missing update situation. And in this scenario what we have is two pieces of code: just a deposit function and it withdraw function. So you can imagine a situation where you have two ATM cards for the exact same bank account and you go to the bank, not at not at nearly the same time but literally at exactly the same microsecond in time, and one person is doing a deposit and the other person is doing withdrawal. And when we do that, we have two transactions that are happening at the same time on different processors. So, now we have the possibility of asynchrony because we're dealing with this with separate processors and symmetric multiprocessing. We're going to assume that the bank has a good enough system that we have more than one CPU, so we can expect that this these pieces of code are running on two separate processors at exactly the same time. The biggest issue is that they share a balance, and the balance starts out at one hundred dollars and the first transaction is the deposit which is fifty dollars and the second transaction is the withdrawal of a hundred dollars. So, we can imagine a situation where two people go to the bank at same time and one is depositing fifty dollars, and the other is withdrawing one hundred dollars. Ultimately, what we should see is that the bank balance is fifty dollars because we had one hundred plus fifty minus one hundred and I'm not great at math but that's pretty easy to do and we'll say that that's fifty dollars. So let's go out and see if that's really the case.",
                    ""
                ]
            },
            {
                "title": "Double Update/Missing Update 1.10",
                "data": [
                    "So, we can take a look at the way this code is going to run; we can see the program counter here. I've broken it down into multiple steps, so that you can see. What we start off with is the deposit function which is going to put in fifty dollars into the account. The balance, the new balance, becomes a hundred fifty dollars so this new balance variable is local to the deposit function and it makes a copy of the balance which was one hundred dollars and it adds the fifty dollars, so we have one hundred fifty dollars. Unfortunately, we get an interrupt or we\u2019re running on a separate CPU, so we go to the other CPU and we check out what's happening there. Now we look at the withdrawal function now and it's running on the other CPU or it's now running on the one CPU, and we say that the new balance is now the balance minus the amount, which would now be zero, so the new balance is now zero. And then we go back and check the previous thread which then goes back and copies the stored New Balance information. We go back and finish out the deposit function put the balance in as a hundred fifty dollars, which makes sense because if we had a hundred dollars and we deposited fifty dollars then we go ahead and put that a hundred fifty value back into the common shared variable, which is balance, and then the deposit thread is finished; the deposit thread exits. But we have to go back and finish out the withdraw thread, and when we do that we get into a lot of trouble because we're going to copy New Balance back up into balance and New Balance is now zero. So, our fifty dollars disappeared, because it was hidden; what happened is we had asynchrony. We had the deposit function running at the same time as the withdrawal function, and the deposit function performed its action, asynchronously, with the withdraw function, and the withdraw function never got the update according to The New Balance. So, in other words when deposit changed new balance, withdraw knew nothing about the new balance and it used the previous balance. So, that over wrote the original balance. So, ultimately what this means is that our bank gets a free fifty dollars, and we're going to have to go to the bank and fight for our fifty dollars and that's kind of annoying. Thankfully we're going to take care of this with critical sections, so that we know that we're not going to change the balance, so we're not going to lose our money.",
                    ""
                ]
            },
            {
                "title": "Critical Sections Identified 1.11",
                "data": [
                    "So, we saw in the supplier/demander thread, an example of checking and changing the buffer and the buffer count and how that has to be done atomically; we have to look at that as one atomic instruction. We have to not be able to stop running that or run anything else until we finished running them, that set of instructions. So, we have to consider the complete supplier threat or at least the subset of the supplier thread in that portion where does the checking the if statement and the changing of the buffer count and the buffer; we have to do that in one atomic instruction, although we can't do it in one atomic instruction. So, the only way that we can consider this is that the that portion of the function needs to be protected by a critical section and we'll see how to do that. In the double update/missing update problem, really those entire functions which are very simple functions well they are as two lines of code, both of those functions need to run atomically so they cannot be interrupted from the time we start the function to the time we finish the function. The need for running those sections of code atomically, actually indicates that those sections of code are critical sections, and that's what we're trying to identify. We're trying to make sure that we know where the critical sections are and that we protect them by protection mechanisms, which we'll talk about in just a few minutes, from any untoward asynchrony that might happen inside the system.",
                    ""
                ]
            },
            {
                "title": "Mutual Exclusion Rules 1.12",
                "data": [
                    "So the mutual exclusion rules are real simple. No two threads may be in a critical section at the same time; it just means that we can have two threads that are in the critical section at the same time.  I know we said before that that is possible if we're talking about two threads that are working in critical sections on completely different resources, and that's fine we're not talking about that. What we're talking about is when we have a shared resource, we have to protect that shared resource by not allowing any threads that have that are in their critical section to access that resource. So, when no thread is in a critical section, any thread that requests entry has to be allowed in without delay. And that means that we can't as the operating system to act as a traffic cop; we can't say to the operating system every time we want to access a critical section: are we allowed to access the critical section? Because that would be a call to the operating system and then the operating system is going to have to stop and check and then run the thread again, which requires context switching in we talked about how expensive that is. ",
                    "",
                    "So, we'd rather have is the way for the thread to check itself to see if it can go into its critical section, to see if it's allowed to go into its critical section, and if it is to just go straight in and not stop. If the thread is not allowed into its critical section, of course, we're going to have to have the thread pause, we\u2019re gonna have to have block and of course that's going to involve the operating system because the thread is going to move from the running state into the block state and that means the operating system has to be invoked. But really we want to avoid any unnecessary delay from the thread if it has to enter its critical section; let it in as long as nobody else is. The other part about this is that the thread can really only remain in its critical section for a very small amount of time; we don't want to have large sets of functions very large functions with a lot of work in a critical section, or in considered to be a critical section, when all we need is a couple of lines of code to be in that critical section. So, we want to have that is minimal as possible; we want to allow the thread to enter into its critical section, do its work and then get out as quickly as possible.",
                    ""
                ]
            },
            {
                "title": "Fundamental Mutual Exclusion 1.13",
                "data": [
                    "The hardware provides us with the mutual exclusion protection mechanism, built into it; we have the system bus, and we know that the system bus can only be used by one processor at any given time. So, even in this scenario where we have symmetric multiprocessing, with multi processors in the system, we recognize that it's impossible for two processors to change the same memory location at the same time. It is absolutely impossible because one of those processors will gain access to the system boss will be allowed to change that memory location and then the other will have to wait. So, we have a protection mechanism built directly into hardware just by the nature of the way we do things, so that we can't change a memory location to two different values at the same time. So, what we'd like to do is develop something that we can use at a much higher level, to protect our code in C++  or in a high level language, from any problems that might happen because of mutual exclusion cases. So, having that fundamental hardware mechanism is very useful, is helpful, but ultimately we need something a lot easier to deal with on a high level and we'll see that.",
                    ""
                ]
            },
            {
                "title": "Peterson\u2019s Algorithm 1.14",
                "data": [
                    "We have better solutions today; we definitely do, we\u2019ll deal with some hardware solutions in just a minute. But originally we didn't have hardware that could help us deal with this mutual exclusion problem and so computer scientists started working to try and solve the mutual exclusion problem using just software, so not having anything in the hardware to directly deal with this except for that system bus limitation. Peterson came up with a fundamental way to protect two threads from accessing the same resource at the same time and what he did was he provided each thread with the Boolean flag variable. So, there's an array of Boolean flag variables for each of the threads and each thread would put up its flag; would set it its Boolean value equal to true, if it wanted to access its critical section. ",
                    "",
                    "Now, unfortunately that's not enough because we're going to have to go and check everybody else's, or the other thread let's say if there's only two threads, we're going to go and check the other threads flag to make sure it's not in its critical section. And we run the risk if we set our flag and then go check the other thread, that the other thread is doing exactly the same thing at the same time, and now we have both threads have their flags up and both threads think that the other thread is in it's critical section and in fact, nobody is in a critical section. So, what Peterson did to solve that problem specifically was introduce another variable he called the turn variable. And in here it's being generous is very helpful he thought, so what the threads do is they offer the turn to the other thread so thread zero would offer the turn to thread one and thread one would offer the turn to thread zero.",
                    "",
                    "So, in that very rare situation where both threads want to access the critical section, both would raise up their flags and one would offer the turn to the other and the second one would win; the second turn right, if you will, would overwrite the first change of the turn variable. So, either thread zero or thread one would be allowed access into its critical section, and then of course eventually it would unset its flag and allow the other thread to access its critical section because it no longer has its flag up. So, the nice feature here is that it does provide protection for mutual exclusion, if we have two threads that need access to mutual exclusion. If we have more than that we have a bit of a problem, but Peterson's algorithm allows us to deal with the situation where we have two threads and we can provide mutual exclusion for those two threads using only software solutions.",
                    ""
                ]
            },
            {
                "title": "Hardware Solutions 1.15",
                "data": [
                    "Now that we have the software solutions figured out, we'd much rather take a look at hardware solution. So, this is what we're really going to do today, after a very short while the CPU designers realize that the computer scientists were having this problem and they said: hey guys we can deal with this much easier and give you some hardware solutions to take care of your concurrency controllers, take care of dealing with your locks. And so what they did was they created some hardware built into the CPU to take care of this; most of these are instructions that are built directly into the CPU. ",
                    "",
                    "Now the first one is the disabling of interrupts. I don't think this is a great idea because interrupts are really necessity; we want to have interrupts in the system. If an interrupt does occurs it means a piece of hardware needs immediate servicing and we want to go ahead and do that. But if we take away the possibility that an interrupt might occur then we effectively take away the possibility of asynchrony; we guarantee that this thread will not be interrupted until it decides to re-enable interrupts. Now this would probably be limited to the operating systems control, so this would require some intervention by the operating system to use this because imagine a situation where a thread disables interrupts and then crashes; the whole system is stuck you can't get out except with a big power switch. So, disabling interrupts is something that we use very sparingly if it is in existence and even then we try to avoid it.",
                    "",
                    "The other options that we have our instructions that are built into the instruction set on the processor. So, the first one is called a test and set, and in the test and set function, test and set instruction, what we do is check a memory location and if its value is of a particular value. So, for example zero if we find that there is a zero stored in that memory location we will change it to a one. So, this is perfect for a Boolean flag variable, for example, which says that maybe somebody is in the critical section. By having this test and set instruction, we can have that single variable that single Boolean variable that says there's somebody in the critical section and we can set it equal to one at the same time it\u2019s testing it.",
                    "Now this is an atomic machine level instruction. So, the CPU takes control of the system bus, goes and checks the location in memory, and then if it is the zero, goes resets that value to one in one atomic instruction; nobody else will be allowed to access that memory location. So, even if we have another processor that processor will not be allowed access to the memory location during a test and set because it has to happen atomically. So, a result is returned back to the back to the operating system or back to the calling program that asked for the test and set, and that indicates whether it was successful. Meaning, we both tested this to make sure nobody's in their critical section and at the same time we set it to go so indicate that you are in the critical section and that means that the process is allowed to go into its critical section. Or we have a failure in which case the thread or the process has to pause and delay and try it again. So, the test and set actually works fairly well.",
                    "",
                    "The other option is very similar it's called exchange, and this is a very common one that we use today, in which we have a swapping of a location in main memory with that of a register. So, we can again use, this is a Boolean flag variable, except now we can put a one in a particular register, call exchange for the memory location and we literally get a swap of the value in the register with the value in the memory location. So, if the memory location had a zero, it will now be set to one and we can go back afterwards and check to make sure that it was a zero, if it was a one then we'll know that it was a one because we'll get it back. But the nice feature is that when we do the exchange, even if we're interrupted immediately after the exchange instruction, the indication that we're in the critical section has already been set and now we just have to check to make sure whether it was successful or not. So, the exchange instruction is a very common one that's in use today and it's relatively easy to understand.",
                    ""
                ]
            },
            {
                "title": "Semaphores 1.16",
                "data": [
                    "The common solution that a programmer would use today as a high-level solution would be the semaphores. Now semaphores of course are going to have some issues internally to them, so they're going to use some of the hardware level instructions that we just talked about. But the semaphore will provide for mutual exclusion. Now the word semaphore actually comes from a nautical term; I think it's needs a flag that used to be flown on top of the ship and it's very much similar. So, what we have here is two functions that we generally use inside the semaphore, we construct the semaphore and then we send it a signal. The idea of a semaphore is that it's a message passing structure, so one thread could send a signal and the other thread could receive that signal. Now the way we receive the signal is by calling wait. But here's where this is very useful for mutual exclusion, the wait function is blocking. So, if there is no signal that has been sent, then the wait function causes a block to wait for the signal to be sent, and that's where we can really use it for mutual exclusion.",
                    ""
                ]
            },
            {
                "title": "Semaphores \u2013 How to Use Them 1.17",
                "data": [
                    "So, initially when we create the semaphore we send a signal in the semaphore, so that's an initial cueing of the signal. Now assuming that we just created it I'm going to assume that nobody's waiting on the semaphore, so the signal is just going to sit there and wait to be received. When we enter a critical section the first thing that we do is call wait. Now as long as there's a signal already there, then wait is non-blocking and the thread is just going to be allowed to enter its critical section. But the key feature is that the wait will consume the signal that's queued, so now there's no signal that's queued. If another thread calls wait to enter its critical section then it will not have a signal and it will be blocked and the operating system will keep a queue of the blocked processes so that it can start those processes again as the signals come in. When we're done, when the thread is finished with its critical section, it\u2019s going to call signal and signal is going to either queue up the signal if there's no threads waiting or if there is a thread waiting then the signal is going to release the next thread from the queue. So, this allows us to create a mutual exclusion protection mechanism using just a simple signal passing routine",
                    "",
                    "Semaphore Internals 1.18 ",
                    "So, we need to look at the internals of the semaphore and we need to have an understanding of how the semaphore works internally. So, what we're going to do is kind of piece together what happens inside a semaphore, without actually going through the nitty-gritty of how the operating system creates a semaphore and maintains a semaphore. But what we do is we create a counter inside the semaphore object, and the semaphore is going to indicate how many signals, or the counter is going to indicate how many signals have been sent. Each weight causes the counter to decrement and if the counter ever becomes negative then the thread that causes it to go negative, and of course all subsequent threads, are going to be blocked and then they're placed on a queue. Now the operating system maintains the queue, and it's a queue of blocked threads waiting for a signal on that semaphore. So, that's going to be our indication that we can release a thread, but they all go into the blocked queue; all those threads that come in and call wait, that cause the counter to go negative, when the counter is negative those threads are all blocked and the only thread that's allowed to run is the one thread that consumed the one and only signal. Now when that thread is finished, it has to send a signal, and if there are any threads on the queue then the signal will cause the counter to increment; first signal always causes the counter to increment. But it also releases the next thread in the queue, if the counter actually goes positive that means that there's nobody in the queue waiting for a signal so there's nothing to be done there, but we recognize that the signal in the wait function allow us to create this mutual exclusion environment. Unfortunately, internally to the semaphore, there exists the critical section; the act of looking at the counter and then changing the counter is itself a critical section. So, here we have a mechanism that we're trying to use to protect against critical sections, and we created a critical section. So, that's a real problem so we have to use a hardware solution to prevent asynchrony when the semaphore has to both check the counter and set the counter. So, this is where the semaphore is going to use someone of the hardware options, either disable interrupts or test and set, probably exchange, and it's going to take care of it internally. So, we have to recognize that semaphore itself has a critical section and it has that in order to solve our critical section.",
                    ""
                ]
            },
            {
                "title": "Deadlocks 1.19",
                "data": [
                    "Now that we've talked about semaphores enough, we have to talk about one downside to having all these locks and that\u2019s the idea of a deadlock. And the deadlock occurs in a system, or can occur in a system, if all the threads are waiting for each other; if we have a set of threads that are all waiting for each other, then there's really no way that any one of them are going to complete. Usually it results from one thread waiting for another thread to release a resource, and it's not really clear what that resource could be; it could be a lock, in terms of a mutual exclusion lock like semaphore, or we could be talking about a file, or we could be talking about a mouth of memory. But there's some resource which is shared among the threads and one thread is waiting for another thread to release the resource, while that other thread is waiting about eventually back for the first thread to release the resource. So, we have one A waiting for B, B waiting for A neither is going to get done; it's a permanent block, it is not going to resolve itself over time. This requires the intervention of an operating system if the deadlock does occur, there\u2019s going to have to be some resolution that the operating system or that's a monitor program takes on top of the the deadlock threads to resolve the issue. And unfortunately today there's really no efficient solution, even today we don't have a very efficient solution to deadlocks. It either amounts to a considerable waste of system resources, processor time or memory, or it amounts to a deadlock occurring and then resolving the deadlock. So, we really don't have a good solution today to deadlocks, other than to say we have to keep them in mind and make sure that they don't happen.",
                    ""
                ]
            },
            {
                "title": "A Real Deadlock 1.20",
                "data": [
                    "I like this picture because it really shows traffic inside a normal city environment. And what we have here is that each of the cars that would enter the intersection sort of went a little bit over and are now blocking a portion of the intersection. And what you can see is that each car is waiting for another car to move and the last car is waiting for the first car to move, and ultimately as you can tell, nothing is going to get done here. There is no way that any of these cars are going to move until someone decides to take action, and just completely change the scenario; maybe one of the cars could make a turn and get out of there and then free up the intersection. But ultimately what's happening is we are controlling one quadrant of the intersection, each car controls one quadrant of the intersection, and needs one more quadrant in order to complete its task of moving through the intersection and we recognize that that can never occur.",
                    ""
                ]
            },
            {
                "title": "Deadlock Resource Types 1.21",
                "data": [
                    "I've been talking about using locks, or using a semaphore, but in reality we have two different resource types that we can talk about when we get into deadlocks. One is reusable resource types, and the other is consumable resource types. Now a reusable resource type happens when we have a resource that we will temporarily consume, for example, we will temporarily take action on the resource, we will temporarily take over control of that, resource. But then once we're done with it we'll return the resource back to the operating system or back to the the whole so it can be reused again. So, examples of this include things like memory, devices, data structures, even semaphores; once we're done with these things we return them back to their original state and they're allowed to be used again. But there's consumable resources, which once we use it it's gone. For example, if we have an interrupt and we deal with the interrupts, or we if you will consume the interrupt, the interrupts can't be replaced. If we get a signal then the signal can't be replaced; if we got a message then the message cannot be replaced. If we're for example, processing data in an IO buffer as you did in previous examples where you read in from the keyboard, once you read in that data there's an easy way to put it back into the buffer. So, consumable resource types, once they're used they're gone, and the reason that we're recognizing this is because how we deal with a deadlock on these different resources is going to be different from one resource type to another.",
                    ""
                ]
            },
            {
                "title": "Items Required for a Deadlock 1.22",
                "data": [
                    "There are four things that have to come into play for a deadlock to actually occur. And some of them are requirements that the operating system needs to have, and some of them are just bad luck that have to happen, but we need all four of these to happen in order for us to have a deadlock. The first one is mutual exclusion and we already know what mutual exclusion is, we've been talking about it, we understand that there's going to be a resource and this resource can be held by only one thread at a time; so we have a lock on that resource if you will. If we have that then, we have the first item that's required for a deadlock. ",
                    "The second one is called a hold-and-wait and that means that while a thread is waiting on the allocation of a new resource it retains all of its existing locks on the old resources. So, if a thread obtains a lock on A and then requests a lock on B, and the lock on B is not available immediately, the thread is going to be blocked. So, it means that the thread could possibly have a lock on something and be in a block state at the same time. So this happens, we understand that this happens, certainly when we're creating more than one lock at any given time.",
                    "The third thing that we need in order for deadlock to happen is that there is no preemption. The operating system cannot forcibly remove a resource from a thread. So, for example on a semaphore, once we've decremented the semaphore counter and it's now zero, the operating system has no way of intervening and saying, \u2018oh whoops I'm sorry you no longer have that lock on that semaphore and you have to go back to doing something entirely different.\u2019 We usually don't have that as a possibility; it's the idea of no preemption.",
                    "",
                    "The last one: and the last one is just case of bad luck it's called a circular wait. And that's a closed chain of threads in which the last thread is waiting on the first thread. This could be as simple as A waiting for B, and B waiting for A, but it's often not as simple as that. We can have four different threads: where we have A, B, C, and D. And we have A waiting for B, B waiting for C, C you waiting for D, but D waiting back for A, so it's not easy to recognize that a circular wait has occurred; it\u2019s not just take a snapshot and see which process, which threads are waiting for which threads it doesn't work that that easily. There's lots of levels of iteration and recursion that might have to happen in order for us to detect that the circular wait is actually happening. If we have all four of these things in play: we have mutual exclusion because the operating system provides us with locks, we have hold and wait because we can have more than one lock and we request one lock separate from the other locks, and we have no preemption meaning the operating system can't take things away from us, and we have a circular wait which just happened to occur, we have a deadlock.",
                    ""
                ]
            },
            {
                "title": "Solutions for Deadlocks 1.23",
                "data": [
                    "So there's three ways of dealing with deadlocks: and the first way is called deadlock prevention, the second is deadlock avoidance, and the third is deadlock detection. Deadlock prevention requires us to take away one of those four requirements; either we're going to take away no preemption, or we're going to take away holding weight, or we're going to avoid the possibility of a circular weight. The idea of taking away preemption, kind of difficult, we\u2019ll talk about that in just a minute. We might have dead lock avoidance; deadlock avoidance creates an algorithm in which we are going to check the system to see if a deadlock could occur whenever we make an allocation for a resource. So, now the operating system now is going to intervene, and say, if you're going to make a request if you're going to lock something, the operating system will check the state of the system to see if a deadlock will occur as a result of you're making a lock on that. And if a deadlock will occur as the result of you're making a lock on that device, the lock is not approved and the thread is either told to go do something else or it's blocked all together until the lock can be approved. So, deadlock avoidance says we recognize that all four requirements for a deadlock are in play, however, we're going to avoid the deadlock by preventing such a situation just from occurring. Deadlock detection is the last way and it's the easiest way, it just says deadlocks are going to happen we'll let them happen and periodically we'll just check to make sure that if there are any dead locks that we just deal with them in some method. So, deadlock detection is the easiest, so to speak but it's also the most difficult to deal with after the fact. So, now we have to deal with the fact that a deadlock has occurred, and we have to recover from it. Those are our solutions that we have for dead locks and we're going to look at them individually.",
                    ""
                ]
            },
            {
                "title": "Deadlock Prevention 1.24",
                "data": [
                    "The idea of deadlock prevention is that we eliminate one of the four requirements for a deadlock. The first one is mutual exclusion, and it's really hard to get rid of mutual exclusion because it's usually a requirement. Whenever we're working with threads, we recognize that these threads are going to have the possibility for asynchrony and if they have the possibility for asynchrony, we have to provide a possibility for mutual exclusion. We have to provide the protection mechanisms for mutual exclusion; so getting rid of mutual exclusion is usually not a possibility. ",
                    "Hold-and-wait: we can say that all locks are required to be obtained at the same time and either they're all approved or the thread is blocked. Now if we do that then there's no additional lock that might occur. If a thread holds a lock, it cant obtain another lock, which means that we can't get into a situation where we're holding a lock and we're asking for a new lock and therefore, we're blocked because we're asking for the new lock. So, if we make the threads ask for all of its locks at the same time, then we've eliminated the possibility of hold-and-wait. ",
                    "No preemption: we can say that if a thread requests a new lock and the new lock is denied then the thread has to release all of its existing locks. So, that's a possibility, it's got to require a callback from the operating system to the thread to say if the request for the lock is denied unlock everything else. Put it back in a safe state; undo what you were doing and release those locks. The operating system might also have the possibility the authority to remove existing locks individually but that's a bit of a bad idea.",
                    "The last one is actually an easy one to deal with especially if we're talking about locks on individual items that the programmer can control; one of the easiest solutions for dealing with the circular wait is to order all the resources. We can number them and order them so that there's never a possibility of holding a higher level resource and requesting a lower level resource. So, one of the problems of the circular weight is that the last thread is waiting on the first thread. What that means is that the first thread holds a lower lock and the last thread holds a higher level lock and if we prevent the last thread from accessing its lower level lock, because it holds the higher lock number then we've prevented the situation of the circular wait. Either that higher level, that last thread will either release the higher level lock and then request the lower level lock allowing other threads to continue or it won't be allowed to do it. So, the circular wait is actually an easy one and it's the one that's recommended by a lot of computer scientists because the programmer can protect his own program internally, just simply by ordering the locks, ordering resources. So, that we never ask for a lower lock while we're holding a higher lock",
                    ""
                ]
            },
            {
                "title": "Deadlock Avoidance 1.25",
                "data": [
                    "In the deadlock avoidance scenario, the operating system has to know everything that the thread is going to do before it actually does it. So, the operating system has to know what locks the thread is going to want, in order to look into the future, in order to have a premonition about what could happen in this deadlock scenario. So, what the operating system does is it takes all the information for all the threads that are running, and says that whenever a thread makes a new request, the operating system is going to run what's called a banker's algorithm, or what's often called a banker's algorithm. And the idea is that we know all the resources that the operating system has, we know all the resources that the threads have, and we have some premonition about what the resources the thread is going to want in its entire lifetime. So, we can tell which threads are going to run to completion just by knowing what we currently have available and what the threads are going to want before they're finished. And what we can do is constantly check, whenever a resource is requested, the operating system is going to run the bankers algorithm to try and figure out if the allocation of that request would cause the system to be in what we call an unsafe state. And an unsafe state is one in which all the threads cannot complete, rather that we can say that any thread cannot complete, so we're protecting the system from ever getting into that unsafe state. So, we sort of simulate by making the allocation in a simulation, and saying can we now finish all of the threads and if we can, we can say that that allocation keeps us in a safe state and we make the allocation and if we can't then we deny the allocation. So, the system is by definition always in a safe state, but we're checking every single time to make sure that the allocations will not put the system in an unsafe state. It\u2019s a lot of work and we usually don't have information about what the threads are going to ask for ahead of time, until they actually ask for it. So, that makes dead lock avoidance a very rare strategy.",
                    ""
                ]
            },
            {
                "title": "Deadlock Detection 1.26",
                "data": [
                    "In deadlock detection, it's really easy going. All it does is say we have all the requirements, there\u2019s a possibility for a deadlock, so we'll check periodically to see that the deadlock is not occurring. It's not at all restrictive; we're not saying that we were going to stop any situation from occurring. We're saying in fact the exact opposite: that deadlocks will occur and when they do occur we're going to have to deal with them, and we'll talk about that in just a minute. But we recognize that the deadlocks will occur and that periodically we're going to have to check the system to see if there are any dead locks that are in existence. Once we detect the deadlock, well we have to figure out what that's going to do and the only ways that we can really deal with a deadlock are either rolling back the deadlock process, or the deadlock thread, to a previous unlock state that's useful for things like SQL servers and database servers, but it requires transaction logs. So, we have to know what this thread, or what this process that's deadlock did in the past X time interval, so that we can roll it back to that X time interval and either restart it or let it continue from a previous state, when which it wasn't deadlocked. We could also just say let's abort all the deadlock threads; we'll kill them all. We could abort a single thread and see if the deadlock is being resolved, so we can arbitrarily just pick one of the deadlock threads and say it's now killed, its resources are released, and now see if the system is back to a stable state, let it run again and see if there's a deadlock or not. Or we can start pre-empting resources. ",
                    "Unfortunately, in all of these situations we have the potential, except for rolling back, in all of these other situations we have the potential for catastrophic loss of data or corruption of data. Basically we're saying that we don't think that the system is in a consistent state, but we really have no other resolution other than to just say we have to take some emergency action and that means we recognize that if the data is corrupt we have to deal with that. So, deadlock detection says, just let it happen and then we'll deal with it afterwards, we hope it doesn't happen. And believe it or not this is the one that's most common today, that we just say let it happen and if it does happen we'll deal with it. You might have had this in your past you have an application that just completely freezes up on a Mac, you get the beach ball on a Windows machine you just get the screen graying out; it's doing something we have no idea what it's doing, so what happens we kill it. We are the deadlock detection algorithm; we have recognized that a deadlock has occurred and we abort all the deadlock threads by killing the entire process. So, congratulations everybody has been promoted to a deadlock detection algorithm.",
                    ""
                ]
            },
            {
                "title": "Dining Philosopher\u2019s Problem 1.27",
                "data": [
                    "I'd like to take a look at one classic computer science problem. It was first presented by Dr. Edsger Dijkstra who was a famous computer scientist that you're going to hear a lot about Dr. Dijkstra with his various assorted algorithms throughout your careers in computer science. But it's a computer, sorry, it\u2019s a concurrency control problem and what he said was\u2026 and I really have no idea how he came up with this idea, this scenario but somehow he created it. He said that there's a house where we have five philosophers and the Philosopher's are numbered 0 through 4. And they alternate, they only do two things, philosophers only do two things: they eat and they think. And when it's time to eat, somebody, we were not figuring out who, sets up a circular table and it has a place setting for each philosopher. And the place setting consists of one fork on the left side, and one plate in front of the philosopher. The meal unfortunately that they're going to eat is a particularly difficult to eat kind of spaghetti. I\u2019m quoting dr. Dijkstra here, this is not me, this is Dijkstra's dining philosophers problem. I don't know why spaghetti, or why you eat spaghetti with two forks, but he said that you need to have two forks to eat this kind of spaghetti. And unfortunately, each philosopher is only given one fork. But we recognize that there's a fork on the left of the philosopher and a fork on the right of the philosopher. ",
                    "The way that we can think about this is that when the philosopher comes he can pick up the fork on the left, and he can try and pick up the fork on the right, the unfortunate problem is that if all five philosophers come to the table at exactly the same time, all of them will pick up their left forks and there will be no right forks. In other words, all of them are holding a lock where there's mutual exclusion because the philosopher doesn't let go of his fork, all of them are holding the lock on the left fork and waiting for the lock on the right fork. All of them have no preemption so nobody's going to reach across the table and start grabbing a fork these are nice philosophers, I guess they're not from Brooklyn, I'm kidding. And we have a circular wait we have the last philosopher in a sequence waiting for the first philosopher on the sequence. So, ultimately what we have is all four requirements, we\u2019ve met the deadlock requirements, and in fact you can see that we have a deadlock. In this scenario all the philosophers will starve to death, because none of them will put down the fork and allow somebody else to eat.",
                    ""
                ]
            },
            {
                "title": "The Dining Deadlock 1.28",
                "data": [
                    "So like we see, if all the philosophers are allowed to take a fork, all will have one fork and none will ever eat. So, what's the solution to this? Well Dijkstra proposed the solution of resource ordering, of course. That's the easiest one. And he said, let's order the forks zero through four. Now, if you pick up fork zero, you're not allowed to pick up fork four, and that leaves fork four for the philosopher on his right to pick up, and that means that the fourth philosopher will be allowed to eat because he has his own fork plus the last fork. We can't select a larger, we can't have a larger fork and a smaller number fork at the same time unless you ask for the smaller fork first then the larger fork second. So ultimately what this means is that one of the philosophers will get both forks, will eat, and put both forks down, allowing more philosophers to eat and more philosophers to eat and ultimately, everybody eats. So that's the resource ordering solution. But there's another solution in which we have a semaphore. And obviously, we're going to have semaphores for each of the forks, you can see that already cause we need mutual exclusion on each of the individual forks. What we're talking about is adding yet another semaphore, and saying that there's a semaphore that has to be met that has to hold mutual exclusion for the entire room. So with the semaphore solution, we have four signals that go into the queue under semaphore; the first four philosophers are allowed to enter the room, and they're allowed to eat. The last philosopher, of course the fourth philosopher, will pick up the fork from the fifth philosopher because he's not allowed in the room at all. And that allows the fourth philosopher to eat, and then he'll put his fork down, allowing the third philosopher to eat, and then he'll put his fork down, and so on and so forth. Eventually, after the fourth philosopher leaves the room, the fifth philosopher will be allowed in and he'll eventually be allowed both forks; everybody is allowed to eat and the problem is resolved. So, we've got two scenarios here and there's actually even more scenarios that get even more complex; there are more solutions that get more complex. There are solutions using a monitor, and there's even more complex ones. But the fundamental, the easiest to understand, is that we either order the resources, and order the forks, or we have a semaphore to prevent one of the philosophers from even entering the room. "
                ]
            }
        ]
    },
    {
        "module_number": 25,
        "module_name": "Memory Management Script",
        "file_name": "Module 25 Memory Management Script.docx",
        "transcript": [
            {
                "title": "Conclusion 1.29",
                "data": [
                    ""
                ]
            },
            {
                "title": "In This Module 1.2",
                "data": [
                    "We're going to look at what the operating system has to do to take care of all of main memory, all of the RAM in the system, and there's a lot that has to be done. So, in this module we're going to cover what the needs are for memory management: some requirements that we have to meet for memory management. We're going to talk about logical addresses and physical addresses and how to convert between them. We're going to talk about a few partitioning strategies for dividing up main memory. We're going to talk about what is virtual memory, when it's in use, how it works, and that sort. We're going to talk about working set, and resident set strategies related to virtual memory, and what the those definitions are. We're going to talk about load control and we're going to talk about how pages and memory can be shared between processes. So, we\u2019ve got a lot to cover in this module; let\u2019s get to it.",
                    ""
                ]
            },
            {
                "title": "Reasons for Memory Management 1.3",
                "data": [
                    "When we look at the operating system, we're looking at a multiprogramming system. And in a multiprogramming system, obviously, we're going to have a lot of processes running all at the same time. And since we're going to have a lot of processes running all the same time, there's never going to be enough main memory. And I've been in this industry now for over twenty years and I can tell you that the problems never change. The sizes certainly do change; when we started out, when I started out in this industry we were talking about main memory sizes of probably around one-hundred twenty-eight megabytes or two-hundred fifty-six megabytes. So, your average system might have two-hundred fifty-six megabytes of main memory and if you bought a system like that today it would be absolutely useless. And we look at more common systems today of eight gigabytes, sixteen gigabytes, probably thirty-two gigabytes is not in the very far distant future, and we think that the problem is solved by adding more memory and it isn't. There will never be enough main memory. Wherever we add memory, programmers tend to use that memory.",
                    "",
                    "So, it's always a little bit of a battle between hardware and software with the hardware trying to keep up, usually, and the software trying to use more and more memory to do more and more work. The operating system is going to be responsible for allocating memory to lots of programs running in the system all at the same time, and the operating system is going to need to move parts between main memory and secondary memory. So, there's a lot of work that the operating system has to do even though we have main memory and main memory just looks like a very large array. We're going to have to divide that up and share it among a lot of different programs that are all running at the same time; that all want access to that main memory, and that all want to use more and more and more.",
                    ""
                ]
            },
            {
                "title": "Memory Management Requirements 1.4",
                "data": [
                    "The operating system has to meet a number of requirements, in terms of taking care of main memory. The first one is relocation. In years past, the operating system, or rather the program, would be the only thing running on the system. If we look back at the old era of the IBM mainframes, or even older than that when we look at really big computers, there was only ever one program running on the system at any given time, and since there was only one program that program had access to the entirety of main memory; it could do whatever it wanted with it. So, in reality the programmer had access to all of main memory inside his program and the programmer could put portions of the program, portions of data, portions of code, anywhere he wanted inside main memory. Unfortunately, that's not true anymore. We're looking at a system where we have a lot of programs running, and we don't know what the order of the programs is that are going to start; we don't know what's free when a program starts, we don't know what's occupied when a program starts. And the operating system is going to have to be responsible for putting a running program into a particular space in main memory and during its lifetime even while the program is running, not actively running on the CPU, but maybe in the ready state or in the block state or even in one of the suspended states. The operating system should be able to relocate that process to a completely different section of main memory, I mean actually take it and move it to a different place in main memory. And so, that's relocation and the operating system has to be able to do that, so that's one of our requirements. ",
                    "",
                    "Another requirement is protection. The operating system will be the only entity in the computer system, which is allowed to access all of main memory. A program cannot be allowed to access another program's memory space, and that's a prime tenet of memory management. So, that we need to make sure that one program cannot interfere with another running program in the system. In fact, one program shouldn't even know that another program is necessarily running without the intervention of the operating system. Now obviously, two programs may need to communicate and we have facilities for doing that; we have the inter-process communication facility. But overall, we should say that protection should be enforced, such that one program cannot access another program's memory space. And we saw this in when we've talking about pointers in earlier modules, because if we let the pointer go outside the memory space of the program then the program would be shut down and that was protection. If you remember or if you ever saw that message that came up in Windows it says, \u201cMicrosoft is sorry but your program has performed an illegal operation,\u201d that's a protection mechanism. Your program, probably via a pointer, was trying to access memory outside of its memory space, and so Windows intervened and said that's not allowed and it shut your program down immediately. One of the problems with this is that the operating system can't prescreen. We can't go through and look at all the memory accesses that your program is going to do before you actually do them, because that would take just too much time. It has to be done dynamically. So as you make a memory request, the CPU, the hardware actually has to look at the memory request and decide whether it's in your memory space, your program's memory space, or outside your program's memory space and should be terminated. So that's protection. ",
                    "",
                    "We also have some situations where sharing might make sense, and we'll talk about these later on. Where the process is actually going to share code with another process. In fact we're going to talk about one situation where we execute a process, and then we fork and create an exact duplicate of this process as a new process. So, we'll talk about that in a little bit. ",
                    "",
                    "We can talk about logical organization where we create modules. So, a lot of programs are written these days using shared objects or dynamically linked libraries and what this is is rather than creating every possible function. So, for example when you were writing your \u201chello world\u201d program, your simple \u201chello world \u201cprograms, you didn't overload the output operator for the IO stream class. You had a library that did that for you and all you had to do was pound include IO stream. Well what that did was tell the operating system that, through a lot of steps I'm oversimplifying it, but that tells the operating system that you're going to use a library and one of the libraries is the Microsoft Visual C++ library MSVCRTDLL. Inside that library is the code for the function on how to output to C-out to the O-stream class. So you don't write that code, it's already in Windows and every program that uses C++ has use for that code. Well we don\u2019t want to load that code with every program. We want to load that code once and have it shared between every program that was written in C++. So that might be seventy programs and we only have one copy of that MSVCRTDLL, it's not a large DLL maybe it takes up only one megabyte or two megabytes. But if we're talking about seventy different programs using it. That means we're saving between seventy and one hundred forty megabytes of memory and that all of a sudden becomes significant. ",
                    "And then we'll talk about the memory management requirement of physical organization, which is really just how we map a logical memory addresses to physical memory addresses. So, these are the requirements that we have to meet. So, we've got relocation, protection, sharing, logical organization, and physical organization. And those are the requirements that the operating system has to meet in order to complete its memory management tasks.",
                    ""
                ]
            },
            {
                "title": "Logical vs Physical Addresses 1.5",
                "data": [
                    "The operating system understands physical addresses, and the computer system, of course, understands, as you saw back when we were talking about instruction sets, the computer system understands physical addresses. main memory is broken down in physical addresses. so if you have four gigabytes of main memory, for example, you have adverse zero through Agis four point two billion. So, the computer system understands a physical address and expects to be communicated with physical addresses. Unfortunately, the programs will only be able to use logical addresses. And that's because the program has no idea where it will be loaded physically. So, we can do this quite simply, we can think of this quite simply, as a relocation problem. If we can use the offset from the beginning of the program, since the program should have no access outside of its own memory space. If the addresses that the program is going to use for all the pointers, and all the code, and all the jumps, and everything that it's going to do, if those addresses could be logical addresses relative to the offset from the beginning of the program, then the program doesn't really need to know where it's physically loaded in main memory. ",
                    "",
                    "So, if we have that then we're going to recognize that those logical addresses need to be converted dynamically at run time into physical addresses. And so what we can do is use what's called the hardware memory management unit, CPUs have now created they were evolved in the 1980\u2019s to have this hardware memory management unit, and that would be responsible for converting during run time the logical address to the physical address. And it's not a difficult calculation, in a very simple example, we can say that the hardware memory management unit can know the starting address of the program. and then when it sees a memory reference, when it sees a pointer, when it sees it jump to code, or anything like that, when we see that logical memory reference the hardware memory management unit will convert it by adding the base address of the program into the logical address that the program is trying to reference to produce a physical address. And then the CPU will actually access that physical address, rather than accessing a logical address that the program is asking for. So, at run time without the operating systems intervention because remember the operating system is asleep while the program is running, the program will make a reference to a logical address and then the OP, the hardware memory management unit, will convert that logical address into a physical address and make the appropriate reference.",
                    "",
                    "So, the program doesn't have to know anything more about where things are physically located, and it meets our relocation and protection requirements because we can now physically move the entire process to a completely different location, while it's not running and then update the hardware memory management unit the next time it runs to tell it where the new, where the process is now. And then the hardware memory management and of course will do that calculation at run time. So it doesn't require a lot of work it meets our protection and relocation requirements; this is a perfect solution.",
                    ""
                ]
            },
            {
                "title": "Partitioning Strategies 1.6",
                "data": [
                    "When the system boots up, we\u2019re looking at what is effectively a desert. There is nothing in main memory, short of a couple of locations where we can communicate with hardware, but the majority of main memory is simply completely empty. And as the operating system, it becomes our responsibility to divide main memory into sections where we're going to allow programs to use. And we're going to have to talk about a number of different partitioning strategies for how to get that done. So, the core of the idea is that we're going to look at a way to break down main memory into different sections so that the programs can all be in their own individual sections, so that's again part of our part of our relocation of protection requirements, more protection than anything else that we can set guidelines we can set borders for a process. But we have to recognize that when the system starts memory is just empty and it's just available for us to use. So, some of the strategies we're going to look at our fix partitioning and dynamic partitioning. We\u2019re going to take a look at a buddy system which is sort of fixed and dynamic both. Then, we're going to look at paging and segmentation and paging segmentation is really what's done today. But we have to understand fixed in the name of partitioning before we can get into the more advanced stuff.",
                    ""
                ]
            },
            {
                "title": "Fixed Partitioning 1.7",
                "data": [
                    "What we do in fixed partitioning is when the system boots up, we can divide main memory into some static number of parts. Let's imagine a situation where we have four megabytes, four gigabytes of main memory and we're going to divide it up into let's say sixteen different partitions, now those could be either equal sized partitions or unequal sized partitions. And if they are equal sized partitions, then let's imagine that those sixteen different partitions, each run two-hundred and fifty megabytes in size, so our four gigabytes of main memory is broken down into sixteen equal partitions of two-hundred and fifty megabytes each. Now that works; it\u2019s fine. It means we have a maximum limit of running sixteen different processes; we can't run any more than sixteen different processes. and one other problem is if we start up notepad, and notepad is going to just edit a small text file and it only needs one megabyte of main memory, well the smallest we can put it in is two hundred fifty megabytes, because we have equal sized partitions. So, what that means is that we're wasting two-hundred and forty megabytes of main memory and that's pretty significant. That's what we call internal fragmentation. We have this division allocated, for notepad, we give it two-hundred and fifty megabytes for notepad and notepad only really wants one megabyte. And the other two hundred forty megabytes now are wasted in internal fragmentation.",
                    "",
                    "One solution that we could use is to maybe use unequal sized partition, so partitions which are differing in size. And if we do that we might say there are sixteen different partitions, one partition is very very big it's one gigabyte, and other partitions are very very small there might be ten meg, ten megabyte partitions or just slightly larger. The point is that we're always going to have some internal fragmentation with these solutions, but we're going to maybe minimize the amount of internal fragmentation. ",
                    "",
                    "Now if we use unequal sized partitions, we\u2019re going to have to worry about which partition we decide to place this process in. Let\u2019s say we open up notepad and we open up a small text file and so the operating system decides, okay we'll put it into one of the ten megabyte partitions and then somebody chooses to open up an enormous file of one gigabyte file. Let's say a video file but they're opening in Notepad. Well now the operating system has to move that process out of its memory space; we can do that because of relocation and move it into the one gigabyte partition that we have, the only one gigabyte partition that we have. And what happens if that one gigabyte partition is taken by something else, maybe chrome, and now notepad cannot run at the same time as Chrome, because they both need that one gigabyte partition. So, there are some real disadvantages to having fixed partition. But one of the big advantages of it is it's really simple, really requires very little effort on the part of the operating system. But the downside is that we have very limiting factors on the number of processes that we can run, and we also waste a lot of memory in internal fragmentation.",
                    "",
                    "Dynamic Partitioning 1.8 ",
                    "Alright, fixed partitioning didn't work out great. What if we took an opposite end of the spectrum and said, okay let's make dynamic partitions. Let's ask the process exactly how much memory it needs and let's give it exactly that much memory. So, if a process says it needs ten megabytes; we\u2019ll give it ten megabytes. Now that causes a problem with the process decides to grow to later on we don't have an issue, we don't have a way of dealing with that. But if a process is in a particular memory space, it can grow as much as it wants inside that memory space. So, once we allocate a memory space for that process, it really can't grow but it can use up everything that that's inside that partition. But what this causes is actually external fragmentation, extremal fragmentation is caused by the dynamic partitioning and what this is is wasted memory between allocations. ",
                    "",
                    "Now what's happening here as you can see in the diagram, is that we created three different processes. We created three processes and the middle process sort of disappears, it ends. Now we've got that little chunk of memory free, but what we would like to do is run a program that's just slightly larger than the space that was allocated there. And unfortunately because it was larger, I can't allocate that space; because the new process is larger than the space that was allocated, I can't put that new process into that memory space. So, what I have to do is put it at the end and that means we're we're using up more and more and more at the end. And then of course what happens if all we have are these little slivers of memory left over and I want to run a big process, which is not so large that it would that we don't have enough memory free memory for it, we have enough free memory for it but we have as these little segments of free memory. And we have to compact all the running processes now into the beginning of main memory to make a big space at the end, so that we can run the new process and that of course requires a lot of copying of memory, a lot of CPU time and that's a big waste of time. We have an issue with dynamic partitioning in that the data structures for the operating system get rather complex because we have to record both the start and the ending location of the process. ",
                    "",
                    "And then the last problem is where do we choose to put a process if we have enough space available, assuming that we have enough space available, where to be choose to put a new process into its into what partition. So, we could have the best fit which is the area size closest, and of course larger, than what the process is asking for. If the process is asking for ten megabytes and we have an eleven megabyte sliver of available then, we can put this ten megabyte process into the eleven megabyte sliver of available space. Well of course unfortunately that means that the last one megabyte is not inside of partition, it's still available space we didn't know allocate it but it's never going to be used because its so small; we\u2019re never going to a process that so small it'll fit into a one megabyte little sliver of space, and that's called external fragmentation because it's outside of the area of an allocation. We could use first fit, which just says the first spot that we becomes available when we start looking, so once we start looking from the beginning of main memory. If a spot is available that's larger than what the process is asking for we'll just put it in there. And the next bits as we begin looking from where we last left off. Of all the options it actually turns out that next fit has the minimum CPU time with the best average utilization of memory, but the point is moot because we don't do this anymore. Dynamic partitioning has so much overhead, that we just can't do it in today's environment.",
                    "",
                    "Buddy System 1.9 ",
                    "The buddy system is actually a compromise between the fixed and the dynamic partitionings. And again there's not a lot of operating systems that do this today, in fact I don't know any of them, but for example, Linux uses this for what it calls its kernel slab allocator. So, when the kernel itself needs to create some memory for the kernel, it will use a chunk of memory and divide that chunk of memory as a buddy system, just to make smaller partitions of a larger chunk of main memory. So, while we don't do this on a large scale for all of main memory, there are certainly situations where even Linux today will do this for parts of its slab allocator for creation of, or for allocation of smaller parts of main memory for kernel purposes. Anyhow, what it does is we take main memory and divide it in multiples of two. And the reason of course that we're doing multiples of two is because it makes it easier on the calculations; we don't have to necessarily record the starting in the ending point, we can record what multiple of two this is and so on and so forth.",
                    "",
                    "Let's take for example, a two megabyte memory allocation, so let's say the the Linux kernel has a two megabyte memory allocation for it and wants to divide it up for purposes for storage of an IO table, or storage of a main memory management table, or who knows. It\u2019s got to store some portion of information but two megabytes would be a huge waste, it needs something much smaller. The kernel means is let's say one hundred kilobytes, for whatever it's going to do. The process then is that we take that two megabytes and start breaking it down into divisions, so you can see that what we've created initially, we take the two megabytes and divide it in half so that we end up with the a one thousand and twenty four kilobyte allocation and another one thousand twenty four kilobyte allocation. But that's way over what we need of hundred kilobytes, so we break down one of the one thousand twenty-four kilobytes into two five hundred twelve kilobyte allocations. Then we'll take one of the five hundred twelve kilobyte allocations and break that down into two two hundred fifty six kilobyte allocations and that's still too much. So, we take that and divide it down again into two one hundred twenty eight kilobyte allocations. If we were to divide the one twenty eight KP allocation again, it would be a sixty-four kilobyte allocation and that's too small to fit the hundred kilobytes need that we have. So, we would have to we would have to stop there and use one hundred twenty eight killed by an allocation.",
                    "",
                    "Now this results of course in internal fragmentation, because it's going to end up where\u2026 we\u2019re going to end up allocating one hundred twenty-eight kilobyte, when only one hundred kilobytes was needed. But we're talking about twenty-eight kilobytes so it's not an overly significant factor. We might end up with some extra fragmentation if we have a lot of these small slivers but one of the nice features of the buddy system is what's called coalescing. We can take those two hundred twenty-eight kilobit allocations, and recombine them up into a two hundred fifty-six kilobyte allocation. and we can keep doing that ultimately getting back to the original two megabyte allocation, if we ever need to. So, coalescing allows us to bring these back into a larger hole. ",
                    "",
                    "If we have a large number of hundred kilobytes processes, we might take that hundred one hundred twenty eight kilobyte allocations that we already have but we'll also divide the two fifty-six into two one twenty-eight. We divide the five twelve into two two fifty-sixes in each of those into one twenty-eight. So, we can we can create a lot of one hundred twenty-eight kilobyte partitions out of this original two megabytes. So, we end up decreasing the amount of internal fragmentation, overall, because if we had only two megabyte allocations there would be one point nine megabytes of completely wasted space, because we'd have to allocate two megabytes for one hundred kilobyte allocation. It makes the operating system structures easier, because they're beginning and ending on a two to the N boundary it. It actually does work quite well, the only problem is coalescing back does take some effort. And so Linux actually delays that, it's called delayed coalescing. It waits until much later on, when the operating system is not very busy and it says: \u201cyou\u2019re probably not going to use these anymore\u201d and then it recombines them back up. But ultimately this does work; it doesn't work on a grand scale, it works on a micro scale. But it is a solution that involves both fixed and dynamic partitioning and it is a compromise between the two.",
                    ""
                ]
            },
            {
                "title": "Paging 1.10",
                "data": [
                    "So, let's look at something that actually happens in today's environment: it\u2019s called paging. What we do is take main memory and we break it down into a lot of equal size frames, and each of the frames are the same size, every frame is the same size. And it's something small like four kilobytes; it's not something significant. Then what we do is we take down a process, take a process and we break that down into the exact same size pages. So, we\u2019ll take the process and break it down into four kilobyte pages. Now I'm sure you can see where this fits, and that is that one page fits exactly into one frame. This is RAM okay, and what RAM means is that we have random access memory, that means that every single frame, in fact every single byte, can be accessed all at the same amount of time. So, there's no benefit to keeping the process as one coherent entity. There's no reason we should keep the process in sequential memory; that wasn't a failed assumption that we had for fixed and dynamic partitioning. That we had to keep the process all together; we don\u2019t. ",
                    "",
                    "If we have bits of the process at the beginning of main memory and bits of the process the end of main memory, it doesn't matter as long as we can keep track of them in order. We can access the front of main memory and the back of them in memory all at the same time. So, there's no reason that the process needs to be continuous. Now the operating system is going to have to keep track of where each page is located in main memory; what frame number each page is loaded into. And it can do that, it does that, in what's known as a page map table or some books call it or just simply a page table. And the page map table is simply in an array of frame numbers and if we look at index three, for example into the page map table.",
                    "That's going to tell us where the fourth page, zero one two three, where the fourth page of the process is located; what frame number the fourth page of the process is loaded into. So, we can look now into the page map table and find out where each of the pages in the system is in physical memory, and that's exactly what the hardware memory management needs to do. ",
                    "",
                    "Whenever any logical memory request is made, to a logical address, basically to the offset from the beginning of the program, the hardware memory management unit needs to calculate how to convert that logical memory address into a physical memory address. And it doesn't take a lot of effort to do so it can simply do a number of bit shift left and bit shift right to get the the physical address from the logical address. and one of the features that it's going to have to do is a look up into the page map table. So, the format of the page map table now is going to be defined by the hardware manufacturer and not by the operating system designer, because the hardware needs to know first where the page map table is stored and each process is going to need its own page map table; each process has its own page map table. And the hardware memory management unit is programmed, during a switch, during a context switch, it's programmed for where that page or app table is. And the hardware memory management unit will look up in the page map table where the appropriate page is loaded and do the conversion between page number and frame number and then look at the offset. So, this conversion process is a little bit involved, and we\u2019ll see it a little bit later on. But the benefit here is that we can use paging, and we'll see the benefits of paging the next slide.",
                    ""
                ]
            },
            {
                "title": "Benefits of Paging 1.11",
                "data": [
                    "Well let's look at the benefits of paging. There\u2019s no external fragmentation; every frame of main memory is equally good, and we can put any page in any frame and there's no detriment. So, there's no reason to keep pages together, which means any pages, any frame is available for use as long as of course it's not occupied. So, there's no external fragmentation. Some argument is that there's a minimum of internal fragmentation. Okay, yes there is internal fragmentation, but how much? We're talking the maximum amount of internal fragmentation we could have is four kilobytes minus one byte. So, if the process is going to use one extra byte beyond the end of a four kilobyte page, then it's wasting four thousand and ninety five bytes of main memory. A process wasting four kilobytes of main memory is not terribly significant, even if we had one hundred processes running on the system that means a maximum of four hundred kilobytes of main memory wasted and that's insignificant in today's environment. So, we don't care about it; so we say yes it's going to have some internal fragmentation possibilities, but it's so small we just don't care. ",
                    "",
                    "Protection is really easy because if the process is making a reference beyond the end of its page map table, the hardware memory management unit won\u2019t be able to convert that into a physical memory address. We know there's a problem and we can interrupt and call the operating system immediately to say that something's gone wrong. So, protection becomes really easy the process literally cannot access anything outside its own memory space. relocation becomes easy and, in fact, relocation now is talking about relocating a page. ",
                    "",
                    "We don't necessarily have to relocate the entire process; we could if we want but if all we need to do is relocate one page, we can just update the page map table to indicate where the new the pages now located what the new frame number is and we're done. It really doesn't take a lot of effort, of course we have to copy those four killer bytes, but copying four kilobytes is not terribly significant anyhow. Besides why would we do relocation? There's very little reason in an simple paging system, there\u2019s very little reason to move a page from one to the other. And then easy sharing which we're going to see a little bit later on. If we just have two processes that want to share a page and it's allowed to do so, doesn't violate protection, if they're allowed to share a page then we can simply enter the same frame number into their page map tables at the same location and we don't have any problems with that. So, there's a huge number of benefits to paging and that's why we'll never go back to the old fixed and dynamic partitioning systems.",
                    ""
                ]
            },
            {
                "title": "Converting Logical to Physical 1.12",
                "data": [
                    "I wanted to just look a little bit more at how we convert a logical address to a physical address in a paging system, and I know it sounds a little bit complex but we're going to boil it down and make it really easy. We have to ultimately remember that what we're trying to do\u2026",
                    "We're given a logical address that is the offset from the beginning of the program, and the program doesn't know that it's not, that it's memory is not completely sequential. The program doesn't know that; it sees logical addresses that are all completely sequential. So, what we're going to do is use a very simple calculation, which is going to be that the physical address is equal to a look up in the page map table for the logical address divided by the page size multiplied by the page size plus the logical address mod the page size. And you see that that's a divide and a mod operation and you say, well wow that's got to be a lot of work; it would be if the page size were something other than a multiple of two a factor of two. And that's what makes this really easy because the divisions and the mod operation are now simply going to become bit shifts. So, because we have the bit shift, and bits shift really doesn't take much effort inside the CPU certainly not in comparison to a divide operation. To calculate the page number: we\u2019ll do bit shifts to find the page number, we\u2019ll look up the page number in the page map table, we\u2019ll find out what the frame number is, do the bit shift again as is the multiple of the page size, and then we'll add the offset. And to calculate the offset we simply looking for the latter half of the physical, excuse me, of the logical address. So, we can do an XOR to wipe out the early parts of the logical address and end up with just the offset, and that tells us where the page is located (what frame number it's in) and how far into that page the offset we're looking for for that particular byte of memory that we need. And that's how we convert a logical to a physical address in a paging system.",
                    ""
                ]
            },
            {
                "title": "Segmentation 1.13",
                "data": [
                    "The idea of segmentation makes sense and there's a lot of systems that do it today; it\u2019s very popular. Fortunately, it's not substantially different than paging. The only key difference here is that we\u2019re now allowed to have unequal sized partitions. So, in paging system, we're assuming that somebody came down and said that it's going to be a four kilobyte page size and you might argue well what if I don't want a four kilobyte page size, for some reason I want something larger or something smaller. Segmentation says, in a simple segmentation system,you could use any segment size and we don't call it a page anymore, we call it a segment; you can choose any segment size. But now the logical addresses really have to change because we need to talk about not just the the logical address, we need to break the logical address into two parts: a segment number and an offset into that segment. And then the operating system can keep different segments in different locations, of course, we need space for different segments.",
                    "",
                    "So, this is kind of leading to a more dynamic partitioning strategy, but the segments don't necessarily have to be contiguous, so we can now divide them up. This isn't as as universal as you might see a paging system. What usually happens in a segmentation system is the CPU designer will give you a few options of segment sizes to choose from and so you might not have just four kilobyte pages, You might have four kilobyte pages, sixteen kilobyte pages and sixty-four kilobyte pages. The reason for this is that the operating system if it knows it's going to need a lot of main memory for this process, we can use all sixty-four pages and then we have less of them. So, the page map tables are smaller and simpler to maintain. So, we don't really have the exact dynamic partitioning or the design of a simple segmentation system, where we have an infinite number of segments sizes. Instead we just have a few limiting segments sizes and the CPU designers decide what segment sizes we have as options.",
                    ""
                ]
            },
            {
                "title": "Virtual Memory 1.14",
                "data": [
                    "If we're using a paging or segmentation system, we have to recognize that the process is only going to be making one memory reference at any given time. Now I know that's an overly simplistic view of things, it's actually going to be making a lot of memory references. But one of the things that you can see is that the memory references are all going to be pretty much clustered together. If we, for example, start a process and the process throws a splash screen up on the screen. You know starting Word or PowerPoint, that splash screen will never appear again during that run of the program. But that splash screen takes a memory, there's code that tells it how to display that splash screen, and there's the splash screen itself probably an image. So, that memory is loaded into main memory and it's never going to be used again.",
                    "So the question then becomes: why should still be loaded in main memory, which is a finite quantity very much in demand, why can't we take it out of main memory and put it on the hard drive? Somewhere that we can bring it back if we ever need to, if the process never needs it again then it can just be deleted off the hard drive and never recovered. But what if the the process was never going to use that splash screen for that run of the process again. ",
                    "And what if we just took it out of main memory and left it on a hard drive. And if we did that we would free up some main memory and we would allow that main memory to be used by other processes. ",
                    "",
                    "So, this is the concept known as virtual memory; what we're doing is we're using a separate portion of the hard drive, it\u2019s not where the process is normally stored. It's a separate portion of the hard drive; Windows calls a virtual memory, I think Mac OS calls it the swap space, most Linux and Unix clones call it a swap space. But we have this area of a Hard drive dedicated for storing pages of main memory, which are no longer in main memory, which are no longer going to be used. Now the operating system can decide, dynamically, which pages to save in main memory, which pages stay in memory and which pages come out. And there has to be some operations that has to go back and forth between main memory and the secondary storage device, but it's a mechanism for actually saving a lot of main memory for more processes than would normally be allowed to fit. So, now if we say have only four gigabytes of main memory, and we want to run one hundred processes, each of which needs a gigabyte of main memory, under normal circumstances there's no way that we could do it. But with virtual memory, we would be allowed to do that; each of the hundred processes would have a portion of the four gigabytes, a portion of itself loaded in the four gigabytes, the rest of it would be on secondary storage and available. So, we have to move these pages in and out into main memory, and a lot of discussion to talk about how that's done, but we have that available now. We can use more memory than is actually in the system.",
                    ""
                ]
            },
            {
                "title": "What Stays? 1.15",
                "data": [
                    "So, what do we keep in main memory and what do we throw out? What is the process going to use and what is the process not going to use? How do we is the operating system, a group of people who had never seen this program before, decide what the process should be able, to be allowed to keep and what it should throw out?",
                    "",
                    "Well the first issue that comes up is that we have a resident set. The resident said is that portion that's in main memory, and we have a working set and that's the portion of the process wants to use. Now in order to run, the working set has to be in the residence set. So, the working set must contain the residence. So, we have to have the stuff that we want loaded into main memory. We're going to extend on that page map table that we talked about just a few slides ago and we're going to say that it now has an a present bit, and of course that means we have to update the hardware memory management unit and that's a much more involved process. The memory management unit has to know about all the stuff that's going on with virtual memory and it does, these days we know about it. The page map table is going to contain what's called a present bit, a P bit and the present bit tells the memory management unit whether or not that page is actually in main memory. If the page is not in main memory, then the present bit will be unset and then we know that we have to look for that page on a secondary storage device if the process wants it.",
                    "",
                    "We also have something known as a modified bit, and the modified bit is useful because we're going to make copies of pages in secondary storage before we realize that the the process doesn't need them anymore. So, what we would like to do is make duplications of the main memory into the secondary storage device, so that when it's time to decide to remove a page from main memory we can look for a page that has the modified bit unset. And what that means is that the copy on the secondary storage device Is the exact same as the copy in main memory, which means we don't have to go to IO to actually do the write operation we can simply erase the page out of main memory and trust that the copy on the secondary storage device is valid.",
                    "",
                    "As I mentioned the hardware memory management unit has to know what happens when it finds a present bit set to zero, and what it does is whenever it looks at the page map table and finds the present bits at zero, it's what's called a page fault. and the CPU stops running the process and instead switches to the operating system, very much in the way that an interrupt would occur, it switches to the operating system and begins running code to handle the page fault. And then the operating system can make the decision on whether the page fault was caused by a page which is no longer present, or perhaps was caused by a page which the process should not have any access to to begin with. And that's a page fault that's going to result in an exception error; that's going to shut down the process. That hardware memory management unit also has to recognize that the modified bit has to be changed any time we make a change to that page in main memory, so any write operations to a page in main memory would cause the modify bit to get set. But now the hardware memory management unit knows all about that and as the operating system we're just responsible for writing the code that brings in and out pages, and updates and maintains the present bits, and works with the modified bits.",
                    ""
                ]
            },
            {
                "title": "Benefits of VM 1.16",
                "data": [
                    "Of course the benefits of a virtual memory are huge; the programmer can look at this as a much larger memory space. So, now each program, or each process even, can view main memory as completely available purely to itself. In a thirty-two bit address space, each process is going to think that it has four gigabytes of main memory available; it doesn\u2019t. There may not even be four gigabytes of main memory available in the system. But that process is going to think that it has four gigabytes because it's using virtual memory. In a sixty-four bit operating system, we\u2019re going\u2026 The programmers are going to perceive a eighteen exabyte memory space which is just enormous, but that's what we can view this as. Even if the system doesn't have that much memory the program can be written as if it were available. The system the operating system can remove unused pages, so pages like that splash screen I talked about a few slides ago, those can be eliminated out of main memory; they're not going to ever be used again. So, let's take them out of main memory, store them in a second or storage device, just in case because we have no guarantee that they're not going to be used again, store them in the swap space and when they're never used again, we just don't put them back in main memory; so freeze up that main memory. More processes can run in the system and that means we have better performance, that means more processes generally means that there's a higher probability that a process will find itself in the ready state, so that will find a process that's in the ready state. So, ultimately we can result in better system performance just by having more main memory available. Huge benefits available for virtual armoring, which is why it's the standard today.",
                    ""
                ]
            },
            {
                "title": "Lookup Problems 1.17",
                "data": [
                    "Well one of the problems that we get now is that memory management is always going to require two look ups. So when we're talking about accessing any memory space, any logical address, it Now requires to look ups into main memory. So the M.M.U. is going to have to first look up in the page map table where this is located, and then translate that into a frame number, and then access that frame number.",
                    "So every main memory access now, by a process, is going to result in two main memory accesses. Now main memory is fast; There\u2019s no question it's that. But when we're doubling the price of everything, It really starts to add up and it slows down the C.P.U. So with the C.P.U. implemented with C.P.U. designers implemented that we just have to take into account, There\u2019s not much as the operating system we need to do about it. but what the C.P.U. designers implemented was what's called a translation look aside buffer. and the translation look aside buffer is a type of content addressable memory, which stores a cache. and I've got a picture here for you So that you can see it. it stores a cache of those entries in the P.M.T. which have been retrieved recently, and what it basically is is a table where we're looking up the virtual page number, which on that image that you're seeing is the the V.P N in that table; the virtual page number with a physical page number. so that virtual page number to physical page number mapping is only temporary, It\u2019s only there for a short period of time. in fact once we have a context switch, we basically have to dump the translation look aside buffer. But that virtual page number to physical page number mapping can be looked up because this is content addressable memory, It can be looked up in a time a big O of one, constant time search through the entire V.P.N. table.",
                    "So the V.P.N. table Can be looked at and if the the virtual page number entered is there, then we'll get back the physical page number and we can use that immediately inside the C.P.U. without accessing main memory. So it's just for speeding things up, so that we can avoid one of those memory references that we can avoid going to main memory and asking for that value. if we don't have the value from the page map table already loaded in the T.L. B, then we have to go look at the page map table and there's no way of to avoid that.",
                    "But the the the page map table is not going to change while the process is running, of course because we can't do relocation while the process is running. So the translation look aside buffer is a way to avoid that double memory access for multiple accesses to the same virtual page number. and it does work, It's not a huge savings but over the course of a run of the program, It does cut the memory references not in half but certainly significantly less.",
                    ""
                ]
            },
            {
                "title": "Replacement Policy 1.18",
                "data": [
                    "What happens when we run out of main memory? I mean it can happen ultimately, since we're giving each process as much memory as it wants affectively there's going to be a point in time when main memory runs out. And of course we're still going to be using virtual memory, so the swap space is going to start to be used, but what happens when we physically run out of main memory. Well now we're going to have to make a decision on which pages to pull out of the main memory, which pages no longer needed in main memory, and which we should load. This is called stealing; the choice of pulling out a page, and pulling a page out of main memory, it's called stealing. And unfortunately if we choose poorly we can really cause a lot of performance issues because if we remove a page which is going to be used, that's going to require an IO operation, it's going to require us possibly writing that page to the secondary storage device if the modified bit is set, we\u2019re going to have to write it to the secondary storage device then remarked the present bit to zero. And when the process runs again and wants to use that page, We're going to have to have a page fault and bring that back in. So, one one way of looking at the performance of virtual memory is to count the total number of page faults that are happening perception and if that number is is too high then one of the problems might be, the first obvious problem is that we have an insufficient amount of main memory. But the second obvious problem might be that we're choosing the wrong pages to remove; we\u2019re choosing pages that the process will immediately will want.",
                    "",
                    "We have a number of different algorithms that we can use for choosing which page the process might want. One of them that we call\u2026 One of them we call the optimal algorithm or this might be called the crystal ball algorithm. Because what it is is we look into our crystal ball, not literally of course, but we would look into our crystal ball and decide which page is not needed for the longest period of time. Now of course without looking at the process and what it's going to do in the next execution cycle, we can't know how long it's not going to need a page for. So, this is more a comparison algorithm, we can look at the memory accesses that a process did in its entire history and use that as a judge for deciding what would have been the optimal choices for page replacement, but ultimately we can't do this in real life.",
                    "",
                    "There's also the LRU algorithm, the least recently used, and in order to do this we would need a timestamp on each page to tell us when the last time that page was accessed; not modified, accessed. And unfortunately, we don't have time stamps on every page and it's too much overhead inside the CPU to timestamp every page. So the LRU is not really feasible either. ",
                    "",
                    "The FIFO algorithm is really simple; we just throughout the oldest page that we've brought in. But that doesn't say anything as to whether or not we're actually still using that page, and if we're still using that page and we throw it out it's going to immediately cause a page fault. In other words, it might be the oldest because the one that's needed the most.",
                    "",
                    "The last algorithm is relatively easy to implement it does require use bits. And thankfully, the hardware memory management units did recognize that, and what we have is the clock page replacement algorithm. So, we're using use bits to indicate when a page was used and what we do is\u2026 The hardware memory management unit updates those used bits whenever a page is used. In fact, it doesn't even have a problem with the translation look aside buffer, because what the hardware memory management unit does is when it loads the physical page number, sorry the frame number if you will, into the TLB it's going to update the use bit to be a one. So, the use bits just indicate whether or not the page was used recently and that's going to allow us to do the clock paid for placement algorithm which we're going to see in the next slide. So, we have to choose a replacement algorithm in some way to choose which slide is going to be, sorry to choose which page is going to be stolen, but if we make the decision with some reasonable piece of information then we can make it an effective decision making process.",
                    ""
                ]
            },
            {
                "title": "Clock Page Replacement Algorithm 1.19",
                "data": [
                    "So this is a very simple view of what the clock page replacement algorithm here is. We have just end pages, end frames available, and what we're going to do is keep a pointer that goes around and that's why we call it the clock page replacement algorithm. Now when we load up a page, we're going to set it\u2019s use bit equal to one and that's fine. So, we're going to have N frames and let's just say it's ten frames, just for easy math. We have ten frames and as the frames, as the pages are loaded up into those frames, we said all their use bits equal to zero and the pointer is pointing at let's say a page, frame number two, as you see it in the the left side image. When it's pointing at page number two, you can see that the use bit for frame number two page forty-five is set to one, the use bit for frame number three page one-ninety-one is set equal to one but the use bit for frame number four page five-hundred fifty-six to set equal to zero. So, what happens now when we have a page fault, we\u2019re going to have to remove a page and we're going to have to load up the new page. So, to do that what we do is real look for advancing the pointer; we\u2019re looking for use bits of zero. But if we find a use bit of one, we\u2019re going to reset its value back to zero. We're not going to steal it quite yet. We just set the use bit back zero. So, frame two and frame three as you can see they their page numbers didn't change but their use bits didn't change to zero. Now we need to load page seven-hundred twenty-seven but page five-hundred fifty-six, which is loaded in frame four is unused. So, when the pointer gets around to that point, we're going to remove that page stored in a second or a storage device if the modify bit or whatever, and load up page seven twenty-seven and set its use bit equal to one. The reason that this is effective is because if page forty-five is no longer in use, then next time the clock pointer comes around to frame two, the use bit will still be set to zero because it's not in use and page forty-five will be stolen now. So, the clock page replacement algorithm does have a very high efficacy rate; it's very effective in not throwing out pages which are actively in use, and it does have a pretty good rate of throwing out pages which aren't in use. So, we can save a lot of memory using this and it's really very simple to implement doesn't require a lot of effort to implement.",
                    ""
                ]
            },
            {
                "title": "Resident Set Management 1.20",
                "data": [
                    "Resident set management actually asks two different questions or two different problems. One of the problems is now how much memory do we give to each process? How much main memory does each process deserve? So, notepad for example, might only ask for one megabyte of main memory and Chrome might be asking for one gigabyte of main memory; should they get the same amount? And the answer probably is no, but how do we make the decision of how much to give each process. Now if we use smaller allocations, if we minimize the allocations then that means we have more processes but that also means we have more page faults because each one has, each process, has a smaller allocation. If we choose larger allocations, well that means we might have less page faults but less processes as well. So, it's sort of a give and take that we're going to see in the next slide, where allocations that are larger tend to mean less page faults and allocations that are smaller tend to mean more page faults. But how do we make the decision of what, how much notepad gets and how much chrome gets? So, that's one of the issues.",
                    "And then the next issue is when a process page faults, we're going to have to steal a page where, we\u2019re going to have to make room for the page. And are we going to choose this process, that it's page faulting to steal out of in order to make room for the new page or are we going to say that the entire system is open for stealing, and we can choose from any process? And obviously it means that that has a a role in whether or not the allocation is a variable allocation, meaning can change over time, or is a fixed allocation. Meaning that it's always going to be exactly the same. So, resident set management tries to deal with these two problems.",
                    ""
                ]
            },
            {
                "title": "Controlling Page Faults by Resident Set Size 1.21",
                "data": [
                    "So, I'm looking at this chart and what the chart is showing you is the page fault rate as compared to the number of frames allocated to the process. Now if we take a look, obviously if we only have one frame for the process it\u2019s going to create an almost infinite number of page faults; every memory access, almost, will need a page fault. And on the other side if we have an infinite number of frames, enough that it fits the entire process into main memory, then we have no page fault. But where it's interesting is that there's a non-linear progression between those two points. If we look at the process that has all the memory that it could possibly want and we remove just one frame, we only cause a very small number of page faults. and on the flip side if we look at a process which only has one Frame and we double the number of frames, that\u2019s going to result in a massive decrease in the number of page faults. So, there's a non linear progression between the two. And one of the things that we can extrapolate is that there is a sort of optimal location, where the process is not faulting too much but also is not is faulting a little bit. In other words, we\u2019re saying that page faults are not necessarily a bad thing; that since main memory is a quantity which is not infinite that we would like to have some number of page faults to indicate that the process hasn't been given too many frames, in other words that were starving other processes. And we can use a couple of algorithms to try and cause the process to try, and manage the resident set, to cause the process to fall in between the upper and lower bounds. If it's above the upper bounds, we can increase the number of frames that are available to that process thereby, effectively decreasing its page fault rate. And if it\u2019s below the lower bounds, then we can decrease the number of frames in the process give them to somebody else to hopefully lower its fault rate. And we can leave it in between this upper and lower bounds so that it has enough frames to do the work that it needs to do, but not so many friends that it's starving other processes.",
                    ""
                ]
            },
            {
                "title": "PFF Algorithm 1.22",
                "data": [
                    "One of the algorithms for resident set management that we're going to talk about is the page fault frequency algorithm or the PFF algorithm. The PFF algorithm tries to look at the frequency of the page faults by looking at the time between the current page fault and the last page fault. And we have some value that we'd like to hit, we have some value we call F. And the F says if we're page faulting more frequently than F, in other words, if the time between page faults is less than F, then the solution is to add a frame. And what that's going to do is if we're page faulting too often it should decrease the page fault rate because now we've given it more memory. But if the page fault rate is greater than F, then we can use those use bits look at all the used bits of zero discard those pages they're not use anymore, reset all the used bits of the remaining pages to zero and load up the the frame, the page, that we need into an available frame.",
                    "",
                    "So one of the points of the PFF algorithm is that we can manage the resident set, how many pages are loaded into main memory, by I looking at how often the algorithm is or how often the process is page faulting. One of course the difficult values is how to figure out a value for F. One of the difficult things is how to figure out a value for F. How do we set a value for F? How do we know what a reasonable F is? And in fact there isn't a reasonable value for F. As you saw in the previous slide it would be better to have an upper bounds and a lower bounds, so that we're not looking for a moving target, we're not looking for F and deciding whether it's too frequent or too infrequent. We can use an upper bounds and lower bounds to say we might add a frame, or we might steal from the current process, or we might steal from the global set. So, there's a number of different variations on the PFF algorithm that we can use to to make it more effective. ",
                    "",
                    "One problems of PFF though is that when the process moves from one locality to a completely different locality; so, it's starting in one area of its code and it jumps very far to do some other work. And it's going to be in the new locality for a long period of time, during a locality shift those pages in the new locality probably are not loaded. So, we'll have to load all those pages from the new locality and then when we're in the new locality, the page fault rate will decrease significantly because we have all the pages that we want. And so now we would, the next time try to unload those old pages from the old locality but unfortunately if we never fault again, for example, unlikely. But if we never fault again we're never going to clear out those old pages. So, the page fault frequency during a locality shift can result in both the old locality and the new locality using memory and that means double the memory and that's a huge waste. So, there are some downsides to using the PFF algorithm.",
                    ""
                ]
            },
            {
                "title": "VSWS Algorithm 1.23",
                "data": [
                    "The variable interval sampled working set or VSWS algorithm tries to solve PFF\u2019s problems by setting a number of different parameters. The first one is an M value, a minimum value for clearing out the unused pages. If we\u2019re ever below M, we\u2019re always going to add on a new page. If the page fault ever happens below M, we always add on a new page.",
                    "",
                    "Then we set a Q value, and we say if we're above M and we've reached Q page faults. We've had Q page faults since the last time we reset all the used bits and threw out the unused pages, then let's go ahead and do that algorithm again. So between M, above M and above Q, we would throw out all the unused pages and reset the use bits. Even if we\u2019re above M, but we've only had a very small number of page faults then it's too much work to go through the whole algorithm of resetting the used bits and throwing out the unused pages; so, we'll just add a page. ",
                    "",
                    "But once we get to L, L is the maximum limit. Once we get to L time units then it's time. It doesn't matter how many page faults we've had; this process had its chance. It's now time to throw out the unused pages, reset the use bits ,and let the process run again. So, even if we don't get a page fault by the time we get to L, we now say it's time to run this algorithm and clear out the old waste. And that's the point that VSWS solves the PFF shortcomings by through setting the L value to say that even if we haven't had a large number of page faults by the time we reach L, there's got to be some stale information there and we'll throw out that that old locality\u2019s information. So, it does actually solve the problems of PFF and it actually works fairly effectively.",
                    ""
                ]
            },
            {
                "title": "Load Control 1.24",
                "data": [
                    "Another issue that we can use page faults to look at is what we call load control, or we can use the number of page faults that are happening in a system to make a decision of whether or not to swap a process altogether out. Now the benefit, of course, of swapping a process is that we get all of its memory, not just a portion of its memory. But that whole amount of main memory is going to be removed and reallocated to the other running processes and of course that process can be brought back at a later time. We can do that we've always been able to do that but now we can do it on a much grander scale.",
                    "So, now we can use the number of page faults to tell us when it's time to choose a process to swap. But the question then becomes which process do we swap? And we have a couple of options, of course, if we have priorities in our processes, we could use the lowest priority process. We could also choose the faulting process as it's the greatest probability that it doesn't have its working set in the resident.",
                    "",
                    "So, the faulting process of course, if we swap that out, that means we don't have to recover a page; we don't have to deal with the page fault, we effectively deal with the page fault by swapping the entire process. The last process activated is certainly the least likely to have its working set resident. We could also choose the process with the smallest local resident set because it's the one that's going to be easiest to offload and easiest to onload later on, when we decide to run this process and finish it up. We could also choose the largest process, the exact the opposite, because we get the most amount of main memory and it's most likely that we're going to be able finish those other processes If we get back a large chunk of main memory. If we know how long we expect this process to run for, we can look at the process with the largest remaining execution window. And because the if we aren't going to finish this process for another five hours, who cares if it's another five hours and five minutes more. So, load control can be done by looking just at the number of page faults and it is a reasonable solution for the medium term scheduling algorithm.",
                    ""
                ]
            },
            {
                "title": "Shared Pages 1.25",
                "data": [
                    "I recognize that if we're using virtual memory and/or paging, then we can understand that we can share structures; we can share portions of a process. And the way that we can do that is quite simple: we can just have duplicate entries, duplicate frame number entries in a page map table. So, since the code doesn't change, for example, if we opened up five copies of Chrome would we need five different copies of the code for Chrome. The answer is no, not necessarily. The operating system may be smart enough, may be capable, of recognizing that multiple copies of the same program are running and it will share the code pages, not the data pages, not the context, just the code pages of that process because the code is not going to change from one process to the next process as long as they are the same program. So, if we're running Chrome twice, we should probably only load it's code once; it will, each copy will have a different data section but who cares because at least we're saving the memory for the code.",
                    "",
                    "Now there's also the possibility that a process will want to share data pages, but unfortunately if we're going to share data pages any changes to those data pages have to result in a duplications of those pages of data; not necessarily the entire portion of data but just those pages that are going to change. So, even if you're just changing one byte inside a page, we have to make an entire copy of the page. Now this is what's called copy on write, or cow, and it allows the sharing of the pages. The page table is now extended to the to have a read only attribute and if the page is marked as read only any attempt to change that page is going to result in a call to the operating system. And the operating system can then say this process is trying to change a page which has COW enabled. And when we change that page the operating system will make a duplications of just that page, update the appropriate page map tables, unsetting the read only bit and then re running the instruction. So, that the the operating system can intervene only of course when necessary, make the copy of the page, and then the processes can run again not realizing that the pages are now separate and different.",
                    ""
                ]
            },
            {
                "title": "Unix FORK Function 1.26",
                "data": [
                    "The greatest example of this is the Unix FORK function; it\u2019s been around for a while but it really does work quite well. The Unix FORK function when you call it, it actually returns two values. The reason or returns two different values is because it's going to create an exact copy of the entire current process. Now this is useful if, for example I'm just using this as an example. It's not actually how it works, but if for example in Chrome you click the button to create a new tab, we wanted different version of, we want to a different process for example, but it might be the exact same code. You might have not just a brand new copy but a duplicate an exact duplicate of the existing page. ",
                    "",
                    "And so what we can do is call the Unix FORK function. FORK will return I believe it's a zero if the child, if the process is the child process what's known as the child process which is the new process, or it return the process identifier of the child process if this is the parent process. So, we're creating two processes it's sort of like, the creation of a new amoeba out of an existing amoeba, right. That's an exact copy of the existing process; but how does it do this? Well, with the operating system will do is create a new PCB a new process control block, it creates all the new page tables, but the page tables are an exact copy of the original page table and everything in both page tables is marked as COW, Copy-On-Write. if any of the process in either process changes the page then that page alone is duplicated, but it saves the operating system having to actually go into main memory and duplicate all of main memory for this process into a new process. All it does is duplicate the page when it changes.",
                    ""
                ]
            }
        ]
    }
]